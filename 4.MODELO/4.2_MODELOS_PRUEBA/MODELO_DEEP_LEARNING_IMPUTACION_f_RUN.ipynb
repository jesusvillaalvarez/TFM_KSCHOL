{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "\n",
    "import pandas as pd  # Manipulación y análisis de datos.\n",
    "import numpy as np  # Soporte para vectores y matrices.\n",
    "\n",
    "# Gráficos\n",
    "\n",
    "import matplotlib.pyplot as plt  # Creación de gráficos estáticos, animados e interactivos.\n",
    "from matplotlib import style  # Personalización del estilo de los gráficos.\n",
    "\n",
    "# Preprocesado y modelado\n",
    "\n",
    "from scipy.stats import pearsonr  # Coeficiente de correlación de Pearson.\n",
    "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba.\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # Métricas para evaluar modelos.\n",
    "import statsmodels.api as sm  # Modelos estadísticos y econometricos.\n",
    "import statsmodels.formula.api as smf  # Modelo estadísticos con fórmulas.\n",
    "from statsmodels.stats.anova import anova_lm  # Análisis de varianza.\n",
    "from scipy import stats  # Funciones estadísticas.\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler  # Preprocesamiento de datos.\n",
    "import category_encoders  # Codificación de variables categóricas.\n",
    "import missingno as msno  # Visualización de datos faltantes.\n",
    "from sklearn.pipeline import Pipeline  # Cadena de transformaciones con un estimador final.\n",
    "from sklearn.experimental import enable_iterative_imputer  # Permitir uso de IterativeImputer.\n",
    "from sklearn.impute import IterativeImputer  # Imputación de datos faltantes.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Modelos de ensamble.\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score  # Búsqueda de hiperparámetros y validación cruzada.\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "import tensorflow as tf  # Biblioteca de Deep Learning.\n",
    "from tensorflow.keras import layers, models  # Construcción de modelos de deep learning.\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Callbacks para controlar el entrenamiento.\n",
    "from keras.models import Sequential  # Creación de modelos secuenciales.\n",
    "from keras.layers import Dense, Dropout, BatchNormalization  # Capas para construir modelos.\n",
    "from keras import regularizers  # Regularización de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Period</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>DESVIO</th>\n",
       "      <th>f_PREV_HIGH</th>\n",
       "      <th>f_PREV_LOW</th>\n",
       "      <th>f_RUN</th>\n",
       "      <th>Dia_Semana</th>\n",
       "      <th>Es_fin_semana</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Día</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Fecha  Period  PREVISION  E_SIMEL  DESVIO  f_PREV_HIGH  \\\n",
       "0           0  2021-01-01       1        0.0      0.0     0.0            0   \n",
       "1           1  2021-01-01       2        0.0      0.0     0.0            0   \n",
       "2           2  2021-01-01       3        0.0      0.0     0.0            0   \n",
       "3           3  2021-01-01       4        0.0      0.0     0.0            0   \n",
       "4           4  2021-01-01       5        0.0      0.0     0.0            0   \n",
       "\n",
       "   f_PREV_LOW  f_RUN  Dia_Semana  Es_fin_semana   Año  Mes  Día  \n",
       "0           0      0           4          False  2021    1    1  \n",
       "1           0      0           4          False  2021    1    1  \n",
       "2           0      0           4          False  2021    1    1  \n",
       "3           0      0           4          False  2021    1    1  \n",
       "4           0      0           4          False  2021    1    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargamos el archivo csv con los datos\n",
    "\n",
    "df_central = pd.read_csv(\"https://raw.githubusercontent.com/jesusvillaalvarez/TFM_KSCHOL/main/5.ARCHIVOS/df_central_2_1.csv\")\n",
    "\n",
    "df_central.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas innecesarias\n",
    "\n",
    "df_central = df_central.drop(columns=['Unnamed: 0', 'DESVIO', 'f_PREV_HIGH', 'f_PREV_LOW'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna 'Fecha' a Datetime para hacer las división en dos dfs\n",
    "\n",
    "df_central['Fecha'] = pd.to_datetime(df_central['Fecha'])\n",
    "\n",
    "# Dividimos el DataFrame en dos según las fechas especificas\n",
    "\n",
    "df_inicio = df_central[df_central['Fecha'] <= '2023-10-31']\n",
    "df_final = df_central[df_central['Fecha'] >= '2023-11-05']\n",
    "\n",
    "# Eliminamos la columna 'Fecha' de ambos DataFrames para poder preparar el modelo de Deep Learning\n",
    "\n",
    "df_inicio = df_inicio.drop(columns=['Fecha'])\n",
    "df_final = df_final.drop(columns=['Fecha'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19764, 8), (4942, 8), (19764,), (4942,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Preparamos df_inicio eliminando la variables objetivo del conjunto de entrenamiento (X) y especificamos la variable objetivo del conjunto de prueba (y)\n",
    "\n",
    "X = df_inicio.drop('E_SIMEL', axis=1)\n",
    "y = df_inicio['E_SIMEL']\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizamos las características, paso necesario para el modelo de deep learning\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verificamos las dimensiones de los conjuntos de datos para asegurarnos de que todo está correcto\n",
    "\n",
    "(X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                576       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4801 (18.75 KB)\n",
      "Trainable params: 4801 (18.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/75\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "495/495 [==============================] - 2s 2ms/step - loss: 83.9675 - mae: 4.5785 - val_loss: 40.4703 - val_mae: 3.0870\n",
      "Epoch 2/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 38.1887 - mae: 2.9013 - val_loss: 39.0416 - val_mae: 2.9969\n",
      "Epoch 3/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 37.3674 - mae: 2.8030 - val_loss: 37.9607 - val_mae: 2.7986\n",
      "Epoch 4/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.9517 - mae: 2.7667 - val_loss: 38.3392 - val_mae: 2.7527\n",
      "Epoch 5/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.8855 - mae: 2.7549 - val_loss: 38.2799 - val_mae: 2.8170\n",
      "Epoch 6/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.6714 - mae: 2.7555 - val_loss: 38.3922 - val_mae: 2.6713\n",
      "Epoch 7/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.2832 - mae: 2.7227 - val_loss: 37.6859 - val_mae: 2.7163\n",
      "Epoch 8/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.2874 - mae: 2.7146 - val_loss: 37.5882 - val_mae: 2.7282\n",
      "Epoch 9/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.1485 - mae: 2.7070 - val_loss: 37.4573 - val_mae: 2.7209\n",
      "Epoch 10/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.9679 - mae: 2.7024 - val_loss: 37.8090 - val_mae: 2.8438\n",
      "Epoch 11/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.1938 - mae: 2.7104 - val_loss: 38.9556 - val_mae: 2.9917\n",
      "Epoch 12/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.0311 - mae: 2.6961 - val_loss: 37.3397 - val_mae: 2.6921\n",
      "Epoch 13/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.6886 - mae: 2.6808 - val_loss: 37.2936 - val_mae: 2.8055\n",
      "Epoch 14/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.4054 - mae: 2.6679 - val_loss: 37.7399 - val_mae: 2.6480\n",
      "Epoch 15/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.7604 - mae: 2.6853 - val_loss: 37.4162 - val_mae: 2.8263\n",
      "Epoch 16/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.2986 - mae: 2.6647 - val_loss: 37.1795 - val_mae: 2.7871\n",
      "Epoch 17/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.4965 - mae: 2.6708 - val_loss: 37.6993 - val_mae: 2.6403\n",
      "Epoch 18/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.2403 - mae: 2.6600 - val_loss: 36.8978 - val_mae: 2.7867\n",
      "Epoch 19/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.9841 - mae: 2.6437 - val_loss: 36.2752 - val_mae: 2.7104\n",
      "Epoch 20/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.0438 - mae: 2.6541 - val_loss: 37.1061 - val_mae: 2.7378\n",
      "Epoch 21/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.7954 - mae: 2.6354 - val_loss: 36.4454 - val_mae: 2.6930\n",
      "Epoch 22/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.6933 - mae: 2.6362 - val_loss: 35.9043 - val_mae: 2.6998\n",
      "Epoch 23/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.5188 - mae: 2.6178 - val_loss: 35.9367 - val_mae: 2.6328\n",
      "Epoch 24/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.3750 - mae: 2.6173 - val_loss: 35.8726 - val_mae: 2.6568\n",
      "Epoch 25/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.4632 - mae: 2.6256 - val_loss: 35.8893 - val_mae: 2.7062\n",
      "Epoch 26/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.3612 - mae: 2.6356 - val_loss: 36.0509 - val_mae: 2.6928\n",
      "Epoch 27/75\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 34.0826 - mae: 2.6160 - val_loss: 36.4986 - val_mae: 2.5992\n",
      "Epoch 28/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.0617 - mae: 2.6086 - val_loss: 35.4754 - val_mae: 2.6570\n",
      "Epoch 29/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.8028 - mae: 2.6045 - val_loss: 35.6283 - val_mae: 2.6638\n",
      "Epoch 30/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.6583 - mae: 2.5987 - val_loss: 35.3612 - val_mae: 2.6694\n",
      "Epoch 31/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.8629 - mae: 2.6189 - val_loss: 35.6989 - val_mae: 2.6024\n",
      "Epoch 32/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.5239 - mae: 2.6032 - val_loss: 36.4907 - val_mae: 2.6943\n",
      "Epoch 33/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.3848 - mae: 2.5997 - val_loss: 35.2830 - val_mae: 2.6189\n",
      "Epoch 34/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.3005 - mae: 2.5977 - val_loss: 35.5082 - val_mae: 2.7661\n",
      "Epoch 35/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.4563 - mae: 2.6083 - val_loss: 35.2993 - val_mae: 2.6277\n",
      "Epoch 36/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.2691 - mae: 2.6104 - val_loss: 35.2604 - val_mae: 2.7376\n",
      "Epoch 37/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.1413 - mae: 2.6081 - val_loss: 35.2739 - val_mae: 2.6669\n",
      "Epoch 38/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.0244 - mae: 2.6021 - val_loss: 35.2155 - val_mae: 2.7443\n",
      "Epoch 39/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.7573 - mae: 2.5968 - val_loss: 35.1515 - val_mae: 2.6291\n",
      "Epoch 40/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.8806 - mae: 2.6039 - val_loss: 35.9093 - val_mae: 2.5979\n",
      "Epoch 41/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.6209 - mae: 2.5813 - val_loss: 35.0229 - val_mae: 2.7303\n",
      "Epoch 42/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.6132 - mae: 2.6042 - val_loss: 34.8247 - val_mae: 2.6261\n",
      "Epoch 43/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.4236 - mae: 2.5859 - val_loss: 35.2667 - val_mae: 2.7631\n",
      "Epoch 44/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.3020 - mae: 2.5782 - val_loss: 35.2374 - val_mae: 2.7664\n",
      "Epoch 45/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.0140 - mae: 2.5699 - val_loss: 35.4078 - val_mae: 2.6140\n",
      "Epoch 46/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.0691 - mae: 2.5748 - val_loss: 34.6694 - val_mae: 2.7172\n",
      "Epoch 47/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.7566 - mae: 2.5684 - val_loss: 34.8464 - val_mae: 2.7139\n",
      "Epoch 48/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.9043 - mae: 2.5807 - val_loss: 34.8301 - val_mae: 2.6258\n",
      "Epoch 49/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.7143 - mae: 2.5650 - val_loss: 34.5438 - val_mae: 2.7165\n",
      "Epoch 50/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.6246 - mae: 2.5630 - val_loss: 34.1378 - val_mae: 2.6518\n",
      "Epoch 51/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.5093 - mae: 2.5693 - val_loss: 34.9882 - val_mae: 2.7967\n",
      "Epoch 52/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.4467 - mae: 2.5701 - val_loss: 34.4249 - val_mae: 2.7500\n",
      "Epoch 53/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.2952 - mae: 2.5636 - val_loss: 34.9423 - val_mae: 2.6586\n",
      "Epoch 54/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.2719 - mae: 2.5612 - val_loss: 34.1587 - val_mae: 2.7524\n",
      "Epoch 55/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.1384 - mae: 2.5537 - val_loss: 34.8359 - val_mae: 2.7683\n",
      "Epoch 56/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.9453 - mae: 2.5498 - val_loss: 34.2478 - val_mae: 2.6575\n",
      "Epoch 57/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.9347 - mae: 2.5565 - val_loss: 34.1569 - val_mae: 2.6172\n",
      "Epoch 58/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.9712 - mae: 2.5498 - val_loss: 34.6099 - val_mae: 2.7296\n",
      "Epoch 59/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.9235 - mae: 2.5547 - val_loss: 34.6274 - val_mae: 2.6553\n",
      "Epoch 60/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.6731 - mae: 2.5520 - val_loss: 34.1796 - val_mae: 2.5865\n",
      "Epoch 61/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.6691 - mae: 2.5512 - val_loss: 33.7693 - val_mae: 2.6092\n",
      "Epoch 62/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.4449 - mae: 2.5446 - val_loss: 33.7532 - val_mae: 2.6649\n",
      "Epoch 63/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.3771 - mae: 2.5312 - val_loss: 37.9060 - val_mae: 3.0410\n",
      "Epoch 64/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.3576 - mae: 2.5437 - val_loss: 34.0745 - val_mae: 2.7421\n",
      "Epoch 65/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.2272 - mae: 2.5378 - val_loss: 33.8901 - val_mae: 2.6926\n",
      "Epoch 66/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.0867 - mae: 2.5251 - val_loss: 33.7117 - val_mae: 2.7425\n",
      "Epoch 67/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.1026 - mae: 2.5471 - val_loss: 34.3658 - val_mae: 2.6401\n",
      "Epoch 68/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.2782 - mae: 2.5482 - val_loss: 33.6121 - val_mae: 2.7312\n",
      "Epoch 69/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.9445 - mae: 2.5362 - val_loss: 34.2769 - val_mae: 2.5972\n",
      "Epoch 70/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.8230 - mae: 2.5195 - val_loss: 33.7442 - val_mae: 2.7792\n",
      "Epoch 71/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.8835 - mae: 2.5247 - val_loss: 32.9186 - val_mae: 2.6062\n",
      "Epoch 72/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.8178 - mae: 2.5243 - val_loss: 33.6670 - val_mae: 2.6692\n",
      "Epoch 73/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.7624 - mae: 2.5285 - val_loss: 34.4967 - val_mae: 2.6766\n",
      "Epoch 74/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.4947 - mae: 2.5185 - val_loss: 33.4782 - val_mae: 2.6484\n",
      "Epoch 75/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.6697 - mae: 2.5223 - val_loss: 33.6374 - val_mae: 2.7501\n",
      "155/155 [==============================] - 0s 772us/step - loss: 31.8486 - mae: 2.7002\n",
      "Loss en el conjunto de prueba: 31.84859275817871, MAE: 2.70015811920166\n"
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura del modelo, con dos capas ocultas de 64 unidades cada una con la función de activación 'relu', que proporciona\n",
    "# capacidad de modelado no lineal.\n",
    "# una capa de salida sin función de activación\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Capa de salida para predicción de un valor continuo\n",
    "])\n",
    "\n",
    "# Compilamos el modelo\n",
    "# utilizamos el optimizador 'adam' con la función de pérdida de Mean Square Error (mse) y la métrica mae\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Sacamos el resumen del modelo\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Entrenamos el modelo con 75 epochs y un batch_size de 32; visualizamos el proceso con verbose = 1\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=75, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluamos el conjunto de prueba\n",
    "\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}, MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imputador MICE entrenado con éxito.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuramos el imputador MICE con GradientBoostingRegressor y el estimador\n",
    "\n",
    "mice_imputer = IterativeImputer(estimator=GradientBoostingRegressor(\n",
    "                                    n_estimators=100,\n",
    "                                    max_depth=10,\n",
    "                                    min_samples_split=4,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    max_features='sqrt'),\n",
    "                                    max_iter=10, random_state=42)\n",
    "\n",
    "# Preparamos los datos para el entrenamiento del imputador MICE eliminando la variable objetivo E_SIMEL\n",
    "\n",
    "X_mice = df_inicio.drop(columns=['E_SIMEL'])\n",
    "\n",
    "# Entrenamos el imputador MICE\n",
    "\n",
    "mice_imputer.fit(X_mice)\n",
    "\n",
    "# Imprimimos confirmación\n",
    "\n",
    "\"Imputador MICE entrenado con éxito.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las filas del dia 5 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_05_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "\n",
    "df_final_05_11_para_imputar = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_05_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_05_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto y\n",
    "# establecemos una condición que si los valores son más grandes que 0.2 establecemos un 1, y si son inferiores establecemos un 0.\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_05_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [7.9727054 ],\n",
       "       [0.06837174],\n",
       "       [0.03427598],\n",
       "       [0.01833531],\n",
       "       [0.0238466 ],\n",
       "       [3.911273  ],\n",
       "       [0.03487203],\n",
       "       [0.04050824],\n",
       "       [0.04614732]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos los datos de df_final_05_11 para la predicción quitando la variable objetivo\n",
    "\n",
    "X_final_05_11 = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalización de los datos\n",
    "\n",
    "X_final_05_11_scaled = scaler.transform(X_final_05_11)\n",
    "\n",
    "# Realizamos la predicciones de E_SIMEL con el modelo de deep learning\n",
    "\n",
    "e_simel_predicciones = model.predict(X_final_05_11_scaled)\n",
    "\n",
    "e_simel_predicciones = np.maximum(e_simel_predicciones, 0)\n",
    "\n",
    "# Mostramos los resultados de la predicción\n",
    "\n",
    "e_simel_predicciones[:25]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_75548\\496075226.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24802</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24803</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24804</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24805</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24806</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24807</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24808</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24809</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24810</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24811</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24812</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24813</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24814</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24815</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24816</th>\n",
       "      <td>4.122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24817</th>\n",
       "      <td>5.437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.972705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24818</th>\n",
       "      <td>6.378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24819</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24820</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24821</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24822</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.911273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24823</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24824</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24825</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24802    0.000        0.0            0.000000\n",
       "24803    0.000        0.0            0.000000\n",
       "24804    0.000        0.0            0.000000\n",
       "24805    0.000        0.0            0.000000\n",
       "24806    0.000        0.0            0.000000\n",
       "24807    0.000        0.0            0.000000\n",
       "24808    0.000        0.0            0.000000\n",
       "24809    0.000        0.0            0.000000\n",
       "24810    0.000        0.0            0.000000\n",
       "24811    0.000        0.0            0.000000\n",
       "24812    0.000        0.0            0.000000\n",
       "24813    0.000        0.0            0.000000\n",
       "24814    0.000        0.0            0.000000\n",
       "24815    0.000        0.0            0.000000\n",
       "24816    4.122        0.0            0.000000\n",
       "24817    5.437        0.0            7.972705\n",
       "24818    6.378        0.0            0.068372\n",
       "24819    0.000        0.0            0.034276\n",
       "24820    0.000        0.0            0.018335\n",
       "24821    0.000        0.0            0.023847\n",
       "24822    0.000        0.0            3.911273\n",
       "24823    0.000        0.0            0.034872\n",
       "24824    0.000        0.0            0.040508\n",
       "24825    0.000        0.0            0.046147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos el array a una lista para facilitar la asignación a una nueva columna que llamamos 'Prediccion_E_SIMEL'\n",
    "\n",
    "predicciones_lista = e_simel_predicciones.flatten().tolist()\n",
    "\n",
    "# Asignar las predicciones a df_final_05_11\n",
    "\n",
    "df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Mostramos las primeras filas para verificar\n",
    "\n",
    "df_final_05_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  15.937000000000001\n",
      "Suma predicha:  12.15033558011055\n",
      "Desviación porcentual:  -23.760208445061497 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# al igual que en todos los casos anteriores, hacemos un sumatorio y porcentaje de desviación para tener una primera idea de como van las predicciones\n",
    "\n",
    "suma_real_05 = df_final_05_11['E_SIMEL'].sum()\n",
    "suma_predicha_05 = df_final_05_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_05 = df_final_05_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_05)\n",
    "print(\"Suma predicha: \", suma_predicha_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2466 - mae: 2.4025 - val_loss: 40.0556 - val_mae: 3.2167\n",
      "Epoch 2/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.8309 - mae: 2.3672 - val_loss: 43.0739 - val_mae: 3.2206\n",
      "Epoch 3/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.6175 - mae: 2.3591 - val_loss: 47.1312 - val_mae: 3.8640\n",
      "Epoch 4/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.5615 - mae: 2.3562 - val_loss: 44.0005 - val_mae: 3.6657\n",
      "Epoch 5/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.4188 - mae: 2.3615 - val_loss: 42.4099 - val_mae: 3.3736\n",
      "Epoch 6/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.2774 - mae: 2.3464 - val_loss: 43.5080 - val_mae: 3.5816\n",
      "Epoch 7/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.2715 - mae: 2.3397 - val_loss: 44.2502 - val_mae: 3.5322\n",
      "Epoch 8/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.0095 - mae: 2.3298 - val_loss: 44.8524 - val_mae: 3.4440\n",
      "Epoch 9/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.9853 - mae: 2.3325 - val_loss: 44.1135 - val_mae: 3.5300\n",
      "Epoch 10/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.6875 - mae: 2.3129 - val_loss: 44.9157 - val_mae: 3.5316\n",
      "Epoch 11/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.7185 - mae: 2.3257 - val_loss: 45.9445 - val_mae: 3.6238\n",
      "Epoch 12/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.6447 - mae: 2.3181 - val_loss: 49.2165 - val_mae: 3.9345\n",
      "Epoch 13/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.6270 - mae: 2.3246 - val_loss: 46.9907 - val_mae: 3.6327\n",
      "Epoch 14/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.4261 - mae: 2.3104 - val_loss: 47.1295 - val_mae: 3.9290\n",
      "Epoch 15/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.4464 - mae: 2.3292 - val_loss: 45.9459 - val_mae: 3.6688\n",
      "Epoch 16/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.3086 - mae: 2.3097 - val_loss: 51.1805 - val_mae: 4.1678\n",
      "Epoch 17/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.2200 - mae: 2.3090 - val_loss: 46.5453 - val_mae: 3.6659\n",
      "Epoch 18/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.1542 - mae: 2.3013 - val_loss: 46.4459 - val_mae: 3.7938\n",
      "Epoch 19/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.8740 - mae: 2.3021 - val_loss: 51.1163 - val_mae: 4.0629\n",
      "Epoch 20/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.9771 - mae: 2.3047 - val_loss: 47.7018 - val_mae: 3.8628\n",
      "Epoch 21/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.8370 - mae: 2.3133 - val_loss: 47.4459 - val_mae: 3.7647\n",
      "Epoch 22/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.8273 - mae: 2.2979 - val_loss: 48.5840 - val_mae: 3.8178\n",
      "Epoch 23/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.6663 - mae: 2.2924 - val_loss: 53.7276 - val_mae: 4.3436\n",
      "Epoch 24/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.6362 - mae: 2.2870 - val_loss: 48.5027 - val_mae: 3.8446\n",
      "Epoch 25/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.3734 - mae: 2.2727 - val_loss: 54.1625 - val_mae: 4.3154\n",
      "Epoch 26/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.3123 - mae: 2.2756 - val_loss: 53.1783 - val_mae: 4.2407\n",
      "Epoch 27/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.2350 - mae: 2.2860 - val_loss: 52.1527 - val_mae: 4.2454\n",
      "Epoch 28/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.1311 - mae: 2.2630 - val_loss: 55.1650 - val_mae: 4.3695\n",
      "Epoch 29/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.2409 - mae: 2.2898 - val_loss: 50.5015 - val_mae: 3.9386\n",
      "Epoch 30/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.9973 - mae: 2.2622 - val_loss: 51.3139 - val_mae: 4.0512\n",
      "Epoch 31/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.9527 - mae: 2.2711 - val_loss: 50.7901 - val_mae: 4.0726\n",
      "Epoch 32/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.0415 - mae: 2.2778 - val_loss: 50.7944 - val_mae: 3.9889\n",
      "Epoch 33/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.8010 - mae: 2.2552 - val_loss: 50.9479 - val_mae: 4.0993\n",
      "Epoch 34/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.7253 - mae: 2.2731 - val_loss: 52.3429 - val_mae: 4.2480\n",
      "Epoch 35/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.7661 - mae: 2.2709 - val_loss: 51.2593 - val_mae: 4.0893\n",
      "Epoch 36/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.7344 - mae: 2.2593 - val_loss: 53.4221 - val_mae: 4.4248\n",
      "Epoch 37/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.6146 - mae: 2.2646 - val_loss: 55.3948 - val_mae: 4.4461\n",
      "Epoch 38/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.3112 - mae: 2.2413 - val_loss: 53.6003 - val_mae: 4.2217\n",
      "Epoch 39/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.5156 - mae: 2.2605 - val_loss: 51.6818 - val_mae: 4.0964\n",
      "Epoch 40/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.4989 - mae: 2.2656 - val_loss: 57.8674 - val_mae: 4.5720\n",
      "Epoch 41/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.1970 - mae: 2.2415 - val_loss: 54.2207 - val_mae: 4.3267\n",
      "Epoch 42/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.2250 - mae: 2.2437 - val_loss: 53.3179 - val_mae: 4.3058\n",
      "Epoch 43/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.2131 - mae: 2.2415 - val_loss: 56.2916 - val_mae: 4.5493\n",
      "Epoch 44/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.1429 - mae: 2.2445 - val_loss: 55.8078 - val_mae: 4.4663\n",
      "Epoch 45/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.0327 - mae: 2.2398 - val_loss: 55.1487 - val_mae: 4.3992\n",
      "Epoch 46/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.9667 - mae: 2.2260 - val_loss: 58.1942 - val_mae: 4.6079\n",
      "Epoch 47/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.1029 - mae: 2.2498 - val_loss: 55.4809 - val_mae: 4.3597\n",
      "Epoch 48/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.1395 - mae: 2.2524 - val_loss: 53.9979 - val_mae: 4.2204\n",
      "Epoch 49/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.8284 - mae: 2.2315 - val_loss: 59.2228 - val_mae: 4.7861\n",
      "Epoch 50/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.7732 - mae: 2.2391 - val_loss: 61.6109 - val_mae: 4.8927\n",
      "Epoch 51/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.7603 - mae: 2.2454 - val_loss: 55.1477 - val_mae: 4.3385\n",
      "Epoch 52/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.7828 - mae: 2.2238 - val_loss: 57.9868 - val_mae: 4.5651\n",
      "Epoch 53/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.7549 - mae: 2.2308 - val_loss: 60.4382 - val_mae: 4.8036\n",
      "Epoch 54/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.6799 - mae: 2.2279 - val_loss: 54.2693 - val_mae: 4.3045\n",
      "Epoch 55/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5946 - mae: 2.2173 - val_loss: 55.6429 - val_mae: 4.4192\n",
      "Epoch 56/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5833 - mae: 2.2264 - val_loss: 55.3662 - val_mae: 4.5053\n",
      "Epoch 57/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5415 - mae: 2.2281 - val_loss: 58.0695 - val_mae: 4.5707\n",
      "Epoch 58/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.3170 - mae: 2.2106 - val_loss: 55.1948 - val_mae: 4.4929\n",
      "Epoch 59/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.4248 - mae: 2.2249 - val_loss: 56.2067 - val_mae: 4.5252\n",
      "Epoch 60/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.2363 - mae: 2.2114 - val_loss: 57.0826 - val_mae: 4.5572\n",
      "Epoch 61/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.1653 - mae: 2.2063 - val_loss: 65.0885 - val_mae: 5.1403\n",
      "Epoch 62/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.4954 - mae: 2.2314 - val_loss: 56.5924 - val_mae: 4.6260\n",
      "Epoch 63/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.1973 - mae: 2.2141 - val_loss: 57.2084 - val_mae: 4.6690\n",
      "Epoch 64/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.2164 - mae: 2.2092 - val_loss: 58.4825 - val_mae: 4.7210\n",
      "Epoch 65/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.1197 - mae: 2.2081 - val_loss: 60.1955 - val_mae: 4.8423\n",
      "Epoch 66/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.0562 - mae: 2.2031 - val_loss: 63.0854 - val_mae: 4.9770\n",
      "Epoch 67/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.9689 - mae: 2.2031 - val_loss: 62.7398 - val_mae: 4.9853\n",
      "Epoch 68/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.0508 - mae: 2.2239 - val_loss: 58.8882 - val_mae: 4.8229\n",
      "Epoch 69/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.9343 - mae: 2.2020 - val_loss: 63.2318 - val_mae: 5.0753\n",
      "Epoch 70/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8190 - mae: 2.1780 - val_loss: 63.0510 - val_mae: 4.9020\n",
      "Epoch 71/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8405 - mae: 2.1997 - val_loss: 59.4425 - val_mae: 4.8152\n",
      "Epoch 72/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8821 - mae: 2.2011 - val_loss: 60.8162 - val_mae: 4.9542\n",
      "Epoch 73/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8101 - mae: 2.1900 - val_loss: 59.6862 - val_mae: 4.9054\n",
      "Epoch 74/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8673 - mae: 2.1943 - val_loss: 60.4717 - val_mae: 4.8614\n",
      "Epoch 75/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.6183 - mae: 2.2001 - val_loss: 61.5383 - val_mae: 5.0040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IterativeImputer</label><div class=\"sk-toggleable__content\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Una vez tenemos la primera predicción, actualizamos df_inicio con los datos del día 5. Así estamos simulando como sería\n",
    "# el proceso de predicción en tiempo real\n",
    "\n",
    "datos_dia_5 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "df_inicio_actualizado = pd.concat([df_inicio, datos_dia_5])\n",
    "\n",
    "# Preparamos df_actualizado con los datos de df_inicio y los datos del día 5 para reentrenar el modelo de deep learning\n",
    "\n",
    "X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "\n",
    "# Normalizamos con fit_transform\n",
    "\n",
    "X_total_scaled = scaler.fit_transform(X_actualizado)  # Utilizamos fit_transform \n",
    "\n",
    "# Y reentrenamos el modelo de deep learning con todos los datos con los parámetros utilizados para el entrenamiento inicial\n",
    "\n",
    "model.fit(X_total_scaled, y_actualizado, epochs=75, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# reentrenamos también el modelo imputador con los nuevos datos con todas las variables menos con E_SIMEL\n",
    "\n",
    "mice_imputer.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_75548\\471906305.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24826</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24827</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24829</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24830</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24831</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24832</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24833</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24834</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24835</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24836</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24837</th>\n",
       "      <td>0.000</td>\n",
       "      <td>11.6</td>\n",
       "      <td>9.273043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>0.000</td>\n",
       "      <td>15.8</td>\n",
       "      <td>11.877087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24839</th>\n",
       "      <td>0.000</td>\n",
       "      <td>19.4</td>\n",
       "      <td>13.325764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24840</th>\n",
       "      <td>3.946</td>\n",
       "      <td>24.3</td>\n",
       "      <td>13.833293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24841</th>\n",
       "      <td>22.905</td>\n",
       "      <td>29.8</td>\n",
       "      <td>12.824780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24842</th>\n",
       "      <td>21.310</td>\n",
       "      <td>32.9</td>\n",
       "      <td>11.541689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24843</th>\n",
       "      <td>9.663</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.352631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24844</th>\n",
       "      <td>0.718</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24845</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24846</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24848</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24849</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24826    0.000        0.0            0.000000\n",
       "24827    0.000        0.0            0.000000\n",
       "24828    0.000        0.0            0.000000\n",
       "24829    0.000        0.0            0.000000\n",
       "24830    0.000        0.0            0.000000\n",
       "24831    0.000        0.0            0.000000\n",
       "24832    0.000        0.0            0.000000\n",
       "24833    0.000        0.0            0.000000\n",
       "24834    0.000        0.0            0.000000\n",
       "24835    0.000        0.0            0.000000\n",
       "24836    0.000        2.6            0.000000\n",
       "24837    0.000       11.6            9.273043\n",
       "24838    0.000       15.8           11.877087\n",
       "24839    0.000       19.4           13.325764\n",
       "24840    3.946       24.3           13.833293\n",
       "24841   22.905       29.8           12.824780\n",
       "24842   21.310       32.9           11.541689\n",
       "24843    9.663       19.0            7.352631\n",
       "24844    0.718        4.0            0.000000\n",
       "24845    0.000        0.0            0.000000\n",
       "24846    0.000        0.0            0.000000\n",
       "24847    0.000        0.0            0.000000\n",
       "24848    0.000        0.0            0.000000\n",
       "24849    0.000        0.0            0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seleccionamos las dilas del dia 6 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_06_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 6)]\n",
    "\n",
    "df_final_06_11_para_imputar = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_06_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_06_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_06_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n",
    "\n",
    "# Verificamos que los valores han sido imputados correctamente\n",
    "# df_final_05_11.head(25)\n",
    "\n",
    "# Preparar los datos de df_final_05_11 para la predicción\n",
    "X_final_06_11 = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalizar los datos de X_final_05_11 utilizando el mismo scaler que para los datos de entrenamiento\n",
    "X_final_06_11_scaled = scaler.transform(X_final_06_11)\n",
    "\n",
    "# Realizar las predicciones de E_SIMEL con el modelo de deep learning\n",
    "e_simel_predicciones_06 = model.predict(X_final_06_11_scaled)\n",
    "\n",
    "e_simel_predicciones_06 = np.maximum(e_simel_predicciones_06, 0)\n",
    "\n",
    "# Mostrar las primeras 5 predicciones\n",
    "# e_simel_predicciones[:25]\n",
    "\n",
    "# Convertir el array de predicciones a una lista para facilitar la asignación\n",
    "predicciones_lista = e_simel_predicciones_06.flatten().tolist()\n",
    "\n",
    "# Asignar las predicciones a df_final_05_11\n",
    "df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "df_final_06_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.542\n",
      "Suma predicha:  80.02828550338745\n",
      "Desviación porcentual:  36.7023427682475 %\n",
      "Suma previsión:  159.4\n",
      "Desviación porcentual:  172.2831471422227 %\n"
     ]
    }
   ],
   "source": [
    "suma_real_06 = df_final_06_11['E_SIMEL'].sum()\n",
    "suma_predicha_06 = df_final_06_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_06 = df_final_06_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_06)\n",
    "print(\"Suma predicha: \", suma_predicha_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un función para agilizar el proceso de actualización, reentreno de los modelos, imputación, predicción y cálculo de las métricas\n",
    "\n",
    "def predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente,  mes, año, df_inicio_actualizado, df_final, modelo_deep, imputador):\n",
    "    \"\"\"\n",
    "    Función para actualizar el conjunto de entrenamiento con los datos reales de un día específico,\n",
    "    realizar la imputación para el día siguiente y predecir los valores de E_SIMEL para ese día.\n",
    "\n",
    "    Args:\n",
    "    dia_actual (int): Día actual para el que se actualizarán los datos.\n",
    "    dia_siguiente (int): Datos del día que queremos hacer las imputaciones y la predicción\n",
    "    mes (int): Mes del día actual.\n",
    "    año (int): Año del día actual.\n",
    "    df_inicio_actualizado (DataFrame): DataFrame actualizado con los datos hasta el día anterior.\n",
    "    df_final (DataFrame): DataFrame con los datos a predecir.\n",
    "    modelo_rf (RandomForestRegressor): Modelo de Random Forest entrenado.\n",
    "    imputador (IterativeImputer): Imputador MICE entrenado.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame con las predicciones para el día siguiente.\n",
    "    DataFrame: DataFrame actualizado con los datos reales del día actual.\n",
    "    \"\"\"\n",
    "    # Actualización de df_actualizado con los datos de dia_actual\n",
    "\n",
    "    datos_dia_actual = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_actual)]\n",
    "    df_inicio_actualizado = pd.concat([df_inicio_actualizado, datos_dia_actual])\n",
    "\n",
    "    # Reentrenamos los modelos\n",
    "\n",
    "    # Una vez tenemos la primera predicción, actualizamos df_inicio con los datos del día 5. Así estamos simulando como sería\n",
    "    # el proceso de predicción en tiempo real\n",
    "\n",
    "\n",
    "\n",
    "    # Preparar los datos para el modelo de deep learning\n",
    "    X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "    y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "    \n",
    "    \n",
    "    # Normalizar las características\n",
    "    X_actualizado_scaled = scaler.transform(X_actualizado)\n",
    "\n",
    "    # Reentrenar el modelo de deep learning con todos los datos\n",
    "    modelo_deep.fit(X_actualizado_scaled, y_actualizado, epochs=75, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "    imputador.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "      # Imputación de valores a la columna f_RUN\n",
    "\n",
    "    df_dia_siguiente = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_siguiente)]\n",
    "    df_dia_siguiente_para_imputar = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "    df_dia_siguiente_para_imputar[['f_RUN']] = np.nan  \n",
    "    \n",
    "    valores_imputados = imputador.transform(df_dia_siguiente_para_imputar)\n",
    "    \n",
    "    df_dia_siguiente.loc[:, 'f_RUN'] = np.where(valores_imputados[:, 2] > 0.2, 1, 0) \n",
    "\n",
    "    \n",
    "    \n",
    "    # Preparar los datos de df_final_05_11 para la predicción\n",
    "    X_prediccion = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "    # Normalizar los datos de X_final_05_11 utilizando el mismo scaler que para los datos de entrenamiento\n",
    "    X_prediccion_scaled = scaler.transform(X_prediccion)\n",
    "\n",
    "    # Realizar las predicciones de E_SIMEL con el modelo de deep learning\n",
    "    predicted_e_simel = model.predict(X_prediccion_scaled)\n",
    "\n",
    "    predicted_e_simel = np.maximum(predicted_e_simel, 0)\n",
    "\n",
    "    # Mostrar las primeras 5 predicciones\n",
    "    # e_simel_predicciones[:25]\n",
    "\n",
    "    # Convertir el array de predicciones a una lista para facilitar la asignación\n",
    "    predicciones_lista = predicted_e_simel.flatten().tolist()\n",
    "\n",
    "    df_predicciones = df_dia_siguiente[['Año', 'Mes', 'Día', 'PREVISION', 'E_SIMEL']].copy()\n",
    "\n",
    "    # Asignar las predicciones a df_final_05_11\n",
    "    df_predicciones['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "    # Mostrar las primeras filas para verificar\n",
    "    df_predicciones[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Cálculo de las métricas\n",
    "\n",
    "    mse = mean_squared_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    mae = mean_absolute_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "\n",
    "\n",
    "    return df_predicciones, df_inicio_actualizado, mse, r2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.5942 - mae: 2.1811 - val_loss: 64.0351 - val_mae: 5.0101\n",
      "Epoch 2/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.7511 - mae: 2.2023 - val_loss: 62.0518 - val_mae: 5.0477\n",
      "Epoch 3/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.5595 - mae: 2.1983 - val_loss: 62.2288 - val_mae: 4.9699\n",
      "Epoch 4/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.5216 - mae: 2.1815 - val_loss: 61.7187 - val_mae: 5.0024\n",
      "Epoch 5/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.4250 - mae: 2.1835 - val_loss: 68.6147 - val_mae: 5.2954\n",
      "Epoch 6/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.5387 - mae: 2.1870 - val_loss: 69.8383 - val_mae: 5.3989\n",
      "Epoch 7/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.4886 - mae: 2.1879 - val_loss: 61.3683 - val_mae: 5.0041\n",
      "Epoch 8/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.3935 - mae: 2.1752 - val_loss: 68.9979 - val_mae: 5.5672\n",
      "Epoch 9/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2660 - mae: 2.1882 - val_loss: 62.2723 - val_mae: 4.9973\n",
      "Epoch 10/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2752 - mae: 2.1820 - val_loss: 69.9197 - val_mae: 5.3270\n",
      "Epoch 11/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.3050 - mae: 2.1768 - val_loss: 66.7830 - val_mae: 5.2901\n",
      "Epoch 12/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2122 - mae: 2.1717 - val_loss: 63.1920 - val_mae: 5.0246\n",
      "Epoch 13/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2965 - mae: 2.1817 - val_loss: 67.8082 - val_mae: 5.3483\n",
      "Epoch 14/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2416 - mae: 2.1717 - val_loss: 63.1002 - val_mae: 4.9680\n",
      "Epoch 15/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2549 - mae: 2.1827 - val_loss: 67.2977 - val_mae: 5.4101\n",
      "Epoch 16/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.0947 - mae: 2.1651 - val_loss: 63.4268 - val_mae: 5.0114\n",
      "Epoch 17/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9540 - mae: 2.1609 - val_loss: 65.9097 - val_mae: 5.3253\n",
      "Epoch 18/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9523 - mae: 2.1605 - val_loss: 71.3582 - val_mae: 5.6188\n",
      "Epoch 19/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9488 - mae: 2.1585 - val_loss: 66.1596 - val_mae: 5.1336\n",
      "Epoch 20/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9799 - mae: 2.1553 - val_loss: 65.5460 - val_mae: 5.1641\n",
      "Epoch 21/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9947 - mae: 2.1632 - val_loss: 64.9091 - val_mae: 5.0779\n",
      "Epoch 22/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9876 - mae: 2.1607 - val_loss: 65.1363 - val_mae: 5.0946\n",
      "Epoch 23/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.8317 - mae: 2.1551 - val_loss: 68.4911 - val_mae: 5.2897\n",
      "Epoch 24/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7123 - mae: 2.1455 - val_loss: 67.2362 - val_mae: 5.2759\n",
      "Epoch 25/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.8823 - mae: 2.1651 - val_loss: 64.3194 - val_mae: 5.2641\n",
      "Epoch 26/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7279 - mae: 2.1631 - val_loss: 69.3135 - val_mae: 5.4272\n",
      "Epoch 27/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7533 - mae: 2.1560 - val_loss: 73.4801 - val_mae: 5.4900\n",
      "Epoch 28/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7331 - mae: 2.1690 - val_loss: 69.7999 - val_mae: 5.3603\n",
      "Epoch 29/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7192 - mae: 2.1728 - val_loss: 63.9281 - val_mae: 4.9248\n",
      "Epoch 30/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7038 - mae: 2.1637 - val_loss: 70.3140 - val_mae: 5.4124\n",
      "Epoch 31/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7657 - mae: 2.1675 - val_loss: 69.2730 - val_mae: 5.3870\n",
      "Epoch 32/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.5110 - mae: 2.1497 - val_loss: 68.1824 - val_mae: 5.4193\n",
      "Epoch 33/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.5615 - mae: 2.1425 - val_loss: 75.9680 - val_mae: 5.8256\n",
      "Epoch 34/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.5278 - mae: 2.1653 - val_loss: 71.1234 - val_mae: 5.4181\n",
      "Epoch 35/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.5361 - mae: 2.1522 - val_loss: 67.5354 - val_mae: 5.2385\n",
      "Epoch 36/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.4040 - mae: 2.1419 - val_loss: 68.3541 - val_mae: 5.3053\n",
      "Epoch 37/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.4977 - mae: 2.1486 - val_loss: 73.2052 - val_mae: 5.6181\n",
      "Epoch 38/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.4797 - mae: 2.1589 - val_loss: 82.5702 - val_mae: 6.0474\n",
      "Epoch 39/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.3088 - mae: 2.1304 - val_loss: 67.3143 - val_mae: 5.2865\n",
      "Epoch 40/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.3989 - mae: 2.1532 - val_loss: 71.0474 - val_mae: 5.5688\n",
      "Epoch 41/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.3657 - mae: 2.1395 - val_loss: 74.1199 - val_mae: 5.4562\n",
      "Epoch 42/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2429 - mae: 2.1514 - val_loss: 69.0425 - val_mae: 5.4501\n",
      "Epoch 43/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.1816 - mae: 2.1301 - val_loss: 68.9410 - val_mae: 5.2808\n",
      "Epoch 44/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2568 - mae: 2.1387 - val_loss: 73.7312 - val_mae: 5.5732\n",
      "Epoch 45/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.0791 - mae: 2.1150 - val_loss: 76.3308 - val_mae: 5.9467\n",
      "Epoch 46/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.1479 - mae: 2.1410 - val_loss: 76.0178 - val_mae: 5.6993\n",
      "Epoch 47/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.1859 - mae: 2.1482 - val_loss: 69.8233 - val_mae: 5.4201\n",
      "Epoch 48/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.0670 - mae: 2.1272 - val_loss: 75.1159 - val_mae: 5.6259\n",
      "Epoch 49/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.0704 - mae: 2.1349 - val_loss: 70.5298 - val_mae: 5.4303\n",
      "Epoch 50/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8558 - mae: 2.1149 - val_loss: 73.3618 - val_mae: 5.4912\n",
      "Epoch 51/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.0644 - mae: 2.1225 - val_loss: 84.0048 - val_mae: 6.2051\n",
      "Epoch 52/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.1035 - mae: 2.1442 - val_loss: 75.9357 - val_mae: 5.8041\n",
      "Epoch 53/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.0427 - mae: 2.1240 - val_loss: 75.8055 - val_mae: 5.6713\n",
      "Epoch 54/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8043 - mae: 2.1125 - val_loss: 81.6798 - val_mae: 5.9863\n",
      "Epoch 55/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.9253 - mae: 2.1312 - val_loss: 86.9294 - val_mae: 6.1239\n",
      "Epoch 56/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.6852 - mae: 2.1191 - val_loss: 73.2841 - val_mae: 5.5211\n",
      "Epoch 57/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8122 - mae: 2.1159 - val_loss: 78.6702 - val_mae: 5.8306\n",
      "Epoch 58/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.7782 - mae: 2.1250 - val_loss: 76.9030 - val_mae: 5.9490\n",
      "Epoch 59/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8602 - mae: 2.1232 - val_loss: 79.5259 - val_mae: 5.8990\n",
      "Epoch 60/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.5922 - mae: 2.1104 - val_loss: 75.9325 - val_mae: 5.7121\n",
      "Epoch 61/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.6411 - mae: 2.1123 - val_loss: 75.1214 - val_mae: 5.5080\n",
      "Epoch 62/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.6213 - mae: 2.1117 - val_loss: 85.1713 - val_mae: 6.2114\n",
      "Epoch 63/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.7120 - mae: 2.1167 - val_loss: 78.4422 - val_mae: 5.9273\n",
      "Epoch 64/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.5693 - mae: 2.1226 - val_loss: 77.3990 - val_mae: 5.8572\n",
      "Epoch 65/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3711 - mae: 2.1067 - val_loss: 74.5456 - val_mae: 5.6394\n",
      "Epoch 66/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.5095 - mae: 2.1154 - val_loss: 77.7844 - val_mae: 5.6595\n",
      "Epoch 67/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3552 - mae: 2.1003 - val_loss: 95.0417 - val_mae: 6.5290\n",
      "Epoch 68/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.6122 - mae: 2.1364 - val_loss: 86.0874 - val_mae: 5.9499\n",
      "Epoch 69/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3679 - mae: 2.0982 - val_loss: 81.9193 - val_mae: 5.9381\n",
      "Epoch 70/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3308 - mae: 2.0992 - val_loss: 83.4596 - val_mae: 6.2061\n",
      "Epoch 71/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3906 - mae: 2.1092 - val_loss: 80.8546 - val_mae: 5.8382\n",
      "Epoch 72/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.4340 - mae: 2.1079 - val_loss: 78.3133 - val_mae: 5.7856\n",
      "Epoch 73/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.4881 - mae: 2.1007 - val_loss: 80.1390 - val_mae: 6.0679\n",
      "Epoch 74/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3038 - mae: 2.0972 - val_loss: 83.0099 - val_mae: 5.9199\n",
      "Epoch 75/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3633 - mae: 2.1138 - val_loss: 76.2019 - val_mae: 5.4978\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 32.794801621742415 R²: 0.1772778852634277 MAE: 2.9352909119923907\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 6    # Actualización de datos con los que reentrenamos los modelos\n",
    "dia_siguiente = 7    # Preparación de datos para la imputación y predicción\n",
    "df_predicciones_07_11, df_inicio_actualizado, mse_07_11, r2_07_11, mae_07_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_07_11, \"R²:\", r2_07_11, \"MAE:\", mae_07_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  67.97999999999999\n",
      "Suma predicha:  50.510860443115234\n",
      "Desviación porcentual:  -25.69746919224001 %\n",
      "Suma previsión:  24.0\n",
      "Desviación porcentual:  -64.6954986760812 %\n"
     ]
    }
   ],
   "source": [
    "# Y como en la predicción anterior calculamos los sumatorios y los porcentajes de desviación\n",
    "\n",
    "suma_real_07_11 = df_predicciones_07_11['E_SIMEL'].sum()\n",
    "suma_predicha_07_11 = df_predicciones_07_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_07_11 = df_predicciones_07_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_07_11)\n",
    "print(\"Suma predicha: \", suma_predicha_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.3306 - mae: 2.1031 - val_loss: 80.6518 - val_mae: 5.8475\n",
      "Epoch 2/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.1459 - mae: 2.1054 - val_loss: 79.2018 - val_mae: 5.8910\n",
      "Epoch 3/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.2732 - mae: 2.1059 - val_loss: 77.2040 - val_mae: 5.6131\n",
      "Epoch 4/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.1852 - mae: 2.0977 - val_loss: 84.1963 - val_mae: 5.9719\n",
      "Epoch 5/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.2720 - mae: 2.1097 - val_loss: 78.7192 - val_mae: 5.7498\n",
      "Epoch 6/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.1981 - mae: 2.0929 - val_loss: 83.8938 - val_mae: 6.0239\n",
      "Epoch 7/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.1117 - mae: 2.0953 - val_loss: 82.6924 - val_mae: 5.8804\n",
      "Epoch 8/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.0124 - mae: 2.0780 - val_loss: 91.4248 - val_mae: 6.1590\n",
      "Epoch 9/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.9940 - mae: 2.0945 - val_loss: 91.8791 - val_mae: 6.2246\n",
      "Epoch 10/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.9430 - mae: 2.0837 - val_loss: 85.7005 - val_mae: 5.9930\n",
      "Epoch 11/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.0280 - mae: 2.0875 - val_loss: 78.3876 - val_mae: 5.6734\n",
      "Epoch 12/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.1133 - mae: 2.0994 - val_loss: 87.8472 - val_mae: 6.1065\n",
      "Epoch 13/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.0438 - mae: 2.0905 - val_loss: 81.3831 - val_mae: 5.7241\n",
      "Epoch 14/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.9958 - mae: 2.0883 - val_loss: 94.8135 - val_mae: 6.4437\n",
      "Epoch 15/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.7262 - mae: 2.0681 - val_loss: 81.4582 - val_mae: 5.8136\n",
      "Epoch 16/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 20.1663 - mae: 2.1274 - val_loss: 89.6795 - val_mae: 6.4342\n",
      "Epoch 17/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.8643 - mae: 2.0788 - val_loss: 89.4735 - val_mae: 6.0873\n",
      "Epoch 18/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.8794 - mae: 2.0945 - val_loss: 83.4621 - val_mae: 6.0378\n",
      "Epoch 19/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.7770 - mae: 2.0848 - val_loss: 90.6432 - val_mae: 6.1658\n",
      "Epoch 20/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.6973 - mae: 2.0861 - val_loss: 86.1193 - val_mae: 5.9170\n",
      "Epoch 21/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.6651 - mae: 2.0781 - val_loss: 87.4588 - val_mae: 6.1603\n",
      "Epoch 22/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.8924 - mae: 2.0958 - val_loss: 79.6599 - val_mae: 5.6664\n",
      "Epoch 23/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.8094 - mae: 2.0839 - val_loss: 86.4188 - val_mae: 6.0823\n",
      "Epoch 24/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.7931 - mae: 2.0825 - val_loss: 90.4140 - val_mae: 6.2718\n",
      "Epoch 25/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.6639 - mae: 2.0809 - val_loss: 84.7133 - val_mae: 5.9312\n",
      "Epoch 26/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.6181 - mae: 2.0817 - val_loss: 98.4939 - val_mae: 6.4514\n",
      "Epoch 27/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4617 - mae: 2.0672 - val_loss: 101.2375 - val_mae: 6.4853\n",
      "Epoch 28/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.7542 - mae: 2.0888 - val_loss: 92.2981 - val_mae: 6.2847\n",
      "Epoch 29/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.5466 - mae: 2.0731 - val_loss: 85.6244 - val_mae: 5.9330\n",
      "Epoch 30/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.6612 - mae: 2.0769 - val_loss: 88.8045 - val_mae: 6.0273\n",
      "Epoch 31/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4509 - mae: 2.0626 - val_loss: 90.8141 - val_mae: 6.1918\n",
      "Epoch 32/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.5279 - mae: 2.0745 - val_loss: 88.9164 - val_mae: 6.0600\n",
      "Epoch 33/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3581 - mae: 2.0783 - val_loss: 94.1977 - val_mae: 6.2923\n",
      "Epoch 34/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4465 - mae: 2.0614 - val_loss: 85.5644 - val_mae: 6.0803\n",
      "Epoch 35/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.5793 - mae: 2.0801 - val_loss: 88.9864 - val_mae: 6.1701\n",
      "Epoch 36/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3648 - mae: 2.0659 - val_loss: 90.8139 - val_mae: 6.1606\n",
      "Epoch 37/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3789 - mae: 2.0656 - val_loss: 89.1127 - val_mae: 6.0578\n",
      "Epoch 38/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3819 - mae: 2.0517 - val_loss: 93.7729 - val_mae: 6.1622\n",
      "Epoch 39/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2407 - mae: 2.0465 - val_loss: 90.8431 - val_mae: 6.1539\n",
      "Epoch 40/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2401 - mae: 2.0518 - val_loss: 100.7327 - val_mae: 6.5060\n",
      "Epoch 41/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2213 - mae: 2.0553 - val_loss: 92.6420 - val_mae: 6.1074\n",
      "Epoch 42/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2946 - mae: 2.0650 - val_loss: 91.5162 - val_mae: 6.1688\n",
      "Epoch 43/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3218 - mae: 2.0649 - val_loss: 93.3088 - val_mae: 6.3067\n",
      "Epoch 44/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2024 - mae: 2.0611 - val_loss: 88.7491 - val_mae: 6.1584\n",
      "Epoch 45/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1714 - mae: 2.0573 - val_loss: 89.1426 - val_mae: 6.1055\n",
      "Epoch 46/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0400 - mae: 2.0546 - val_loss: 91.2253 - val_mae: 6.2809\n",
      "Epoch 47/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1448 - mae: 2.0509 - val_loss: 91.4937 - val_mae: 6.2110\n",
      "Epoch 48/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0049 - mae: 2.0453 - val_loss: 91.5547 - val_mae: 6.0026\n",
      "Epoch 49/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.9113 - mae: 2.0366 - val_loss: 99.2114 - val_mae: 6.3365\n",
      "Epoch 50/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1986 - mae: 2.0642 - val_loss: 98.4980 - val_mae: 6.3688\n",
      "Epoch 51/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2727 - mae: 2.0690 - val_loss: 100.9208 - val_mae: 6.4985\n",
      "Epoch 52/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1905 - mae: 2.0661 - val_loss: 93.6535 - val_mae: 6.3332\n",
      "Epoch 53/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.9579 - mae: 2.0506 - val_loss: 93.6640 - val_mae: 6.2301\n",
      "Epoch 54/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2165 - mae: 2.0620 - val_loss: 105.2750 - val_mae: 6.5771\n",
      "Epoch 55/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2078 - mae: 2.0678 - val_loss: 90.4660 - val_mae: 6.0256\n",
      "Epoch 56/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0807 - mae: 2.0548 - val_loss: 99.6744 - val_mae: 6.6553\n",
      "Epoch 57/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8864 - mae: 2.0516 - val_loss: 100.1294 - val_mae: 6.5584\n",
      "Epoch 58/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8989 - mae: 2.0360 - val_loss: 98.9068 - val_mae: 6.4165\n",
      "Epoch 59/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.9540 - mae: 2.0629 - val_loss: 100.8429 - val_mae: 6.3185\n",
      "Epoch 60/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8873 - mae: 2.0304 - val_loss: 92.8953 - val_mae: 6.0845\n",
      "Epoch 61/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7873 - mae: 2.0425 - val_loss: 100.4392 - val_mae: 6.4439\n",
      "Epoch 62/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7441 - mae: 2.0238 - val_loss: 103.6906 - val_mae: 6.4819\n",
      "Epoch 63/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1121 - mae: 2.0614 - val_loss: 96.1150 - val_mae: 6.3612\n",
      "Epoch 64/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7590 - mae: 2.0394 - val_loss: 113.6485 - val_mae: 6.8094\n",
      "Epoch 65/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8128 - mae: 2.0350 - val_loss: 102.8849 - val_mae: 6.3977\n",
      "Epoch 66/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6404 - mae: 2.0174 - val_loss: 99.5758 - val_mae: 6.3720\n",
      "Epoch 67/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7034 - mae: 2.0501 - val_loss: 108.0061 - val_mae: 6.6035\n",
      "Epoch 68/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6715 - mae: 2.0314 - val_loss: 102.6890 - val_mae: 6.4113\n",
      "Epoch 69/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6903 - mae: 2.0367 - val_loss: 100.0702 - val_mae: 6.2516\n",
      "Epoch 70/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8480 - mae: 2.0538 - val_loss: 104.5239 - val_mae: 6.7141\n",
      "Epoch 71/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8263 - mae: 2.0423 - val_loss: 113.1293 - val_mae: 6.7715\n",
      "Epoch 72/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.5917 - mae: 2.0152 - val_loss: 101.9601 - val_mae: 6.3436\n",
      "Epoch 73/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6367 - mae: 2.0336 - val_loss: 96.0258 - val_mae: 6.2662\n",
      "Epoch 74/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.5781 - mae: 2.0270 - val_loss: 112.7706 - val_mae: 6.7921\n",
      "Epoch 75/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6091 - mae: 2.0309 - val_loss: 111.4016 - val_mae: 6.7569\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 3.241508989230945 R²: -0.06716057003258435 MAE: 0.8521580402851106\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función. Actualización de datos reales del día 7 para reentrenar los modelos\n",
    "# Preparación de los datos del día posterior, en este caso del día 8, para la imputación y predicción\n",
    "\n",
    "dia_actual = 7\n",
    "dia_siguiente = 8\n",
    "\n",
    "df_predicciones_08_11, df_inicio_actualizado, mse_08_11, r2_08_11, mae_08_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_08_11, \"R²:\", r2_08_11, \"MAE:\", mae_08_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  11.296999999999999\n",
      "Suma predicha:  25.515476942062378\n",
      "Desviación porcentual:  125.86064390601382 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_08_11 = df_predicciones_08_11['E_SIMEL'].sum()\n",
    "suma_predicha_08_11 = df_predicciones_08_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_08_11 = df_predicciones_08_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_08_11)\n",
    "print(\"Suma predicha: \", suma_predicha_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.4017 - mae: 2.0285 - val_loss: 105.6398 - val_mae: 6.4915\n",
      "Epoch 2/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.6117 - mae: 2.0561 - val_loss: 100.8316 - val_mae: 6.2792\n",
      "Epoch 3/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.7929 - mae: 2.0447 - val_loss: 109.2232 - val_mae: 6.6216\n",
      "Epoch 4/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.3662 - mae: 2.0160 - val_loss: 110.5621 - val_mae: 6.8013\n",
      "Epoch 5/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.1595 - mae: 2.0083 - val_loss: 104.0406 - val_mae: 6.5344\n",
      "Epoch 6/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.3751 - mae: 2.0224 - val_loss: 113.2461 - val_mae: 6.8429\n",
      "Epoch 7/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.4965 - mae: 2.0183 - val_loss: 114.2841 - val_mae: 6.8761\n",
      "Epoch 8/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.5164 - mae: 2.0371 - val_loss: 98.3650 - val_mae: 6.3090\n",
      "Epoch 9/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.4011 - mae: 2.0182 - val_loss: 97.9933 - val_mae: 6.2542\n",
      "Epoch 10/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.4355 - mae: 2.0414 - val_loss: 109.4329 - val_mae: 6.7169\n",
      "Epoch 11/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.1482 - mae: 1.9982 - val_loss: 102.6427 - val_mae: 6.3588\n",
      "Epoch 12/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.4078 - mae: 2.0411 - val_loss: 109.0810 - val_mae: 6.6949\n",
      "Epoch 13/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.1868 - mae: 2.0155 - val_loss: 110.9770 - val_mae: 6.7501\n",
      "Epoch 14/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.5164 - mae: 2.0365 - val_loss: 112.3272 - val_mae: 7.0637\n",
      "Epoch 15/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.2345 - mae: 2.0435 - val_loss: 110.3663 - val_mae: 6.7617\n",
      "Epoch 16/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.0076 - mae: 2.0046 - val_loss: 104.5233 - val_mae: 6.4615\n",
      "Epoch 17/75\n",
      "621/621 [==============================] - 2s 2ms/step - loss: 18.0681 - mae: 2.0052 - val_loss: 122.9699 - val_mae: 7.1520\n",
      "Epoch 18/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.3439 - mae: 2.0342 - val_loss: 107.0830 - val_mae: 6.5946\n",
      "Epoch 19/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.2585 - mae: 2.0360 - val_loss: 109.4724 - val_mae: 6.6704\n",
      "Epoch 20/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.0583 - mae: 2.0028 - val_loss: 108.7457 - val_mae: 6.6549\n",
      "Epoch 21/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.1104 - mae: 2.0231 - val_loss: 133.5777 - val_mae: 7.2890\n",
      "Epoch 22/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.0322 - mae: 2.0014 - val_loss: 120.1203 - val_mae: 6.9584\n",
      "Epoch 23/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.0665 - mae: 2.0135 - val_loss: 107.3422 - val_mae: 6.8115\n",
      "Epoch 24/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.0540 - mae: 2.0238 - val_loss: 111.9729 - val_mae: 6.7068\n",
      "Epoch 25/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.1753 - mae: 2.0159 - val_loss: 103.3523 - val_mae: 6.4018\n",
      "Epoch 26/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.8943 - mae: 1.9847 - val_loss: 114.2648 - val_mae: 6.8749\n",
      "Epoch 27/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.8454 - mae: 1.9949 - val_loss: 114.6446 - val_mae: 6.8034\n",
      "Epoch 28/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.8211 - mae: 1.9860 - val_loss: 128.9947 - val_mae: 7.2944\n",
      "Epoch 29/75\n",
      "621/621 [==============================] - 2s 3ms/step - loss: 18.3355 - mae: 2.0106 - val_loss: 113.1451 - val_mae: 6.8026\n",
      "Epoch 30/75\n",
      "621/621 [==============================] - 2s 3ms/step - loss: 17.7661 - mae: 1.9734 - val_loss: 113.0597 - val_mae: 6.7451\n",
      "Epoch 31/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.8884 - mae: 1.9988 - val_loss: 129.8667 - val_mae: 7.2921\n",
      "Epoch 32/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.7879 - mae: 1.9926 - val_loss: 119.8595 - val_mae: 6.9578\n",
      "Epoch 33/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6536 - mae: 1.9868 - val_loss: 109.4173 - val_mae: 6.6571\n",
      "Epoch 34/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.9550 - mae: 2.0049 - val_loss: 116.7228 - val_mae: 6.9149\n",
      "Epoch 35/75\n",
      "621/621 [==============================] - 2s 2ms/step - loss: 17.7133 - mae: 1.9899 - val_loss: 114.3067 - val_mae: 6.8257\n",
      "Epoch 36/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.7452 - mae: 1.9920 - val_loss: 124.0047 - val_mae: 7.1179\n",
      "Epoch 37/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.7220 - mae: 1.9897 - val_loss: 130.5359 - val_mae: 7.2982\n",
      "Epoch 38/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.8198 - mae: 1.9979 - val_loss: 122.6726 - val_mae: 7.2407\n",
      "Epoch 39/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.5603 - mae: 1.9842 - val_loss: 124.6200 - val_mae: 7.0550\n",
      "Epoch 40/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6034 - mae: 1.9916 - val_loss: 113.7062 - val_mae: 6.8758\n",
      "Epoch 41/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.4564 - mae: 1.9888 - val_loss: 116.3617 - val_mae: 6.8798\n",
      "Epoch 42/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.7255 - mae: 1.9996 - val_loss: 113.4935 - val_mae: 6.6048\n",
      "Epoch 43/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.5195 - mae: 2.0023 - val_loss: 121.9975 - val_mae: 7.1670\n",
      "Epoch 44/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.7178 - mae: 2.0026 - val_loss: 123.5358 - val_mae: 7.1490\n",
      "Epoch 45/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.4851 - mae: 1.9836 - val_loss: 115.1274 - val_mae: 6.7926\n",
      "Epoch 46/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6560 - mae: 1.9903 - val_loss: 121.1388 - val_mae: 6.9244\n",
      "Epoch 47/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6080 - mae: 1.9878 - val_loss: 122.6442 - val_mae: 7.0517\n",
      "Epoch 48/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6170 - mae: 1.9962 - val_loss: 111.6758 - val_mae: 6.6850\n",
      "Epoch 49/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6309 - mae: 1.9887 - val_loss: 118.6151 - val_mae: 6.8916\n",
      "Epoch 50/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6234 - mae: 1.9946 - val_loss: 118.5171 - val_mae: 6.7965\n",
      "Epoch 51/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.4498 - mae: 1.9889 - val_loss: 123.3317 - val_mae: 7.1431\n",
      "Epoch 52/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.3337 - mae: 1.9842 - val_loss: 140.8047 - val_mae: 7.4039\n",
      "Epoch 53/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.3949 - mae: 1.9725 - val_loss: 114.7324 - val_mae: 6.7532\n",
      "Epoch 54/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.5051 - mae: 2.0029 - val_loss: 122.3133 - val_mae: 7.2686\n",
      "Epoch 55/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.5136 - mae: 1.9952 - val_loss: 110.6399 - val_mae: 6.5705\n",
      "Epoch 56/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.4132 - mae: 1.9845 - val_loss: 126.1278 - val_mae: 7.1707\n",
      "Epoch 57/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.3440 - mae: 1.9882 - val_loss: 126.2406 - val_mae: 7.0588\n",
      "Epoch 58/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.2449 - mae: 1.9877 - val_loss: 118.2066 - val_mae: 7.0010\n",
      "Epoch 59/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.2970 - mae: 1.9800 - val_loss: 123.0376 - val_mae: 7.0787\n",
      "Epoch 60/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.0855 - mae: 1.9659 - val_loss: 137.8167 - val_mae: 7.3476\n",
      "Epoch 61/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.3437 - mae: 1.9919 - val_loss: 117.3074 - val_mae: 6.8464\n",
      "Epoch 62/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.2386 - mae: 1.9898 - val_loss: 125.5416 - val_mae: 7.2505\n",
      "Epoch 63/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.2587 - mae: 1.9687 - val_loss: 127.0963 - val_mae: 7.2090\n",
      "Epoch 64/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.1338 - mae: 1.9617 - val_loss: 121.4408 - val_mae: 7.1462\n",
      "Epoch 65/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.2873 - mae: 1.9753 - val_loss: 139.0278 - val_mae: 7.4349\n",
      "Epoch 66/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.1129 - mae: 1.9702 - val_loss: 135.5408 - val_mae: 7.6052\n",
      "Epoch 67/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.3219 - mae: 1.9902 - val_loss: 162.9033 - val_mae: 8.4833\n",
      "Epoch 68/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.9990 - mae: 2.0301 - val_loss: 120.8577 - val_mae: 7.0361\n",
      "Epoch 69/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9455 - mae: 1.9519 - val_loss: 128.0518 - val_mae: 7.2473\n",
      "Epoch 70/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.0809 - mae: 1.9709 - val_loss: 125.9114 - val_mae: 7.1055\n",
      "Epoch 71/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.0242 - mae: 1.9545 - val_loss: 125.3591 - val_mae: 7.0594\n",
      "Epoch 72/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9125 - mae: 1.9711 - val_loss: 124.3494 - val_mae: 7.1974\n",
      "Epoch 73/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9709 - mae: 1.9511 - val_loss: 157.3504 - val_mae: 8.1074\n",
      "Epoch 74/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9476 - mae: 1.9576 - val_loss: 122.4963 - val_mae: 7.1399\n",
      "Epoch 75/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.9747 - mae: 1.9559 - val_loss: 124.6764 - val_mae: 7.1202\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "MSE: 46.51304179851535 R²: 0.5026446755412615 MAE: 3.1001743804613753\n"
     ]
    }
   ],
   "source": [
    "# Seguimos con el mismo proceso anterior. Llamamos a la función con los días específicos que queremos actualizar, imputar y predecir.\n",
    "\n",
    "dia_actual = 8\n",
    "dia_siguiente = 9\n",
    "\n",
    "df_predicciones_09_11, df_inicio_actualizado, mse_09_11, r2_09_11, mae_09_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_09_11, \"R²:\", r2_09_11, \"MAE:\", mae_09_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  102.43700000000001\n",
      "Suma predicha:  91.57408380508423\n",
      "Desviación porcentual:  -10.604484897952675 %\n",
      "Suma previsión:  127.30000000000001\n",
      "Desviación porcentual:  24.271503460663627 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y procentajes\n",
    "\n",
    "suma_real_09_11 = df_predicciones_09_11['E_SIMEL'].sum()\n",
    "suma_predicha_09_11 = df_predicciones_09_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_09_11 = df_predicciones_09_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_09_11)\n",
    "print(\"Suma predicha: \", suma_predicha_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "621/621 [==============================] - 2s 3ms/step - loss: 16.9602 - mae: 1.9549 - val_loss: 125.0082 - val_mae: 7.1424\n",
      "Epoch 2/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.0389 - mae: 1.9706 - val_loss: 122.0263 - val_mae: 7.0829\n",
      "Epoch 3/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9866 - mae: 1.9820 - val_loss: 132.2433 - val_mae: 7.4971\n",
      "Epoch 4/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9619 - mae: 1.9532 - val_loss: 133.3500 - val_mae: 7.3964\n",
      "Epoch 5/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9253 - mae: 1.9660 - val_loss: 135.3522 - val_mae: 7.4021\n",
      "Epoch 6/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9816 - mae: 1.9534 - val_loss: 128.5741 - val_mae: 7.2190\n",
      "Epoch 7/75\n",
      "621/621 [==============================] - 2s 3ms/step - loss: 17.0070 - mae: 1.9749 - val_loss: 155.5136 - val_mae: 8.1077\n",
      "Epoch 8/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9437 - mae: 1.9616 - val_loss: 132.4956 - val_mae: 7.3460\n",
      "Epoch 9/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9037 - mae: 1.9645 - val_loss: 126.1159 - val_mae: 7.0768\n",
      "Epoch 10/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.7089 - mae: 1.9491 - val_loss: 125.0144 - val_mae: 7.1630\n",
      "Epoch 11/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6772 - mae: 1.9592 - val_loss: 140.5495 - val_mae: 7.6546\n",
      "Epoch 12/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.8230 - mae: 1.9568 - val_loss: 127.1388 - val_mae: 7.2615\n",
      "Epoch 13/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9013 - mae: 1.9616 - val_loss: 134.1702 - val_mae: 7.5554\n",
      "Epoch 14/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.8966 - mae: 1.9625 - val_loss: 134.7750 - val_mae: 7.5236\n",
      "Epoch 15/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6517 - mae: 1.9366 - val_loss: 123.3497 - val_mae: 7.1556\n",
      "Epoch 16/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6605 - mae: 1.9438 - val_loss: 136.3072 - val_mae: 7.5302\n",
      "Epoch 17/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.7652 - mae: 1.9467 - val_loss: 137.6562 - val_mae: 7.5751\n",
      "Epoch 18/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5526 - mae: 1.9321 - val_loss: 139.7883 - val_mae: 7.5612\n",
      "Epoch 19/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6395 - mae: 1.9544 - val_loss: 152.8601 - val_mae: 7.9058\n",
      "Epoch 20/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.7724 - mae: 1.9464 - val_loss: 139.5675 - val_mae: 7.5862\n",
      "Epoch 21/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5754 - mae: 1.9401 - val_loss: 138.3821 - val_mae: 7.5201\n",
      "Epoch 22/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.7342 - mae: 1.9556 - val_loss: 131.0202 - val_mae: 7.2986\n",
      "Epoch 23/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5990 - mae: 1.9580 - val_loss: 134.7811 - val_mae: 7.4549\n",
      "Epoch 24/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4331 - mae: 1.9290 - val_loss: 165.2814 - val_mae: 8.1707\n",
      "Epoch 25/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5354 - mae: 1.9345 - val_loss: 141.4209 - val_mae: 7.6803\n",
      "Epoch 26/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5186 - mae: 1.9557 - val_loss: 155.1832 - val_mae: 8.0166\n",
      "Epoch 27/75\n",
      "621/621 [==============================] - 2s 3ms/step - loss: 16.5226 - mae: 1.9418 - val_loss: 132.3844 - val_mae: 7.2114\n",
      "Epoch 28/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5136 - mae: 1.9510 - val_loss: 134.8258 - val_mae: 7.4986\n",
      "Epoch 29/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3623 - mae: 1.9461 - val_loss: 146.8612 - val_mae: 7.7624\n",
      "Epoch 30/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2732 - mae: 1.9208 - val_loss: 136.8821 - val_mae: 7.5034\n",
      "Epoch 31/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5236 - mae: 1.9344 - val_loss: 144.6432 - val_mae: 7.7759\n",
      "Epoch 32/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.4639 - mae: 1.9546 - val_loss: 141.4186 - val_mae: 7.6468\n",
      "Epoch 33/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5633 - mae: 1.9672 - val_loss: 136.7662 - val_mae: 7.4585\n",
      "Epoch 34/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4271 - mae: 1.9369 - val_loss: 136.1291 - val_mae: 7.5946\n",
      "Epoch 35/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3792 - mae: 1.9295 - val_loss: 155.4552 - val_mae: 8.0613\n",
      "Epoch 36/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5160 - mae: 1.9511 - val_loss: 140.5930 - val_mae: 7.7345\n",
      "Epoch 37/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3313 - mae: 1.9330 - val_loss: 142.1406 - val_mae: 7.7914\n",
      "Epoch 38/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3578 - mae: 1.9319 - val_loss: 158.5644 - val_mae: 8.1396\n",
      "Epoch 39/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2930 - mae: 1.9161 - val_loss: 141.3242 - val_mae: 7.6453\n",
      "Epoch 40/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1828 - mae: 1.9291 - val_loss: 132.2816 - val_mae: 7.3351\n",
      "Epoch 41/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4641 - mae: 1.9312 - val_loss: 145.7530 - val_mae: 7.9350\n",
      "Epoch 42/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3534 - mae: 1.9385 - val_loss: 145.2205 - val_mae: 7.7954\n",
      "Epoch 43/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1892 - mae: 1.9233 - val_loss: 146.3975 - val_mae: 7.8996\n",
      "Epoch 44/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2858 - mae: 1.9359 - val_loss: 145.0491 - val_mae: 7.7551\n",
      "Epoch 45/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2494 - mae: 1.9355 - val_loss: 136.3308 - val_mae: 7.3927\n",
      "Epoch 46/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2204 - mae: 1.9270 - val_loss: 153.2701 - val_mae: 8.1091\n",
      "Epoch 47/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2531 - mae: 1.9356 - val_loss: 141.4346 - val_mae: 7.7655\n",
      "Epoch 48/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1708 - mae: 1.9288 - val_loss: 141.9399 - val_mae: 7.8003\n",
      "Epoch 49/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1136 - mae: 1.9408 - val_loss: 149.6750 - val_mae: 8.0249\n",
      "Epoch 50/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1166 - mae: 1.9268 - val_loss: 178.3918 - val_mae: 8.6701\n",
      "Epoch 51/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3193 - mae: 1.9547 - val_loss: 145.4041 - val_mae: 7.8818\n",
      "Epoch 52/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1175 - mae: 1.9356 - val_loss: 143.8040 - val_mae: 7.9225\n",
      "Epoch 53/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1575 - mae: 1.9390 - val_loss: 140.9528 - val_mae: 7.7324\n",
      "Epoch 54/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9309 - mae: 1.9208 - val_loss: 151.7105 - val_mae: 8.0176\n",
      "Epoch 55/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9770 - mae: 1.9201 - val_loss: 138.0333 - val_mae: 7.7581\n",
      "Epoch 56/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2808 - mae: 1.9331 - val_loss: 141.8864 - val_mae: 7.7606\n",
      "Epoch 57/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.0816 - mae: 1.9354 - val_loss: 146.2952 - val_mae: 7.7961\n",
      "Epoch 58/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9105 - mae: 1.9037 - val_loss: 163.7466 - val_mae: 8.3991\n",
      "Epoch 59/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9580 - mae: 1.9290 - val_loss: 138.7806 - val_mae: 7.6255\n",
      "Epoch 60/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9952 - mae: 1.9070 - val_loss: 148.6998 - val_mae: 7.9336\n",
      "Epoch 61/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.0095 - mae: 1.9160 - val_loss: 170.9545 - val_mae: 8.4932\n",
      "Epoch 62/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.8965 - mae: 1.9264 - val_loss: 135.4363 - val_mae: 7.6694\n",
      "Epoch 63/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9492 - mae: 1.9081 - val_loss: 135.5839 - val_mae: 7.5821\n",
      "Epoch 64/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.8972 - mae: 1.9305 - val_loss: 137.0623 - val_mae: 7.6237\n",
      "Epoch 65/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.0667 - mae: 1.9316 - val_loss: 149.1237 - val_mae: 8.1283\n",
      "Epoch 66/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9006 - mae: 1.9156 - val_loss: 141.5202 - val_mae: 7.7118\n",
      "Epoch 67/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.8191 - mae: 1.9118 - val_loss: 167.6766 - val_mae: 8.4470\n",
      "Epoch 68/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.7952 - mae: 1.8987 - val_loss: 141.7027 - val_mae: 7.7026\n",
      "Epoch 69/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.7667 - mae: 1.9064 - val_loss: 124.6692 - val_mae: 7.4011\n",
      "Epoch 70/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.9713 - mae: 1.9277 - val_loss: 159.4645 - val_mae: 8.3096\n",
      "Epoch 71/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.7467 - mae: 1.9186 - val_loss: 152.0786 - val_mae: 8.2149\n",
      "Epoch 72/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.7409 - mae: 1.8987 - val_loss: 143.3237 - val_mae: 7.8409\n",
      "Epoch 73/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.6758 - mae: 1.9133 - val_loss: 140.4657 - val_mae: 7.7931\n",
      "Epoch 74/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8780 - mae: 1.9153 - val_loss: 144.3027 - val_mae: 7.8396\n",
      "Epoch 75/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.7611 - mae: 1.9085 - val_loss: 156.5146 - val_mae: 8.2634\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "MSE: 16.36305261226612 R²: 0.3248991866781178 MAE: 1.8736665856043497\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 9\n",
    "dia_siguiente = 10\n",
    "\n",
    "df_predicciones_10_11, df_inicio_actualizado, mse_10_11, r2_10_11, mae_10_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_10_11, \"R²:\", r2_10_11, \"MAE:\", mae_10_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.955\n",
      "Suma predicha:  102.16216373443604\n",
      "Desviación porcentual:  73.28837882187437 %\n",
      "Suma previsión:  112.6\n",
      "Desviación porcentual:  90.99313035365958 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_10_11 = df_predicciones_10_11['E_SIMEL'].sum()\n",
    "suma_predicha_10_11 = df_predicciones_10_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_10_11 = df_predicciones_10_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_10_11)\n",
    "print(\"Suma predicha: \", suma_predicha_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.8066 - mae: 1.9185 - val_loss: 163.0389 - val_mae: 8.3304\n",
      "Epoch 2/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.9389 - mae: 1.9363 - val_loss: 161.2475 - val_mae: 8.2916\n",
      "Epoch 3/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.9187 - mae: 1.9305 - val_loss: 146.6945 - val_mae: 7.9465\n",
      "Epoch 4/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.6610 - mae: 1.8900 - val_loss: 141.0400 - val_mae: 7.8420\n",
      "Epoch 5/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.6708 - mae: 1.9106 - val_loss: 161.0320 - val_mae: 8.2512\n",
      "Epoch 6/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.7087 - mae: 1.9123 - val_loss: 157.0636 - val_mae: 8.3384\n",
      "Epoch 7/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.8648 - mae: 1.9234 - val_loss: 162.6537 - val_mae: 8.2603\n",
      "Epoch 8/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5768 - mae: 1.8956 - val_loss: 147.6096 - val_mae: 7.9535\n",
      "Epoch 9/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.6825 - mae: 1.9190 - val_loss: 149.3769 - val_mae: 8.0151\n",
      "Epoch 10/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5825 - mae: 1.9158 - val_loss: 155.4002 - val_mae: 8.0682\n",
      "Epoch 11/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.6204 - mae: 1.9022 - val_loss: 131.1058 - val_mae: 7.3347\n",
      "Epoch 12/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4869 - mae: 1.8970 - val_loss: 183.7587 - val_mae: 8.8434\n",
      "Epoch 13/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5660 - mae: 1.9025 - val_loss: 151.8917 - val_mae: 8.0998\n",
      "Epoch 14/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.7824 - mae: 1.9194 - val_loss: 150.8866 - val_mae: 8.1076\n",
      "Epoch 15/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5303 - mae: 1.8926 - val_loss: 141.4129 - val_mae: 7.7851\n",
      "Epoch 16/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5820 - mae: 1.9036 - val_loss: 148.1394 - val_mae: 7.9343\n",
      "Epoch 17/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4728 - mae: 1.9017 - val_loss: 147.5512 - val_mae: 7.9782\n",
      "Epoch 18/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.6223 - mae: 1.9112 - val_loss: 163.6729 - val_mae: 8.4701\n",
      "Epoch 19/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5519 - mae: 1.9226 - val_loss: 142.0899 - val_mae: 7.9066\n",
      "Epoch 20/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5486 - mae: 1.8994 - val_loss: 152.6488 - val_mae: 8.0371\n",
      "Epoch 21/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4565 - mae: 1.9093 - val_loss: 149.6797 - val_mae: 7.9194\n",
      "Epoch 22/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5895 - mae: 1.9121 - val_loss: 144.5057 - val_mae: 7.7927\n",
      "Epoch 23/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5184 - mae: 1.8918 - val_loss: 175.4184 - val_mae: 8.7335\n",
      "Epoch 24/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3952 - mae: 1.9011 - val_loss: 152.6301 - val_mae: 7.9972\n",
      "Epoch 25/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4104 - mae: 1.8941 - val_loss: 166.8508 - val_mae: 8.5463\n",
      "Epoch 26/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4670 - mae: 1.8961 - val_loss: 153.8527 - val_mae: 8.0702\n",
      "Epoch 27/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4804 - mae: 1.9142 - val_loss: 156.8307 - val_mae: 8.2240\n",
      "Epoch 28/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5509 - mae: 1.9015 - val_loss: 153.6688 - val_mae: 8.0934\n",
      "Epoch 29/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.5438 - mae: 1.9074 - val_loss: 164.7215 - val_mae: 8.4476\n",
      "Epoch 30/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1558 - mae: 1.8751 - val_loss: 145.9514 - val_mae: 7.8977\n",
      "Epoch 31/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2205 - mae: 1.8769 - val_loss: 157.8314 - val_mae: 8.4192\n",
      "Epoch 32/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3231 - mae: 1.9158 - val_loss: 158.0988 - val_mae: 8.2501\n",
      "Epoch 33/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4192 - mae: 1.9025 - val_loss: 140.7738 - val_mae: 7.7320\n",
      "Epoch 34/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.6123 - mae: 1.9224 - val_loss: 160.8303 - val_mae: 8.2857\n",
      "Epoch 35/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2189 - mae: 1.8774 - val_loss: 173.1855 - val_mae: 8.4800\n",
      "Epoch 36/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3799 - mae: 1.9023 - val_loss: 151.8507 - val_mae: 8.1697\n",
      "Epoch 37/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2804 - mae: 1.8949 - val_loss: 159.8836 - val_mae: 8.3383\n",
      "Epoch 38/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3490 - mae: 1.8864 - val_loss: 164.5022 - val_mae: 8.4157\n",
      "Epoch 39/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3107 - mae: 1.8972 - val_loss: 161.2012 - val_mae: 8.2750\n",
      "Epoch 40/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3403 - mae: 1.8989 - val_loss: 157.7592 - val_mae: 8.2110\n",
      "Epoch 41/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2232 - mae: 1.8952 - val_loss: 141.5235 - val_mae: 7.8119\n",
      "Epoch 42/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2067 - mae: 1.8796 - val_loss: 179.0473 - val_mae: 8.6651\n",
      "Epoch 43/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1934 - mae: 1.8823 - val_loss: 155.0963 - val_mae: 8.1588\n",
      "Epoch 44/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2310 - mae: 1.8776 - val_loss: 149.8466 - val_mae: 7.9748\n",
      "Epoch 45/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9799 - mae: 1.8819 - val_loss: 149.3216 - val_mae: 8.0166\n",
      "Epoch 46/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2817 - mae: 1.8915 - val_loss: 149.3269 - val_mae: 8.1699\n",
      "Epoch 47/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.0978 - mae: 1.8739 - val_loss: 170.8769 - val_mae: 8.5326\n",
      "Epoch 48/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1681 - mae: 1.8902 - val_loss: 159.5976 - val_mae: 8.1519\n",
      "Epoch 49/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1606 - mae: 1.9052 - val_loss: 158.5046 - val_mae: 8.2791\n",
      "Epoch 50/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1375 - mae: 1.8965 - val_loss: 151.0245 - val_mae: 8.0560\n",
      "Epoch 51/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9757 - mae: 1.8673 - val_loss: 166.4319 - val_mae: 8.5312\n",
      "Epoch 52/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1079 - mae: 1.8926 - val_loss: 183.2037 - val_mae: 8.9375\n",
      "Epoch 53/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.2316 - mae: 1.8793 - val_loss: 156.6801 - val_mae: 8.2906\n",
      "Epoch 54/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9006 - mae: 1.8842 - val_loss: 176.2004 - val_mae: 8.7696\n",
      "Epoch 55/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9562 - mae: 1.8859 - val_loss: 206.0513 - val_mae: 9.6721\n",
      "Epoch 56/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9569 - mae: 1.8807 - val_loss: 174.2540 - val_mae: 8.7583\n",
      "Epoch 57/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.0685 - mae: 1.8864 - val_loss: 165.1742 - val_mae: 8.4646\n",
      "Epoch 58/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9755 - mae: 1.8739 - val_loss: 175.8947 - val_mae: 8.8065\n",
      "Epoch 59/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.1259 - mae: 1.8901 - val_loss: 163.3596 - val_mae: 8.5103\n",
      "Epoch 60/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7008 - mae: 1.8584 - val_loss: 181.7018 - val_mae: 8.9853\n",
      "Epoch 61/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.0482 - mae: 1.8938 - val_loss: 176.1718 - val_mae: 8.7886\n",
      "Epoch 62/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9181 - mae: 1.8846 - val_loss: 157.2108 - val_mae: 8.3010\n",
      "Epoch 63/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.8952 - mae: 1.8875 - val_loss: 168.1877 - val_mae: 8.7943\n",
      "Epoch 64/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.8755 - mae: 1.8747 - val_loss: 165.7489 - val_mae: 8.5190\n",
      "Epoch 65/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.0359 - mae: 1.8815 - val_loss: 144.7483 - val_mae: 7.8264\n",
      "Epoch 66/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9158 - mae: 1.8892 - val_loss: 158.0891 - val_mae: 8.3557\n",
      "Epoch 67/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7347 - mae: 1.8722 - val_loss: 162.8265 - val_mae: 8.4023\n",
      "Epoch 68/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7794 - mae: 1.8702 - val_loss: 170.6094 - val_mae: 8.4416\n",
      "Epoch 69/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.8184 - mae: 1.8780 - val_loss: 168.1664 - val_mae: 8.5935\n",
      "Epoch 70/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.8656 - mae: 1.8652 - val_loss: 158.4053 - val_mae: 8.2606\n",
      "Epoch 71/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.8198 - mae: 1.8744 - val_loss: 165.6908 - val_mae: 8.5201\n",
      "Epoch 72/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9239 - mae: 1.8852 - val_loss: 165.9074 - val_mae: 8.5958\n",
      "Epoch 73/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7977 - mae: 1.8793 - val_loss: 174.1740 - val_mae: 8.7867\n",
      "Epoch 74/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.6411 - mae: 1.8436 - val_loss: 163.0974 - val_mae: 8.4741\n",
      "Epoch 75/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7198 - mae: 1.8587 - val_loss: 154.0338 - val_mae: 8.1834\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "MSE: 74.28807881700612 R²: 0.14852405115862177 MAE: 3.974584727128347\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 10\n",
    "dia_siguiente = 13\n",
    "\n",
    "df_predicciones_13_11, df_inicio_actualizado, mse_13_11, r2_13_11, mae_13_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_13_11, \"R²:\", r2_13_11, \"MAE:\", mae_13_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  107.141\n",
      "Suma predicha:  33.22418928146362\n",
      "Desviación porcentual:  -68.9902191677662 %\n",
      "Suma previsión:  20.900000000000002\n",
      "Desviación porcentual:  -80.492995211917 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_13_11 = df_predicciones_13_11['E_SIMEL'].sum()\n",
    "suma_predicha_13_11 = df_predicciones_13_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_13_11 = df_predicciones_13_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_13_11)\n",
    "print(\"Suma predicha: \", suma_predicha_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7585 - mae: 1.8817 - val_loss: 179.5751 - val_mae: 8.8684\n",
      "Epoch 2/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7321 - mae: 1.8767 - val_loss: 146.1567 - val_mae: 8.0329\n",
      "Epoch 3/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7307 - mae: 1.8695 - val_loss: 171.2052 - val_mae: 8.6894\n",
      "Epoch 4/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7981 - mae: 1.8739 - val_loss: 152.4395 - val_mae: 8.0938\n",
      "Epoch 5/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5472 - mae: 1.8566 - val_loss: 147.9252 - val_mae: 8.1195\n",
      "Epoch 6/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.6984 - mae: 1.8775 - val_loss: 170.1491 - val_mae: 8.5346\n",
      "Epoch 7/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5215 - mae: 1.8515 - val_loss: 178.5178 - val_mae: 8.7208\n",
      "Epoch 8/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9470 - mae: 1.8899 - val_loss: 162.8212 - val_mae: 8.4828\n",
      "Epoch 9/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.6633 - mae: 1.8557 - val_loss: 189.8663 - val_mae: 9.1002\n",
      "Epoch 10/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7123 - mae: 1.8602 - val_loss: 175.2077 - val_mae: 8.7114\n",
      "Epoch 11/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4095 - mae: 1.8711 - val_loss: 166.8151 - val_mae: 8.6405\n",
      "Epoch 12/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5813 - mae: 1.8393 - val_loss: 163.1097 - val_mae: 8.4825\n",
      "Epoch 13/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5607 - mae: 1.8487 - val_loss: 164.0139 - val_mae: 8.3530\n",
      "Epoch 14/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4740 - mae: 1.8660 - val_loss: 165.8050 - val_mae: 8.5181\n",
      "Epoch 15/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5154 - mae: 1.8764 - val_loss: 165.1966 - val_mae: 8.6265\n",
      "Epoch 16/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5676 - mae: 1.8671 - val_loss: 158.8273 - val_mae: 8.5040\n",
      "Epoch 17/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.7098 - mae: 1.8691 - val_loss: 169.4559 - val_mae: 8.4513\n",
      "Epoch 18/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4701 - mae: 1.8558 - val_loss: 167.1158 - val_mae: 8.5092\n",
      "Epoch 19/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4548 - mae: 1.8574 - val_loss: 167.5130 - val_mae: 8.5139\n",
      "Epoch 20/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4953 - mae: 1.8783 - val_loss: 172.6494 - val_mae: 8.7016\n",
      "Epoch 21/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5094 - mae: 1.8684 - val_loss: 162.3285 - val_mae: 8.3181\n",
      "Epoch 22/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3671 - mae: 1.8539 - val_loss: 170.0104 - val_mae: 8.7041\n",
      "Epoch 23/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5176 - mae: 1.8523 - val_loss: 164.8072 - val_mae: 8.6746\n",
      "Epoch 24/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5600 - mae: 1.8752 - val_loss: 169.1001 - val_mae: 8.5869\n",
      "Epoch 25/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3402 - mae: 1.8443 - val_loss: 190.7306 - val_mae: 9.1976\n",
      "Epoch 26/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3087 - mae: 1.8324 - val_loss: 186.4305 - val_mae: 9.0315\n",
      "Epoch 27/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.2999 - mae: 1.8468 - val_loss: 171.4156 - val_mae: 8.5970\n",
      "Epoch 28/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4770 - mae: 1.8516 - val_loss: 168.2229 - val_mae: 8.6212\n",
      "Epoch 29/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.5252 - mae: 1.8650 - val_loss: 193.8703 - val_mae: 9.2550\n",
      "Epoch 30/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4320 - mae: 1.8625 - val_loss: 179.5674 - val_mae: 8.9003\n",
      "Epoch 31/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3446 - mae: 1.8437 - val_loss: 175.3398 - val_mae: 8.8193\n",
      "Epoch 32/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1463 - mae: 1.8424 - val_loss: 180.2751 - val_mae: 8.9470\n",
      "Epoch 33/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3677 - mae: 1.8709 - val_loss: 165.8795 - val_mae: 8.5135\n",
      "Epoch 34/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1111 - mae: 1.8493 - val_loss: 175.6933 - val_mae: 8.7193\n",
      "Epoch 35/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3634 - mae: 1.8586 - val_loss: 160.6756 - val_mae: 8.3480\n",
      "Epoch 36/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1552 - mae: 1.8258 - val_loss: 168.4834 - val_mae: 8.5797\n",
      "Epoch 37/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.2485 - mae: 1.8590 - val_loss: 169.8927 - val_mae: 8.4585\n",
      "Epoch 38/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1296 - mae: 1.8325 - val_loss: 158.6330 - val_mae: 8.3030\n",
      "Epoch 39/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.2566 - mae: 1.8280 - val_loss: 177.2769 - val_mae: 8.7094\n",
      "Epoch 40/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.2468 - mae: 1.8397 - val_loss: 164.1105 - val_mae: 8.4764\n",
      "Epoch 41/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1714 - mae: 1.8375 - val_loss: 173.6039 - val_mae: 8.7930\n",
      "Epoch 42/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.9471 - mae: 1.8165 - val_loss: 167.9408 - val_mae: 8.5171\n",
      "Epoch 43/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3615 - mae: 1.8499 - val_loss: 171.6332 - val_mae: 8.6757\n",
      "Epoch 44/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3047 - mae: 1.8629 - val_loss: 177.3931 - val_mae: 8.8286\n",
      "Epoch 45/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4304 - mae: 1.8776 - val_loss: 177.4980 - val_mae: 8.8475\n",
      "Epoch 46/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1042 - mae: 1.8305 - val_loss: 173.4759 - val_mae: 8.6889\n",
      "Epoch 47/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0270 - mae: 1.8306 - val_loss: 170.7418 - val_mae: 8.6306\n",
      "Epoch 48/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1857 - mae: 1.8341 - val_loss: 163.4482 - val_mae: 8.5130\n",
      "Epoch 49/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1751 - mae: 1.8522 - val_loss: 169.1208 - val_mae: 8.6106\n",
      "Epoch 50/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.9923 - mae: 1.8099 - val_loss: 178.7012 - val_mae: 8.7248\n",
      "Epoch 51/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1980 - mae: 1.8489 - val_loss: 172.1603 - val_mae: 8.6225\n",
      "Epoch 52/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0883 - mae: 1.8581 - val_loss: 180.2359 - val_mae: 8.7414\n",
      "Epoch 53/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.9089 - mae: 1.8348 - val_loss: 175.3589 - val_mae: 8.9103\n",
      "Epoch 54/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1622 - mae: 1.8437 - val_loss: 176.8298 - val_mae: 8.7885\n",
      "Epoch 55/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1347 - mae: 1.8349 - val_loss: 188.4579 - val_mae: 9.1243\n",
      "Epoch 56/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0347 - mae: 1.8398 - val_loss: 179.9648 - val_mae: 8.8355\n",
      "Epoch 57/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.9854 - mae: 1.8505 - val_loss: 166.5284 - val_mae: 8.5106\n",
      "Epoch 58/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8708 - mae: 1.8333 - val_loss: 172.6386 - val_mae: 8.6507\n",
      "Epoch 59/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.9626 - mae: 1.8456 - val_loss: 179.5609 - val_mae: 8.9570\n",
      "Epoch 60/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0212 - mae: 1.8350 - val_loss: 202.8776 - val_mae: 9.5081\n",
      "Epoch 61/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8159 - mae: 1.8255 - val_loss: 172.4598 - val_mae: 8.7401\n",
      "Epoch 62/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0488 - mae: 1.8437 - val_loss: 168.5931 - val_mae: 8.5671\n",
      "Epoch 63/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0158 - mae: 1.8441 - val_loss: 191.9198 - val_mae: 9.1436\n",
      "Epoch 64/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8759 - mae: 1.8235 - val_loss: 174.1155 - val_mae: 8.7276\n",
      "Epoch 65/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.0592 - mae: 1.8265 - val_loss: 167.8294 - val_mae: 8.4449\n",
      "Epoch 66/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8836 - mae: 1.8257 - val_loss: 176.2816 - val_mae: 8.8025\n",
      "Epoch 67/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8094 - mae: 1.8260 - val_loss: 174.0611 - val_mae: 8.7069\n",
      "Epoch 68/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8547 - mae: 1.8298 - val_loss: 171.1033 - val_mae: 8.6430\n",
      "Epoch 69/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.7316 - mae: 1.8237 - val_loss: 195.2292 - val_mae: 9.2215\n",
      "Epoch 70/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8210 - mae: 1.8224 - val_loss: 174.6501 - val_mae: 8.7381\n",
      "Epoch 71/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.7542 - mae: 1.8142 - val_loss: 193.6951 - val_mae: 9.2806\n",
      "Epoch 72/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8407 - mae: 1.8244 - val_loss: 187.1656 - val_mae: 9.0677\n",
      "Epoch 73/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.9005 - mae: 1.8378 - val_loss: 196.2151 - val_mae: 9.2222\n",
      "Epoch 74/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.6763 - mae: 1.8187 - val_loss: 177.3849 - val_mae: 8.6975\n",
      "Epoch 75/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 13.8783 - mae: 1.8417 - val_loss: 182.2487 - val_mae: 9.0238\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "MSE: 40.2503954394115 R²: 0.5724760232089097 MAE: 2.891280995051066\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 13\n",
    "dia_siguiente = 14\n",
    "\n",
    "df_predicciones_14_11, df_inicio_actualizado, mse_14_11, r2_14_11, mae_14_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_14_11, \"R²:\", r2_14_11, \"MAE:\", mae_14_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  112.33099999999999\n",
      "Suma predicha:  58.250741958618164\n",
      "Desviación porcentual:  -48.14366296158837 %\n",
      "Suma previsión:  120.5\n",
      "Desviación porcentual:  7.272257880727503 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_14_11 = df_predicciones_14_11['E_SIMEL'].sum()\n",
    "suma_predicha_14_11 = df_predicciones_14_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_14_11 = df_predicciones_14_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_14_11)\n",
    "print(\"Suma predicha: \", suma_predicha_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.8809 - mae: 1.8425 - val_loss: 182.1708 - val_mae: 8.9742\n",
      "Epoch 2/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6229 - mae: 1.8110 - val_loss: 185.6766 - val_mae: 9.0538\n",
      "Epoch 3/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.8163 - mae: 1.8354 - val_loss: 163.5432 - val_mae: 8.4617\n",
      "Epoch 4/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7686 - mae: 1.8196 - val_loss: 182.5052 - val_mae: 8.9682\n",
      "Epoch 5/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7886 - mae: 1.8260 - val_loss: 177.2149 - val_mae: 8.8141\n",
      "Epoch 6/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.9202 - mae: 1.8525 - val_loss: 179.0961 - val_mae: 8.7610\n",
      "Epoch 7/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7944 - mae: 1.8190 - val_loss: 193.7831 - val_mae: 9.2761\n",
      "Epoch 8/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7614 - mae: 1.8195 - val_loss: 180.1442 - val_mae: 8.8492\n",
      "Epoch 9/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6765 - mae: 1.8151 - val_loss: 167.1158 - val_mae: 8.4568\n",
      "Epoch 10/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6866 - mae: 1.8412 - val_loss: 171.5940 - val_mae: 8.7225\n",
      "Epoch 11/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6448 - mae: 1.8221 - val_loss: 162.2458 - val_mae: 8.4541\n",
      "Epoch 12/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.8721 - mae: 1.8318 - val_loss: 218.4357 - val_mae: 9.6820\n",
      "Epoch 13/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7671 - mae: 1.8184 - val_loss: 167.6710 - val_mae: 8.5356\n",
      "Epoch 14/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7466 - mae: 1.8183 - val_loss: 167.3694 - val_mae: 8.5873\n",
      "Epoch 15/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.8503 - mae: 1.8310 - val_loss: 183.0805 - val_mae: 8.8708\n",
      "Epoch 16/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5738 - mae: 1.8127 - val_loss: 195.3106 - val_mae: 9.2643\n",
      "Epoch 17/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7255 - mae: 1.8222 - val_loss: 193.9386 - val_mae: 9.1900\n",
      "Epoch 18/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6758 - mae: 1.8223 - val_loss: 181.5463 - val_mae: 8.8456\n",
      "Epoch 19/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5865 - mae: 1.8127 - val_loss: 181.9266 - val_mae: 8.9611\n",
      "Epoch 20/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6403 - mae: 1.8378 - val_loss: 199.8254 - val_mae: 9.3464\n",
      "Epoch 21/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6530 - mae: 1.8220 - val_loss: 171.8627 - val_mae: 8.6308\n",
      "Epoch 22/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5379 - mae: 1.8200 - val_loss: 176.4133 - val_mae: 8.9299\n",
      "Epoch 23/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6834 - mae: 1.8231 - val_loss: 169.9503 - val_mae: 8.5481\n",
      "Epoch 24/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7890 - mae: 1.8279 - val_loss: 193.1889 - val_mae: 9.4503\n",
      "Epoch 25/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5154 - mae: 1.8131 - val_loss: 175.2075 - val_mae: 8.8613\n",
      "Epoch 26/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4405 - mae: 1.8000 - val_loss: 194.5429 - val_mae: 9.2574\n",
      "Epoch 27/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5382 - mae: 1.8158 - val_loss: 186.7651 - val_mae: 8.9711\n",
      "Epoch 28/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7387 - mae: 1.8211 - val_loss: 186.7251 - val_mae: 8.9838\n",
      "Epoch 29/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5989 - mae: 1.8067 - val_loss: 180.6233 - val_mae: 8.8276\n",
      "Epoch 30/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4706 - mae: 1.8115 - val_loss: 174.2711 - val_mae: 8.8368\n",
      "Epoch 31/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4753 - mae: 1.8287 - val_loss: 169.3813 - val_mae: 8.7902\n",
      "Epoch 32/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5356 - mae: 1.8176 - val_loss: 166.1955 - val_mae: 8.5048\n",
      "Epoch 33/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5398 - mae: 1.8166 - val_loss: 190.5614 - val_mae: 9.1062\n",
      "Epoch 34/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5709 - mae: 1.8272 - val_loss: 174.2039 - val_mae: 8.6732\n",
      "Epoch 35/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3610 - mae: 1.8040 - val_loss: 180.6724 - val_mae: 8.7974\n",
      "Epoch 36/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3006 - mae: 1.7911 - val_loss: 174.5259 - val_mae: 8.7712\n",
      "Epoch 37/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5502 - mae: 1.8096 - val_loss: 201.3098 - val_mae: 9.3470\n",
      "Epoch 38/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3747 - mae: 1.8107 - val_loss: 200.6275 - val_mae: 9.4606\n",
      "Epoch 39/75\n",
      "623/623 [==============================] - 2s 3ms/step - loss: 13.4602 - mae: 1.8144 - val_loss: 171.1779 - val_mae: 8.5520\n",
      "Epoch 40/75\n",
      "623/623 [==============================] - 2s 2ms/step - loss: 13.3291 - mae: 1.8000 - val_loss: 177.7399 - val_mae: 8.8177\n",
      "Epoch 41/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3715 - mae: 1.8064 - val_loss: 170.7395 - val_mae: 8.5426\n",
      "Epoch 42/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4592 - mae: 1.8012 - val_loss: 186.4378 - val_mae: 8.9784\n",
      "Epoch 43/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5285 - mae: 1.8120 - val_loss: 188.0460 - val_mae: 9.0027\n",
      "Epoch 44/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3241 - mae: 1.7945 - val_loss: 165.9532 - val_mae: 8.5472\n",
      "Epoch 45/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5570 - mae: 1.8099 - val_loss: 169.1389 - val_mae: 8.6213\n",
      "Epoch 46/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3607 - mae: 1.7998 - val_loss: 189.7365 - val_mae: 9.0614\n",
      "Epoch 47/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3693 - mae: 1.8046 - val_loss: 165.9723 - val_mae: 8.4881\n",
      "Epoch 48/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4392 - mae: 1.8103 - val_loss: 176.5319 - val_mae: 8.8898\n",
      "Epoch 49/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2519 - mae: 1.7963 - val_loss: 179.7351 - val_mae: 8.8511\n",
      "Epoch 50/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3084 - mae: 1.8118 - val_loss: 201.1904 - val_mae: 9.3620\n",
      "Epoch 51/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3104 - mae: 1.7898 - val_loss: 184.5724 - val_mae: 9.0032\n",
      "Epoch 52/75\n",
      "623/623 [==============================] - 2s 3ms/step - loss: 13.3127 - mae: 1.8006 - val_loss: 175.2835 - val_mae: 8.6839\n",
      "Epoch 53/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2940 - mae: 1.8015 - val_loss: 173.8091 - val_mae: 8.7945\n",
      "Epoch 54/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3544 - mae: 1.7894 - val_loss: 203.2720 - val_mae: 9.3556\n",
      "Epoch 55/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2226 - mae: 1.7989 - val_loss: 195.5953 - val_mae: 9.2580\n",
      "Epoch 56/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4416 - mae: 1.8096 - val_loss: 193.7879 - val_mae: 9.2028\n",
      "Epoch 57/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.0948 - mae: 1.7842 - val_loss: 177.0327 - val_mae: 8.7095\n",
      "Epoch 58/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3337 - mae: 1.7992 - val_loss: 195.2157 - val_mae: 9.3330\n",
      "Epoch 59/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2789 - mae: 1.8063 - val_loss: 179.8559 - val_mae: 8.8129\n",
      "Epoch 60/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2859 - mae: 1.8015 - val_loss: 201.4936 - val_mae: 9.3560\n",
      "Epoch 61/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2386 - mae: 1.7878 - val_loss: 181.4193 - val_mae: 8.8362\n",
      "Epoch 62/75\n",
      "623/623 [==============================] - 2s 3ms/step - loss: 13.1516 - mae: 1.7868 - val_loss: 185.7004 - val_mae: 8.8459\n",
      "Epoch 63/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.1817 - mae: 1.7892 - val_loss: 177.7527 - val_mae: 8.8755\n",
      "Epoch 64/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3371 - mae: 1.8000 - val_loss: 193.0101 - val_mae: 9.2147\n",
      "Epoch 65/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2759 - mae: 1.8122 - val_loss: 179.7869 - val_mae: 8.8353\n",
      "Epoch 66/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3720 - mae: 1.8204 - val_loss: 194.1340 - val_mae: 9.2119\n",
      "Epoch 67/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.1317 - mae: 1.7892 - val_loss: 189.5559 - val_mae: 9.0398\n",
      "Epoch 68/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.0464 - mae: 1.7982 - val_loss: 190.8190 - val_mae: 9.1751\n",
      "Epoch 69/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3020 - mae: 1.8093 - val_loss: 176.8224 - val_mae: 8.6021\n",
      "Epoch 70/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3275 - mae: 1.8130 - val_loss: 187.4397 - val_mae: 9.0712\n",
      "Epoch 71/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.1775 - mae: 1.7950 - val_loss: 193.3066 - val_mae: 9.0829\n",
      "Epoch 72/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.1247 - mae: 1.7984 - val_loss: 210.5943 - val_mae: 9.5410\n",
      "Epoch 73/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3264 - mae: 1.8045 - val_loss: 190.8493 - val_mae: 9.2208\n",
      "Epoch 74/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.0925 - mae: 1.7884 - val_loss: 199.2328 - val_mae: 9.2841\n",
      "Epoch 75/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.0296 - mae: 1.7694 - val_loss: 167.7979 - val_mae: 8.5472\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "MSE: 60.699817202375264 R²: -0.18668227059698128 MAE: 4.080462739308675\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 14\n",
    "dia_siguiente = 15\n",
    "\n",
    "df_predicciones_15_11, df_inicio_actualizado, mse_15_11, r2_15_11, mae_15_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_15_11, \"R²:\", r2_15_11, \"MAE:\", mae_15_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  88.333\n",
      "Suma predicha:  72.01645565032959\n",
      "Desviación porcentual:  -18.471629345397993 %\n",
      "Suma previsión:  131.89999999999998\n",
      "Desviación porcentual:  49.32131819365354 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_15_11 = df_predicciones_15_11['E_SIMEL'].sum()\n",
    "suma_predicha_15_11 = df_predicciones_15_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_15_11 = df_predicciones_15_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_15_11)\n",
    "print(\"Suma predicha: \", suma_predicha_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todos los Dataframes que contienen las prediccions, previsiones y datos reales para calcular las méstricas en conjunto\n",
    "\n",
    "df_predicciones_totales = pd.concat([df_final_05_11, df_final_06_11, df_predicciones_07_11, \n",
    "                                     df_predicciones_08_11, df_predicciones_09_11,df_predicciones_10_11, \n",
    "                                     df_predicciones_13_11, df_predicciones_14_11, df_predicciones_15_11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Predicciones:  2.5800903688437407\n",
      "MSE Predicciones:  34.08994007223815\n",
      "R² Predicciones:  0.32598771009603866\n",
      "MAE Previsiones:  2.3999490740740743\n",
      "MSE Previsiones:  35.93822944907407\n",
      "R² Previsiones:  0.2894440918718205\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de las métricas para ver si nos indican si mejoran los errores entren la predicción (Prediccion_E_SIMEL), previsión (PREVISION)y producción real(E_SIMEL)\n",
    "\n",
    "def calcular_metricas(df):\n",
    "    mae = mean_absolute_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    mse = mean_squared_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    return mae, mse, r2\n",
    "\n",
    "# Métricas para la predicciones\n",
    "\n",
    "mae_pred, mse_pred, r2_pred = calcular_metricas(df_predicciones_totales)\n",
    "\n",
    "# Cambiamos la columna de predicción por la de previsión\n",
    "\n",
    "df_previsiones = df_predicciones_totales.copy()\n",
    "df_previsiones['Prediccion_E_SIMEL'] = df_previsiones['PREVISION']\n",
    "\n",
    "# Métricas para la previsión\n",
    "\n",
    "mae_prev, mse_prev, r2_prev = calcular_metricas(df_previsiones)\n",
    "\n",
    "# Visualizamos los resultados\n",
    "\n",
    "print(\"MAE Predicciones: \", mae_pred)\n",
    "print(\"MSE Predicciones: \", mse_pred)\n",
    "print(\"R² Predicciones: \", r2_pred)\n",
    "print(\"MAE Previsiones: \", mae_prev)\n",
    "print(\"MSE Previsiones: \", mse_prev)\n",
    "print(\"R² Previsiones: \", r2_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLas métricas visualizadas nos indican que el modelo utilizado mejoran las métricas de las previsiones, vamos \\na ver a continuación si las métricas reflejan esta mejora en predicción respecto la previsión.\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Las métricas visualizadas nos indican que el modelo utilizado mejoran las métricas de las previsiones, vamos \n",
    "a ver a continuación si las métricas reflejan esta mejora en predicción respecto la previsión.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de los valores en la columna E_SIMEL: 622.953\n",
      "Suma de las predicciones: 525.4325928986073\n",
      "Suma de las previsiones : 696.6\n",
      "Diferencia entre predicciones totales y E_SIMEL total: 97.52040710139272\n",
      "Diferencia entre previsiones y E_SIMEL total: 73.64700000000005\n",
      "No mejoramos la predicción respecto la PREVISION real en: 23.873407101392672, por lo tanto, con este modelo, no estamos mejorando las previsiones.\n"
     ]
    }
   ],
   "source": [
    "# sumamos todos los valores de las columnas que queremo comparar\n",
    "\n",
    "suma_e_simel = df_predicciones_totales['E_SIMEL'].sum()\n",
    "sumas_totales_predicciones = df_predicciones_totales['Prediccion_E_SIMEL'].sum()\n",
    "sumas_previsiones = df_predicciones_totales['PREVISION'].sum()\n",
    "\n",
    "\n",
    "# Calculamos las diferencias entre la prediccion y la previsión respecto la producción real E_SIMEL\n",
    "\n",
    "diferencia_prediccion_vs_produccion_real = abs(sumas_totales_predicciones - suma_e_simel)\n",
    "diferencia_prevision_vs_produccion_real = abs(sumas_previsiones - suma_e_simel)\n",
    "\n",
    "\n",
    "# Imprimimos los resultados para poder visualizar si mejoramos las previsiones a lo largo de todas las predicciones.\n",
    "\n",
    "print(f\"Suma de los valores en la columna E_SIMEL: {suma_e_simel}\")\n",
    "print(f\"Suma de las predicciones: {sumas_totales_predicciones}\")\n",
    "print(f\"Suma de las previsiones : {sumas_previsiones}\")\n",
    "\n",
    "\n",
    "print(f\"Diferencia entre predicciones totales y E_SIMEL total: {diferencia_prediccion_vs_produccion_real}\")\n",
    "print(f\"Diferencia entre previsiones y E_SIMEL total: {diferencia_prevision_vs_produccion_real}\")\n",
    "\n",
    "\n",
    "# Calculamos la diferencia entre la predicción y la previsión para saber si el modelo de predicción mejora la previsión\n",
    "\n",
    "diferencia = diferencia_prediccion_vs_produccion_real - diferencia_prevision_vs_produccion_real\n",
    "\n",
    "if diferencia_prediccion_vs_produccion_real > diferencia_prevision_vs_produccion_real:\n",
    "    print(f\"No mejoramos la predicción respecto la PREVISION real en: {diferencia}, por lo tanto, con este modelo, no estamos mejorando las previsiones.\")\n",
    "else:\n",
    "    print(f\"La predicción es MEJOR que la previsión en: {-diferencia} unidades, por lo tanto, cumplimos nuestro objetivo de mejorar la PREVISIÓN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valores que cogemos para el gráfico\n",
    "\n",
    "categorias = ['E_SIMEL Total', 'Predicciones Total', 'Previsiones Total']\n",
    "valores = [suma_e_simel, sumas_totales_predicciones, sumas_previsiones]\n",
    "\n",
    "# Creamos un gráfico de barras\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barra = plt.bar(categorias, valores, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Añadimos las etiqutas\n",
    "\n",
    "for rect in barra:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Título del gráfico y ponemos las etiquteas a cada barra\n",
    "    \n",
    "plt.title('Comparación de la Producción Real, Predicciones y Previsiones Totales')\n",
    "plt.xlabel('Categorías')\n",
    "plt.ylabel('Valor Total')\n",
    "\n",
    "\n",
    "# Ubicamos el texto de la diferencia\n",
    "\n",
    "pos_y = valores[1] / 2\n",
    "pos_x = categorias[1]\n",
    "plt.text(pos_x, pos_y, f'No mejoramos la predicción respecto a la previsión en\\n{diferencia:.2f} unidades', ha='center', va='center', fontsize=12, color='black', bbox=dict(facecolor='green', alpha=0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696.5999999999999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_prevision = df_final['PREVISION'].sum()\n",
    "suma_prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622.953"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_E_SIMEL = df_final['E_SIMEL'].sum()\n",
    "suma_E_SIMEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
