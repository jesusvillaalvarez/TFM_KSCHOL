{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Preprocesado y modelado\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import category_encoders\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import missingno\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Period</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>DESVIO</th>\n",
       "      <th>f_PREV_HIGH</th>\n",
       "      <th>f_PREV_LOW</th>\n",
       "      <th>f_RUN</th>\n",
       "      <th>Dia_Semana</th>\n",
       "      <th>Es_fin_semana</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Día</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Fecha  Period  PREVISION  E_SIMEL  DESVIO  f_PREV_HIGH  \\\n",
       "0           0  2021-01-01       1        0.0      0.0     0.0            0   \n",
       "1           1  2021-01-01       2        0.0      0.0     0.0            0   \n",
       "2           2  2021-01-01       3        0.0      0.0     0.0            0   \n",
       "3           3  2021-01-01       4        0.0      0.0     0.0            0   \n",
       "4           4  2021-01-01       5        0.0      0.0     0.0            0   \n",
       "\n",
       "   f_PREV_LOW  f_RUN  Dia_Semana  Es_fin_semana   Año  Mes  Día  \n",
       "0           0      0           4          False  2021    1    1  \n",
       "1           0      0           4          False  2021    1    1  \n",
       "2           0      0           4          False  2021    1    1  \n",
       "3           0      0           4          False  2021    1    1  \n",
       "4           0      0           4          False  2021    1    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargamos el archivo csv con los datos\n",
    "\n",
    "df_central = pd.read_csv(\"C:/Users/Windows 10/Desktop/MASTER DATASCIENCE/TFM/df_central_2_1.csv\")\n",
    "\n",
    "df_central.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas innecesarias\n",
    "\n",
    "df_central = df_central.drop(columns=['Unnamed: 0', 'DESVIO', 'f_PREV_HIGH', 'f_PREV_LOW'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna 'Fecha' a Datetime para hacer las división en dos dfs\n",
    "\n",
    "df_central['Fecha'] = pd.to_datetime(df_central['Fecha'])\n",
    "\n",
    "# Dividimos el DataFrame en dos según las fechas especificas\n",
    "\n",
    "df_inicio = df_central[df_central['Fecha'] <= '2023-10-31']\n",
    "df_final = df_central[df_central['Fecha'] >= '2023-11-05']\n",
    "\n",
    "# Eliminamos la columna 'Fecha' de ambos DataFrames para poder preparar el modelo de Deep Learning\n",
    "\n",
    "df_inicio = df_inicio.drop(columns=['Fecha'])\n",
    "df_final = df_final.drop(columns=['Fecha'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19764, 8), (4942, 8), (19764,), (4942,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparamos df_inicio eliminando la variables objetivo del conjunto de entrenamiento (X) y especificamos la variable objetivo del conjunto de prueba (y)\n",
    "\n",
    "X = df_inicio.drop('E_SIMEL', axis=1)\n",
    "y = df_inicio['E_SIMEL']\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizamos las características, paso necesario para el modelo de deep learning\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verificamos las dimensiones de los conjuntos de datos para asegurarnos de que todo está correcto\n",
    "\n",
    "(X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                576       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4801 (18.75 KB)\n",
      "Trainable params: 4801 (18.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/75\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "495/495 [==============================] - 2s 1ms/step - loss: 84.3720 - mae: 4.6760 - val_loss: 41.5248 - val_mae: 3.2022\n",
      "Epoch 2/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 39.2515 - mae: 3.0336 - val_loss: 38.7564 - val_mae: 2.9450\n",
      "Epoch 3/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 37.6200 - mae: 2.8624 - val_loss: 38.4239 - val_mae: 2.8128\n",
      "Epoch 4/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 37.1750 - mae: 2.7950 - val_loss: 38.5460 - val_mae: 2.7507\n",
      "Epoch 5/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.8577 - mae: 2.7546 - val_loss: 38.0327 - val_mae: 2.7922\n",
      "Epoch 6/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.5126 - mae: 2.7302 - val_loss: 38.2449 - val_mae: 2.7771\n",
      "Epoch 7/75\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.4795 - mae: 2.7111 - val_loss: 39.2612 - val_mae: 3.0472\n",
      "Epoch 8/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.3394 - mae: 2.7208 - val_loss: 37.7262 - val_mae: 2.7929\n",
      "Epoch 9/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.3639 - mae: 2.7212 - val_loss: 37.3819 - val_mae: 2.7473\n",
      "Epoch 10/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.3711 - mae: 2.7153 - val_loss: 37.7168 - val_mae: 2.7890\n",
      "Epoch 11/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 36.1101 - mae: 2.6993 - val_loss: 37.5907 - val_mae: 2.6573\n",
      "Epoch 12/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.7409 - mae: 2.6813 - val_loss: 37.4039 - val_mae: 2.6524\n",
      "Epoch 13/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.7675 - mae: 2.6839 - val_loss: 38.0047 - val_mae: 2.5957\n",
      "Epoch 14/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.7973 - mae: 2.6805 - val_loss: 37.4942 - val_mae: 2.6569\n",
      "Epoch 15/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.6831 - mae: 2.6814 - val_loss: 37.4004 - val_mae: 2.7210\n",
      "Epoch 16/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.4664 - mae: 2.6634 - val_loss: 37.1595 - val_mae: 2.6300\n",
      "Epoch 17/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.4993 - mae: 2.6663 - val_loss: 37.1515 - val_mae: 2.7045\n",
      "Epoch 18/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.2902 - mae: 2.6601 - val_loss: 38.1025 - val_mae: 2.6369\n",
      "Epoch 19/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 35.2741 - mae: 2.6727 - val_loss: 37.8750 - val_mae: 2.9099\n",
      "Epoch 20/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.9699 - mae: 2.6713 - val_loss: 36.8857 - val_mae: 2.6505\n",
      "Epoch 21/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.9313 - mae: 2.6572 - val_loss: 36.6301 - val_mae: 2.6923\n",
      "Epoch 22/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.8195 - mae: 2.6511 - val_loss: 36.8966 - val_mae: 2.7438\n",
      "Epoch 23/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.6849 - mae: 2.6506 - val_loss: 36.6800 - val_mae: 2.6979\n",
      "Epoch 24/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.5939 - mae: 2.6535 - val_loss: 36.2724 - val_mae: 2.7293\n",
      "Epoch 25/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.5383 - mae: 2.6518 - val_loss: 36.6825 - val_mae: 2.7181\n",
      "Epoch 26/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.3751 - mae: 2.6454 - val_loss: 36.2163 - val_mae: 2.7439\n",
      "Epoch 27/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.4994 - mae: 2.6671 - val_loss: 36.3291 - val_mae: 2.8157\n",
      "Epoch 28/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 34.1737 - mae: 2.6473 - val_loss: 35.9987 - val_mae: 2.7296\n",
      "Epoch 29/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.8288 - mae: 2.6389 - val_loss: 36.6125 - val_mae: 2.6298\n",
      "Epoch 30/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.9772 - mae: 2.6468 - val_loss: 35.9604 - val_mae: 2.6993\n",
      "Epoch 31/75\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.9539 - mae: 2.6485 - val_loss: 36.2797 - val_mae: 2.6913\n",
      "Epoch 32/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.5918 - mae: 2.6306 - val_loss: 36.0596 - val_mae: 2.7932\n",
      "Epoch 33/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.4478 - mae: 2.6434 - val_loss: 36.9437 - val_mae: 2.7022\n",
      "Epoch 34/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.3193 - mae: 2.6371 - val_loss: 35.4703 - val_mae: 2.7630\n",
      "Epoch 35/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.3268 - mae: 2.6264 - val_loss: 35.5266 - val_mae: 2.7251\n",
      "Epoch 36/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.0917 - mae: 2.6215 - val_loss: 35.5460 - val_mae: 2.7877\n",
      "Epoch 37/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.1009 - mae: 2.6415 - val_loss: 36.3243 - val_mae: 2.8404\n",
      "Epoch 38/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 33.0107 - mae: 2.6282 - val_loss: 36.1078 - val_mae: 2.8298\n",
      "Epoch 39/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.7835 - mae: 2.6150 - val_loss: 35.1685 - val_mae: 2.6743\n",
      "Epoch 40/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.8215 - mae: 2.6326 - val_loss: 35.0444 - val_mae: 2.6503\n",
      "Epoch 41/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.5724 - mae: 2.6130 - val_loss: 35.6417 - val_mae: 2.8304\n",
      "Epoch 42/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.3896 - mae: 2.6088 - val_loss: 35.1688 - val_mae: 2.6674\n",
      "Epoch 43/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.3727 - mae: 2.6155 - val_loss: 35.5495 - val_mae: 2.8156\n",
      "Epoch 44/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.2853 - mae: 2.6236 - val_loss: 34.7644 - val_mae: 2.6880\n",
      "Epoch 45/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.0798 - mae: 2.6023 - val_loss: 36.9445 - val_mae: 2.6359\n",
      "Epoch 46/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 32.0490 - mae: 2.6087 - val_loss: 35.6307 - val_mae: 2.8060\n",
      "Epoch 47/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.6948 - mae: 2.5948 - val_loss: 34.7117 - val_mae: 2.7705\n",
      "Epoch 48/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.7025 - mae: 2.6046 - val_loss: 34.5503 - val_mae: 2.7128\n",
      "Epoch 49/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.6264 - mae: 2.6004 - val_loss: 33.9747 - val_mae: 2.6820\n",
      "Epoch 50/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.6358 - mae: 2.6059 - val_loss: 34.4236 - val_mae: 2.7372\n",
      "Epoch 51/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.4222 - mae: 2.5985 - val_loss: 34.1335 - val_mae: 2.7390\n",
      "Epoch 52/75\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.1834 - mae: 2.5784 - val_loss: 34.0341 - val_mae: 2.7521\n",
      "Epoch 53/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.1875 - mae: 2.5923 - val_loss: 33.8345 - val_mae: 2.6263\n",
      "Epoch 54/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.0073 - mae: 2.5869 - val_loss: 35.8218 - val_mae: 2.8691\n",
      "Epoch 55/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 31.0677 - mae: 2.5913 - val_loss: 34.2160 - val_mae: 2.7980\n",
      "Epoch 56/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.7266 - mae: 2.5835 - val_loss: 34.0245 - val_mae: 2.6679\n",
      "Epoch 57/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.8841 - mae: 2.5869 - val_loss: 34.8281 - val_mae: 2.8358\n",
      "Epoch 58/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.7289 - mae: 2.5852 - val_loss: 33.5066 - val_mae: 2.6825\n",
      "Epoch 59/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.5612 - mae: 2.5789 - val_loss: 34.3131 - val_mae: 2.7725\n",
      "Epoch 60/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.4170 - mae: 2.5693 - val_loss: 33.9986 - val_mae: 2.7033\n",
      "Epoch 61/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.4854 - mae: 2.5732 - val_loss: 34.0699 - val_mae: 2.7969\n",
      "Epoch 62/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.2175 - mae: 2.5499 - val_loss: 33.9701 - val_mae: 2.7294\n",
      "Epoch 63/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.2508 - mae: 2.5683 - val_loss: 33.3072 - val_mae: 2.6444\n",
      "Epoch 64/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.1139 - mae: 2.5577 - val_loss: 33.6520 - val_mae: 2.7433\n",
      "Epoch 65/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.0756 - mae: 2.5562 - val_loss: 34.1055 - val_mae: 2.6488\n",
      "Epoch 66/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.0204 - mae: 2.5657 - val_loss: 33.4549 - val_mae: 2.6844\n",
      "Epoch 67/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.8000 - mae: 2.5548 - val_loss: 33.3504 - val_mae: 2.6237\n",
      "Epoch 68/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 30.0278 - mae: 2.5621 - val_loss: 33.6135 - val_mae: 2.7700\n",
      "Epoch 69/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.8211 - mae: 2.5751 - val_loss: 34.6882 - val_mae: 2.6884\n",
      "Epoch 70/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.7203 - mae: 2.5556 - val_loss: 32.9467 - val_mae: 2.6098\n",
      "Epoch 71/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.8240 - mae: 2.5769 - val_loss: 33.1652 - val_mae: 2.7193\n",
      "Epoch 72/75\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.4551 - mae: 2.5422 - val_loss: 33.3257 - val_mae: 2.7008\n",
      "Epoch 73/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.4386 - mae: 2.5522 - val_loss: 33.2050 - val_mae: 2.7576\n",
      "Epoch 74/75\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.4012 - mae: 2.5492 - val_loss: 33.6728 - val_mae: 2.6576\n",
      "Epoch 75/75\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 29.2031 - mae: 2.5324 - val_loss: 33.7586 - val_mae: 2.7605\n",
      "155/155 [==============================] - 0s 767us/step - loss: 31.6169 - mae: 2.7270\n",
      "Loss en el conjunto de prueba: 31.61685562133789, MAE: 2.7270309925079346\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Definimos la arquitectura del modelo, con dos capas ocultas de 64 unidades cada una con la función de activación 'relu', que proporciona\n",
    "# capacidad de modelado no lineal.\n",
    "# una capa de salida sin función de activación\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Capa de salida para predicción de un valor continuo\n",
    "])\n",
    "\n",
    "# Compilamos el modelo\n",
    "# utilizamos el optimizador 'adam' con la función de pérdida de Mean Square Error (mse) y la métrica mae\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Sacamos el resumen del modelo\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Entrenamos el modelo con 75 epochs y un batch_size de 32; visualizamos el proceso con verbose = 1\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=75, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluamos el conjunto de prueba\n",
    "\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}, MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imputador MICE entrenado con éxito.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Configuramos el imputador MICE con GradientBoostingRegressor y el estimador\n",
    "\n",
    "mice_imputer = IterativeImputer(estimator=GradientBoostingRegressor(\n",
    "                                    n_estimators=100,\n",
    "                                    max_depth=10,\n",
    "                                    min_samples_split=4,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    max_features='sqrt'),\n",
    "                                    max_iter=10, random_state=42)\n",
    "\n",
    "# Preparamos los datos para el entrenamiento del imputador MICE eliminando la variable objetivo E_SIMEL\n",
    "\n",
    "X_mice = df_inicio.drop(columns=['E_SIMEL'])\n",
    "\n",
    "# Entrenamos el imputador MICE\n",
    "\n",
    "mice_imputer.fit(X_mice)\n",
    "\n",
    "# Imprimimos confirmación\n",
    "\n",
    "\"Imputador MICE entrenado con éxito.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las filas del dia 5 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_05_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "\n",
    "df_final_05_11_para_imputar = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_05_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_05_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto y\n",
    "# establecemos una condición que si los valores son más grandes que 0.2 establecemos un 1, y si son inferiores establecemos un 0.\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_05_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.09970585],\n",
       "       [0.23204055],\n",
       "       [0.27280918],\n",
       "       [0.12085357],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.07039228],\n",
       "       [6.5706143 ],\n",
       "       [7.279591  ],\n",
       "       [7.2673693 ],\n",
       "       [6.052717  ],\n",
       "       [4.527865  ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos los datos de df_final_05_11 para la predicción quitando la variable objetivo\n",
    "\n",
    "X_final_05_11 = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalización de los datos\n",
    "\n",
    "X_final_05_11_scaled = scaler.transform(X_final_05_11)\n",
    "\n",
    "# Realizamos la predicciones de E_SIMEL con el modelo de deep learning\n",
    "\n",
    "e_simel_predicciones = model.predict(X_final_05_11_scaled)\n",
    "\n",
    "e_simel_predicciones = np.maximum(e_simel_predicciones, 0)\n",
    "\n",
    "# Mostramos los resultados de la predicción\n",
    "\n",
    "e_simel_predicciones[:25]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_70736\\496075226.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24802</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24803</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24804</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24805</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24806</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24807</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24808</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24809</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24810</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24811</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24812</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24813</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24814</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24815</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24816</th>\n",
       "      <td>4.122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.570614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24817</th>\n",
       "      <td>5.437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.279591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24818</th>\n",
       "      <td>6.378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.267369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24819</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.052717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24820</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.527865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24821</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24822</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24823</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24824</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24825</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24802    0.000        0.0            0.000000\n",
       "24803    0.000        0.0            0.000000\n",
       "24804    0.000        0.0            0.000000\n",
       "24805    0.000        0.0            0.000000\n",
       "24806    0.000        0.0            0.000000\n",
       "24807    0.000        0.0            0.099706\n",
       "24808    0.000        0.0            0.232041\n",
       "24809    0.000        0.0            0.272809\n",
       "24810    0.000        0.0            0.120854\n",
       "24811    0.000        0.0            0.000000\n",
       "24812    0.000        0.0            0.000000\n",
       "24813    0.000        0.0            0.000000\n",
       "24814    0.000        0.0            0.000000\n",
       "24815    0.000        0.0            0.070392\n",
       "24816    4.122        0.0            6.570614\n",
       "24817    5.437        0.0            7.279591\n",
       "24818    6.378        0.0            7.267369\n",
       "24819    0.000        0.0            6.052717\n",
       "24820    0.000        0.0            4.527865\n",
       "24821    0.000        0.0            0.000000\n",
       "24822    0.000        0.0            0.000000\n",
       "24823    0.000        0.0            0.000000\n",
       "24824    0.000        0.0            0.000000\n",
       "24825    0.000        0.0            0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos el array a una lista para facilitar la asignación a una nueva columna que llamamos 'Prediccion_E_SIMEL'\n",
    "\n",
    "predicciones_lista = e_simel_predicciones.flatten().tolist()\n",
    "\n",
    "# Asignar las predicciones a df_final_05_11\n",
    "\n",
    "df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Mostramos las primeras filas para verificar\n",
    "\n",
    "df_final_05_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  15.937000000000001\n",
      "Suma predicha:  32.49395826458931\n",
      "Desviación porcentual:  103.89005625016821 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# al igual que en todos los casos anteriores, hacemos un sumatorio y porcentaje de desviación para tener una primera idea de como van las predicciones\n",
    "\n",
    "suma_real_05 = df_final_05_11['E_SIMEL'].sum()\n",
    "suma_predicha_05 = df_final_05_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_05 = df_final_05_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_05)\n",
    "print(\"Suma predicha: \", suma_predicha_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.0296 - mae: 2.4212 - val_loss: 40.4497 - val_mae: 3.3711\n",
      "Epoch 2/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.6672 - mae: 2.4101 - val_loss: 40.7874 - val_mae: 3.3364\n",
      "Epoch 3/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.4614 - mae: 2.3735 - val_loss: 42.6523 - val_mae: 3.3496\n",
      "Epoch 4/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.3850 - mae: 2.4009 - val_loss: 43.3455 - val_mae: 3.5478\n",
      "Epoch 5/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.1732 - mae: 2.3812 - val_loss: 44.6780 - val_mae: 3.3988\n",
      "Epoch 6/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 27.0080 - mae: 2.3799 - val_loss: 45.0284 - val_mae: 3.6889\n",
      "Epoch 7/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.9944 - mae: 2.3690 - val_loss: 45.5487 - val_mae: 3.5983\n",
      "Epoch 8/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.6750 - mae: 2.3568 - val_loss: 44.9804 - val_mae: 3.6792\n",
      "Epoch 9/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.6343 - mae: 2.3650 - val_loss: 44.8600 - val_mae: 3.5652\n",
      "Epoch 10/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.5308 - mae: 2.3640 - val_loss: 45.1043 - val_mae: 3.6254\n",
      "Epoch 11/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.6835 - mae: 2.3818 - val_loss: 46.6228 - val_mae: 3.6728\n",
      "Epoch 12/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.2346 - mae: 2.3420 - val_loss: 46.9099 - val_mae: 3.5902\n",
      "Epoch 13/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.2500 - mae: 2.3505 - val_loss: 47.9220 - val_mae: 3.5710\n",
      "Epoch 14/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 26.2275 - mae: 2.3602 - val_loss: 46.2629 - val_mae: 3.5718\n",
      "Epoch 15/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.9698 - mae: 2.3376 - val_loss: 48.5318 - val_mae: 3.8202\n",
      "Epoch 16/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.8479 - mae: 2.3501 - val_loss: 47.1545 - val_mae: 3.6496\n",
      "Epoch 17/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.8181 - mae: 2.3588 - val_loss: 50.9960 - val_mae: 3.7428\n",
      "Epoch 18/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.5715 - mae: 2.3386 - val_loss: 50.1544 - val_mae: 3.8404\n",
      "Epoch 19/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.7653 - mae: 2.3556 - val_loss: 50.4142 - val_mae: 3.8273\n",
      "Epoch 20/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 25.3900 - mae: 2.3313 - val_loss: 49.8439 - val_mae: 3.7139\n",
      "Epoch 21/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.2901 - mae: 2.3320 - val_loss: 50.3234 - val_mae: 3.8883\n",
      "Epoch 22/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.5280 - mae: 2.3513 - val_loss: 50.8051 - val_mae: 3.7203\n",
      "Epoch 23/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.2480 - mae: 2.3408 - val_loss: 48.5650 - val_mae: 3.7555\n",
      "Epoch 24/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.2236 - mae: 2.3309 - val_loss: 49.9783 - val_mae: 3.8630\n",
      "Epoch 25/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.1943 - mae: 2.3279 - val_loss: 51.8514 - val_mae: 3.8456\n",
      "Epoch 26/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 25.0377 - mae: 2.3402 - val_loss: 50.4100 - val_mae: 3.8286\n",
      "Epoch 27/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.7578 - mae: 2.3068 - val_loss: 59.7741 - val_mae: 4.0354\n",
      "Epoch 28/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.8809 - mae: 2.3303 - val_loss: 54.6443 - val_mae: 3.9293\n",
      "Epoch 29/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.7156 - mae: 2.3141 - val_loss: 55.0679 - val_mae: 4.0274\n",
      "Epoch 30/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.7849 - mae: 2.3388 - val_loss: 50.8928 - val_mae: 3.8962\n",
      "Epoch 31/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.5341 - mae: 2.3148 - val_loss: 52.0955 - val_mae: 3.9586\n",
      "Epoch 32/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.4651 - mae: 2.3085 - val_loss: 55.5268 - val_mae: 4.3182\n",
      "Epoch 33/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.3523 - mae: 2.3175 - val_loss: 53.1723 - val_mae: 4.0656\n",
      "Epoch 34/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.4026 - mae: 2.3141 - val_loss: 53.0337 - val_mae: 4.0279\n",
      "Epoch 35/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.2116 - mae: 2.3160 - val_loss: 55.0836 - val_mae: 4.1009\n",
      "Epoch 36/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.2051 - mae: 2.3020 - val_loss: 54.2735 - val_mae: 3.9203\n",
      "Epoch 37/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 24.2235 - mae: 2.3184 - val_loss: 55.9842 - val_mae: 3.9548\n",
      "Epoch 38/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.9197 - mae: 2.2916 - val_loss: 62.9106 - val_mae: 4.1891\n",
      "Epoch 39/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 23.9197 - mae: 2.3011 - val_loss: 56.7606 - val_mae: 4.2491\n",
      "Epoch 40/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.9152 - mae: 2.3095 - val_loss: 57.4224 - val_mae: 4.0685\n",
      "Epoch 41/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.8323 - mae: 2.3136 - val_loss: 59.6452 - val_mae: 4.2017\n",
      "Epoch 42/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.8934 - mae: 2.3125 - val_loss: 60.0410 - val_mae: 4.1402\n",
      "Epoch 43/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5536 - mae: 2.2833 - val_loss: 57.0964 - val_mae: 4.1123\n",
      "Epoch 44/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5650 - mae: 2.2731 - val_loss: 64.0103 - val_mae: 4.3176\n",
      "Epoch 45/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5858 - mae: 2.2925 - val_loss: 58.8858 - val_mae: 4.2992\n",
      "Epoch 46/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.4695 - mae: 2.2940 - val_loss: 57.0588 - val_mae: 4.1221\n",
      "Epoch 47/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.5963 - mae: 2.2910 - val_loss: 64.2076 - val_mae: 4.3914\n",
      "Epoch 48/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.4702 - mae: 2.2859 - val_loss: 56.1764 - val_mae: 4.1396\n",
      "Epoch 49/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.3459 - mae: 2.2834 - val_loss: 60.3759 - val_mae: 4.1664\n",
      "Epoch 50/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.3151 - mae: 2.2839 - val_loss: 64.5731 - val_mae: 4.3961\n",
      "Epoch 51/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.3490 - mae: 2.2896 - val_loss: 59.0830 - val_mae: 4.2494\n",
      "Epoch 52/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.3366 - mae: 2.2975 - val_loss: 67.1653 - val_mae: 4.4271\n",
      "Epoch 53/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.2444 - mae: 2.2844 - val_loss: 59.2696 - val_mae: 4.1800\n",
      "Epoch 54/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.2883 - mae: 2.2893 - val_loss: 68.3701 - val_mae: 4.5102\n",
      "Epoch 55/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.0953 - mae: 2.2807 - val_loss: 63.9758 - val_mae: 4.3681\n",
      "Epoch 56/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.1071 - mae: 2.2764 - val_loss: 64.4853 - val_mae: 4.3350\n",
      "Epoch 57/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.0201 - mae: 2.2781 - val_loss: 65.2038 - val_mae: 4.4076\n",
      "Epoch 58/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 23.0509 - mae: 2.2806 - val_loss: 65.0952 - val_mae: 4.5027\n",
      "Epoch 59/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8300 - mae: 2.2724 - val_loss: 64.3632 - val_mae: 4.4260\n",
      "Epoch 60/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 23.0447 - mae: 2.2803 - val_loss: 63.4539 - val_mae: 4.3329\n",
      "Epoch 61/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.8902 - mae: 2.2658 - val_loss: 65.2847 - val_mae: 4.4360\n",
      "Epoch 62/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.9343 - mae: 2.2768 - val_loss: 63.6882 - val_mae: 4.3872\n",
      "Epoch 63/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.6144 - mae: 2.2511 - val_loss: 66.8594 - val_mae: 4.6146\n",
      "Epoch 64/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.6611 - mae: 2.2515 - val_loss: 64.2109 - val_mae: 4.4250\n",
      "Epoch 65/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.7477 - mae: 2.2693 - val_loss: 70.0661 - val_mae: 4.5650\n",
      "Epoch 66/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.6548 - mae: 2.2574 - val_loss: 60.9235 - val_mae: 4.4110\n",
      "Epoch 67/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.4881 - mae: 2.2527 - val_loss: 73.8954 - val_mae: 4.7134\n",
      "Epoch 68/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.4761 - mae: 2.2502 - val_loss: 72.2730 - val_mae: 4.6567\n",
      "Epoch 69/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 22.4482 - mae: 2.2406 - val_loss: 65.2725 - val_mae: 4.4338\n",
      "Epoch 70/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.3353 - mae: 2.2341 - val_loss: 65.5671 - val_mae: 4.4793\n",
      "Epoch 71/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.5789 - mae: 2.2557 - val_loss: 68.2045 - val_mae: 4.5459\n",
      "Epoch 72/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.3971 - mae: 2.2386 - val_loss: 69.0706 - val_mae: 4.5263\n",
      "Epoch 73/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.3158 - mae: 2.2445 - val_loss: 69.4119 - val_mae: 4.6157\n",
      "Epoch 74/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.3367 - mae: 2.2575 - val_loss: 63.4306 - val_mae: 4.3714\n",
      "Epoch 75/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 22.1502 - mae: 2.2302 - val_loss: 68.9395 - val_mae: 4.6298\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IterativeImputer</label><div class=\"sk-toggleable__content\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Una vez tenemos la primera predicción, actualizamos df_inicio con los datos del día 5. Así estamos simulando como sería\n",
    "# el proceso de predicción en tiempo real\n",
    "\n",
    "datos_dia_5 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "df_inicio_actualizado = pd.concat([df_inicio, datos_dia_5])\n",
    "\n",
    "# Preparamos df_actualizado con los datos de df_inicio y los datos del día 5 para reentrenar el modelo de deep learning\n",
    "\n",
    "X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "\n",
    "# Normalizamos con fit_transform\n",
    "\n",
    "X_total_scaled = scaler.fit_transform(X_actualizado)  # Utilizamos fit_transform \n",
    "\n",
    "# Y reentrenamos el modelo de deep learning con todos los datos con los parámetros utilizados para el entrenamiento inicial\n",
    "\n",
    "model.fit(X_total_scaled, y_actualizado, epochs=75, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# reentrenamos también el modelo imputador con los nuevos datos con todas las variables menos con E_SIMEL\n",
    "\n",
    "mice_imputer.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_70736\\3426074952.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24826</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24827</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24829</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24830</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24831</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24832</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24833</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24834</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24835</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24836</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24837</th>\n",
       "      <td>0.000</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.734871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>0.000</td>\n",
       "      <td>15.8</td>\n",
       "      <td>14.081008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24839</th>\n",
       "      <td>0.000</td>\n",
       "      <td>19.4</td>\n",
       "      <td>14.526280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24840</th>\n",
       "      <td>3.946</td>\n",
       "      <td>24.3</td>\n",
       "      <td>14.614648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24841</th>\n",
       "      <td>22.905</td>\n",
       "      <td>29.8</td>\n",
       "      <td>13.170194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24842</th>\n",
       "      <td>21.310</td>\n",
       "      <td>32.9</td>\n",
       "      <td>12.106301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24843</th>\n",
       "      <td>9.663</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.213801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24844</th>\n",
       "      <td>0.718</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.129489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24845</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24846</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24848</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24849</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24826    0.000        0.0            0.000000\n",
       "24827    0.000        0.0            0.000000\n",
       "24828    0.000        0.0            0.000000\n",
       "24829    0.000        0.0            0.000000\n",
       "24830    0.000        0.0            0.000000\n",
       "24831    0.000        0.0            0.000000\n",
       "24832    0.000        0.0            0.000000\n",
       "24833    0.000        0.0            0.000000\n",
       "24834    0.000        0.0            0.000000\n",
       "24835    0.000        0.0            0.000000\n",
       "24836    0.000        2.6            0.000000\n",
       "24837    0.000       11.6            8.734871\n",
       "24838    0.000       15.8           14.081008\n",
       "24839    0.000       19.4           14.526280\n",
       "24840    3.946       24.3           14.614648\n",
       "24841   22.905       29.8           13.170194\n",
       "24842   21.310       32.9           12.106301\n",
       "24843    9.663       19.0            9.213801\n",
       "24844    0.718        4.0            1.129489\n",
       "24845    0.000        0.0            0.000000\n",
       "24846    0.000        0.0            0.000000\n",
       "24847    0.000        0.0            0.000000\n",
       "24848    0.000        0.0            0.000000\n",
       "24849    0.000        0.0            0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seleccionamos las dilas del dia 6 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_06_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 6)]\n",
    "\n",
    "df_final_06_11_para_imputar = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_06_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_06_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_06_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n",
    "\n",
    "# Verificamos que los valores han sido imputados correctamente\n",
    "# df_final_05_11.head(25)\n",
    "\n",
    "# Preparar los datos de df_final_05_11 para la predicción\n",
    "X_final_06_11 = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalizar los datos de X_final_05_11 utilizando el mismo scaler que para los datos de entrenamiento\n",
    "X_final_06_11_scaled = scaler.transform(X_final_06_11)\n",
    "\n",
    "# Realizar las predicciones de E_SIMEL con el modelo de deep learning\n",
    "e_simel_predicciones_06 = model.predict(X_final_06_11_scaled)\n",
    "\n",
    "e_simel_predicciones_06 = np.maximum(e_simel_predicciones_06, 0)\n",
    "\n",
    "# Mostrar las primeras 5 predicciones\n",
    "# e_simel_predicciones[:25]\n",
    "\n",
    "# Convertir el array de predicciones a una lista para facilitar la asignación\n",
    "predicciones_lista = e_simel_predicciones_06.flatten().tolist()\n",
    "\n",
    "# Asignar las predicciones a df_final_05_11\n",
    "df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "df_final_06_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.542\n",
      "Suma predicha:  87.57659208774567\n",
      "Desviación porcentual:  49.596173837152236 %\n",
      "Suma previsión:  159.4\n",
      "Desviación porcentual:  172.2831471422227 %\n"
     ]
    }
   ],
   "source": [
    "suma_real_06 = df_final_06_11['E_SIMEL'].sum()\n",
    "suma_predicha_06 = df_final_06_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_06 = df_final_06_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_06)\n",
    "print(\"Suma predicha: \", suma_predicha_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un función para agilizar el proceso de actualización, reentreno de los modelos, imputación, predicción y cálculo de las métricas\n",
    "\n",
    "def predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente,  mes, año, df_inicio_actualizado, df_final, modelo_deep, imputador):\n",
    "    \"\"\"\n",
    "    Función para actualizar el conjunto de entrenamiento con los datos reales de un día específico,\n",
    "    realizar la imputación para el día siguiente y predecir los valores de E_SIMEL para ese día.\n",
    "\n",
    "    Args:\n",
    "    dia_actual (int): Día actual para el que se actualizarán los datos.\n",
    "    dia_siguiente (int): Datos del día que queremos hacer las imputaciones y la predicción\n",
    "    mes (int): Mes del día actual.\n",
    "    ano (int): Año del día actual.\n",
    "    df_inicio_actualizado (DataFrame): DataFrame actualizado con los datos hasta el día anterior.\n",
    "    df_final (DataFrame): DataFrame con los datos a predecir.\n",
    "    modelo_rf (RandomForestRegressor): Modelo de Random Forest entrenado.\n",
    "    imputador (IterativeImputer): Imputador MICE entrenado.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame con las predicciones para el día siguiente.\n",
    "    DataFrame: DataFrame actualizado con los datos reales del día actual.\n",
    "    \"\"\"\n",
    "    # Actualización de df_actualizado con los datos de dia_actual\n",
    "\n",
    "    datos_dia_actual = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_actual)]\n",
    "    df_inicio_actualizado = pd.concat([df_inicio_actualizado, datos_dia_actual])\n",
    "\n",
    "    # Reentrenamos los modelos\n",
    "\n",
    "    # Una vez tenemos la primera predicción, actualizamos df_inicio con los datos del día 5. Así estamos simulando como sería\n",
    "    # el proceso de predicción en tiempo real\n",
    "\n",
    "\n",
    "\n",
    "    # Preparar los datos para el modelo de deep learning\n",
    "    X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "    y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "    \n",
    "    \n",
    "    # Normalizar las características\n",
    "    X_actualizado_scaled = scaler.transform(X_actualizado)\n",
    "\n",
    "    # Reentrenar el modelo de deep learning con todos los datos\n",
    "    modelo_deep.fit(X_actualizado_scaled, y_actualizado, epochs=75, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "    imputador.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "      # Imputación de valores a la columna f_RUN\n",
    "\n",
    "    df_dia_siguiente = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_siguiente)]\n",
    "    df_dia_siguiente_para_imputar = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "    df_dia_siguiente_para_imputar[['f_RUN']] = np.nan  \n",
    "    \n",
    "    valores_imputados = imputador.transform(df_dia_siguiente_para_imputar)\n",
    "    \n",
    "    df_dia_siguiente.loc[:, 'f_RUN'] = np.where(valores_imputados[:, 2] > 0.2, 1, 0) \n",
    "\n",
    "    \n",
    "    \n",
    "    # Preparar los datos de df_final_05_11 para la predicción\n",
    "    X_prediccion = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "    # Normalizar los datos de X_final_05_11 utilizando el mismo scaler que para los datos de entrenamiento\n",
    "    X_prediccion_scaled = scaler.transform(X_prediccion)\n",
    "\n",
    "    # Realizar las predicciones de E_SIMEL con el modelo de deep learning\n",
    "    predicted_e_simel = model.predict(X_prediccion_scaled)\n",
    "\n",
    "    predicted_e_simel = np.maximum(predicted_e_simel, 0)\n",
    "\n",
    "    # Mostrar las primeras 5 predicciones\n",
    "    # e_simel_predicciones[:25]\n",
    "\n",
    "    # Convertir el array de predicciones a una lista para facilitar la asignación\n",
    "    predicciones_lista = predicted_e_simel.flatten().tolist()\n",
    "\n",
    "    df_predicciones = df_dia_siguiente[['Año', 'Mes', 'Día', 'PREVISION', 'E_SIMEL']].copy()\n",
    "\n",
    "    # Asignar las predicciones a df_final_05_11\n",
    "    df_predicciones['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "    # Mostrar las primeras filas para verificar\n",
    "    df_predicciones[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Cálculo de las métricas\n",
    "\n",
    "    mse = mean_squared_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    mae = mean_absolute_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "\n",
    "\n",
    "    return df_predicciones, df_inicio_actualizado, mse, r2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 22.0962 - mae: 2.2366 - val_loss: 69.2733 - val_mae: 4.5985\n",
      "Epoch 2/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.0963 - mae: 2.2283 - val_loss: 68.6266 - val_mae: 4.6204\n",
      "Epoch 3/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.2297 - mae: 2.2350 - val_loss: 65.2816 - val_mae: 4.4604\n",
      "Epoch 4/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.0281 - mae: 2.2278 - val_loss: 68.1271 - val_mae: 4.5153\n",
      "Epoch 5/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.8273 - mae: 2.2158 - val_loss: 68.3372 - val_mae: 4.5568\n",
      "Epoch 6/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 22.0642 - mae: 2.2237 - val_loss: 74.5212 - val_mae: 4.6654\n",
      "Epoch 7/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9455 - mae: 2.2183 - val_loss: 67.1883 - val_mae: 4.5578\n",
      "Epoch 8/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9326 - mae: 2.2249 - val_loss: 72.1095 - val_mae: 4.8359\n",
      "Epoch 9/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9882 - mae: 2.2253 - val_loss: 71.5078 - val_mae: 4.7047\n",
      "Epoch 10/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7801 - mae: 2.2084 - val_loss: 69.5027 - val_mae: 4.6595\n",
      "Epoch 11/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.9490 - mae: 2.2130 - val_loss: 73.7600 - val_mae: 4.7820\n",
      "Epoch 12/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7814 - mae: 2.2169 - val_loss: 63.7553 - val_mae: 4.4049\n",
      "Epoch 13/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 21.6682 - mae: 2.1998 - val_loss: 65.8165 - val_mae: 4.5563\n",
      "Epoch 14/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.7530 - mae: 2.2094 - val_loss: 70.1690 - val_mae: 4.6431\n",
      "Epoch 15/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 21.5699 - mae: 2.1976 - val_loss: 71.5853 - val_mae: 4.6471\n",
      "Epoch 16/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.6312 - mae: 2.1999 - val_loss: 65.8927 - val_mae: 4.5044\n",
      "Epoch 17/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.6618 - mae: 2.2003 - val_loss: 72.9607 - val_mae: 4.8492\n",
      "Epoch 18/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.5884 - mae: 2.2015 - val_loss: 74.5925 - val_mae: 4.7223\n",
      "Epoch 19/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.5444 - mae: 2.2045 - val_loss: 74.6973 - val_mae: 4.8642\n",
      "Epoch 20/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 21.4942 - mae: 2.1920 - val_loss: 71.0778 - val_mae: 4.7657\n",
      "Epoch 21/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.4765 - mae: 2.1959 - val_loss: 68.4515 - val_mae: 4.5830\n",
      "Epoch 22/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.4299 - mae: 2.1910 - val_loss: 74.3423 - val_mae: 4.8238\n",
      "Epoch 23/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.3140 - mae: 2.1890 - val_loss: 73.9441 - val_mae: 4.7456\n",
      "Epoch 24/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2526 - mae: 2.1976 - val_loss: 72.9438 - val_mae: 4.7234\n",
      "Epoch 25/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2201 - mae: 2.1843 - val_loss: 80.5118 - val_mae: 4.8509\n",
      "Epoch 26/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2447 - mae: 2.1789 - val_loss: 71.8433 - val_mae: 4.7921\n",
      "Epoch 27/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2167 - mae: 2.1868 - val_loss: 76.8783 - val_mae: 4.9169\n",
      "Epoch 28/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.3450 - mae: 2.1953 - val_loss: 71.5689 - val_mae: 4.7517\n",
      "Epoch 29/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.1461 - mae: 2.1980 - val_loss: 75.8746 - val_mae: 4.9185\n",
      "Epoch 30/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.2728 - mae: 2.2124 - val_loss: 73.6153 - val_mae: 4.8155\n",
      "Epoch 31/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.9590 - mae: 2.1715 - val_loss: 76.8386 - val_mae: 4.8125\n",
      "Epoch 32/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 20.9346 - mae: 2.1565 - val_loss: 70.1722 - val_mae: 4.6275\n",
      "Epoch 33/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.1886 - mae: 2.1830 - val_loss: 73.6928 - val_mae: 4.6936\n",
      "Epoch 34/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.9583 - mae: 2.1695 - val_loss: 74.2479 - val_mae: 4.8542\n",
      "Epoch 35/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 21.0483 - mae: 2.1894 - val_loss: 76.4725 - val_mae: 4.8540\n",
      "Epoch 36/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 21.0267 - mae: 2.1771 - val_loss: 70.3215 - val_mae: 4.6682\n",
      "Epoch 37/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8070 - mae: 2.1603 - val_loss: 73.6733 - val_mae: 4.7376\n",
      "Epoch 38/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.9444 - mae: 2.1756 - val_loss: 72.9583 - val_mae: 4.6844\n",
      "Epoch 39/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8390 - mae: 2.1612 - val_loss: 75.8592 - val_mae: 4.8711\n",
      "Epoch 40/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.7525 - mae: 2.1576 - val_loss: 76.4188 - val_mae: 4.9567\n",
      "Epoch 41/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.6997 - mae: 2.1445 - val_loss: 79.9041 - val_mae: 5.0455\n",
      "Epoch 42/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.8518 - mae: 2.1775 - val_loss: 75.2733 - val_mae: 4.7472\n",
      "Epoch 43/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.7171 - mae: 2.1565 - val_loss: 70.8923 - val_mae: 4.7842\n",
      "Epoch 44/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.5363 - mae: 2.1475 - val_loss: 78.5750 - val_mae: 5.0941\n",
      "Epoch 45/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.7056 - mae: 2.1630 - val_loss: 76.0846 - val_mae: 4.9965\n",
      "Epoch 46/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.6082 - mae: 2.1516 - val_loss: 71.2054 - val_mae: 4.7650\n",
      "Epoch 47/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.7045 - mae: 2.1678 - val_loss: 73.3309 - val_mae: 4.8280\n",
      "Epoch 48/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 20.5346 - mae: 2.1513 - val_loss: 77.8153 - val_mae: 5.0250\n",
      "Epoch 49/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 20.4979 - mae: 2.1530 - val_loss: 72.7324 - val_mae: 4.8685\n",
      "Epoch 50/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.4823 - mae: 2.1553 - val_loss: 82.0259 - val_mae: 5.2517\n",
      "Epoch 51/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.5994 - mae: 2.1508 - val_loss: 75.6037 - val_mae: 4.9641\n",
      "Epoch 52/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3881 - mae: 2.1283 - val_loss: 73.0831 - val_mae: 4.8313\n",
      "Epoch 53/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3897 - mae: 2.1504 - val_loss: 77.7690 - val_mae: 5.0212\n",
      "Epoch 54/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3211 - mae: 2.1340 - val_loss: 72.7537 - val_mae: 4.9727\n",
      "Epoch 55/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.2318 - mae: 2.1323 - val_loss: 76.5223 - val_mae: 5.0178\n",
      "Epoch 56/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3189 - mae: 2.1469 - val_loss: 81.4407 - val_mae: 5.2760\n",
      "Epoch 57/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.1830 - mae: 2.1292 - val_loss: 77.1951 - val_mae: 5.0445\n",
      "Epoch 58/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.3289 - mae: 2.1445 - val_loss: 76.1667 - val_mae: 4.9465\n",
      "Epoch 59/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 20.1870 - mae: 2.1397 - val_loss: 76.4207 - val_mae: 4.8885\n",
      "Epoch 60/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 20.1246 - mae: 2.1240 - val_loss: 79.6359 - val_mae: 5.0481\n",
      "Epoch 61/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.1618 - mae: 2.1381 - val_loss: 76.6431 - val_mae: 5.0555\n",
      "Epoch 62/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.0405 - mae: 2.1166 - val_loss: 76.4269 - val_mae: 4.9916\n",
      "Epoch 63/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 20.0121 - mae: 2.1304 - val_loss: 77.2694 - val_mae: 5.0115\n",
      "Epoch 64/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.2006 - mae: 2.1586 - val_loss: 83.1416 - val_mae: 5.2365\n",
      "Epoch 65/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 19.9531 - mae: 2.1110 - val_loss: 74.9167 - val_mae: 4.8973\n",
      "Epoch 66/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 19.9164 - mae: 2.1215 - val_loss: 80.4018 - val_mae: 5.0570\n",
      "Epoch 67/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.0920 - mae: 2.1297 - val_loss: 77.0796 - val_mae: 5.2331\n",
      "Epoch 68/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 20.0382 - mae: 2.1417 - val_loss: 75.9100 - val_mae: 4.9699\n",
      "Epoch 69/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 19.8797 - mae: 2.1174 - val_loss: 78.5736 - val_mae: 5.0483\n",
      "Epoch 70/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 19.9335 - mae: 2.1401 - val_loss: 84.8658 - val_mae: 5.4442\n",
      "Epoch 71/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 19.7457 - mae: 2.1166 - val_loss: 77.4482 - val_mae: 4.9417\n",
      "Epoch 72/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 19.6652 - mae: 2.1060 - val_loss: 84.0043 - val_mae: 5.2221\n",
      "Epoch 73/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 19.7864 - mae: 2.1084 - val_loss: 79.6631 - val_mae: 5.2212\n",
      "Epoch 74/75\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 19.6756 - mae: 2.1075 - val_loss: 74.2664 - val_mae: 4.8373\n",
      "Epoch 75/75\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 19.6296 - mae: 2.1115 - val_loss: 76.8825 - val_mae: 5.0185\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "MSE: 48.065272984722434 R²: -0.20581192993599973 MAE: 3.2383921784162517\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 6    # Actualización de datos con los que reentrenamos los modelos\n",
    "dia_siguiente = 7    # Preparación de datos para la imputación y predicción\n",
    "df_predicciones_07_11, df_inicio_actualizado, mse_07_11, r2_07_11, mae_07_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_07_11, \"R²:\", r2_07_11, \"MAE:\", mae_07_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  67.97999999999999\n",
      "Suma predicha:  13.508122086524963\n",
      "Desviación porcentual:  -80.12927024635927 %\n",
      "Suma previsión:  24.0\n",
      "Desviación porcentual:  -64.6954986760812 %\n"
     ]
    }
   ],
   "source": [
    "# Y como en la predicción anterior calculamos los sumatorios y los porcentajes de desviación\n",
    "\n",
    "suma_real_07_11 = df_predicciones_07_11['E_SIMEL'].sum()\n",
    "suma_predicha_07_11 = df_predicciones_07_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_07_11 = df_predicciones_07_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_07_11)\n",
    "print(\"Suma predicha: \", suma_predicha_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 19.7991 - mae: 2.1116 - val_loss: 79.0149 - val_mae: 5.1278\n",
      "Epoch 2/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 19.6061 - mae: 2.1073 - val_loss: 81.5929 - val_mae: 5.2723\n",
      "Epoch 3/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 19.6776 - mae: 2.1182 - val_loss: 75.5619 - val_mae: 4.9359\n",
      "Epoch 4/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 19.5434 - mae: 2.1064 - val_loss: 79.4953 - val_mae: 5.1842\n",
      "Epoch 5/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 19.5776 - mae: 2.1011 - val_loss: 81.8439 - val_mae: 5.2555\n",
      "Epoch 6/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4254 - mae: 2.0867 - val_loss: 78.0406 - val_mae: 5.0659\n",
      "Epoch 7/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4935 - mae: 2.0834 - val_loss: 79.0402 - val_mae: 5.1444\n",
      "Epoch 8/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4450 - mae: 2.0881 - val_loss: 81.2753 - val_mae: 5.3591\n",
      "Epoch 9/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.5355 - mae: 2.1124 - val_loss: 82.0078 - val_mae: 5.2841\n",
      "Epoch 10/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4051 - mae: 2.0889 - val_loss: 81.9289 - val_mae: 5.5191\n",
      "Epoch 11/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4038 - mae: 2.0926 - val_loss: 79.5722 - val_mae: 5.2416\n",
      "Epoch 12/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.5488 - mae: 2.1169 - val_loss: 85.3498 - val_mae: 5.2661\n",
      "Epoch 13/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.5446 - mae: 2.0867 - val_loss: 84.8597 - val_mae: 5.3042\n",
      "Epoch 14/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3457 - mae: 2.0948 - val_loss: 83.5855 - val_mae: 5.4998\n",
      "Epoch 15/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.4365 - mae: 2.0972 - val_loss: 82.4538 - val_mae: 5.3691\n",
      "Epoch 16/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3554 - mae: 2.0833 - val_loss: 82.6711 - val_mae: 5.4611\n",
      "Epoch 17/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3147 - mae: 2.0829 - val_loss: 81.6266 - val_mae: 5.1296\n",
      "Epoch 18/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2254 - mae: 2.0653 - val_loss: 84.0993 - val_mae: 5.1673\n",
      "Epoch 19/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0699 - mae: 2.0712 - val_loss: 85.9633 - val_mae: 5.5189\n",
      "Epoch 20/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.3227 - mae: 2.0863 - val_loss: 87.7745 - val_mae: 5.4588\n",
      "Epoch 21/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.2037 - mae: 2.0657 - val_loss: 87.5901 - val_mae: 5.6877\n",
      "Epoch 22/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.9957 - mae: 2.0570 - val_loss: 86.4726 - val_mae: 5.3304\n",
      "Epoch 23/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 19.0900 - mae: 2.0864 - val_loss: 86.3426 - val_mae: 5.4750\n",
      "Epoch 24/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1159 - mae: 2.0757 - val_loss: 86.5235 - val_mae: 5.5739\n",
      "Epoch 25/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.1116 - mae: 2.0857 - val_loss: 86.9049 - val_mae: 5.4373\n",
      "Epoch 26/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0178 - mae: 2.0753 - val_loss: 80.0221 - val_mae: 5.1929\n",
      "Epoch 27/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0897 - mae: 2.0722 - val_loss: 94.4969 - val_mae: 5.4781\n",
      "Epoch 28/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7936 - mae: 2.0590 - val_loss: 84.7087 - val_mae: 5.4399\n",
      "Epoch 29/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8745 - mae: 2.0853 - val_loss: 89.2754 - val_mae: 5.4408\n",
      "Epoch 30/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0082 - mae: 2.0640 - val_loss: 82.8087 - val_mae: 5.3545\n",
      "Epoch 31/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0216 - mae: 2.0763 - val_loss: 85.6287 - val_mae: 5.2567\n",
      "Epoch 32/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.9055 - mae: 2.0507 - val_loss: 88.0974 - val_mae: 5.4746\n",
      "Epoch 33/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 19.0270 - mae: 2.0665 - val_loss: 96.5561 - val_mae: 5.6519\n",
      "Epoch 34/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8639 - mae: 2.0529 - val_loss: 88.3947 - val_mae: 5.3789\n",
      "Epoch 35/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.9026 - mae: 2.0548 - val_loss: 89.0250 - val_mae: 5.3490\n",
      "Epoch 36/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8868 - mae: 2.0621 - val_loss: 95.4024 - val_mae: 5.6994\n",
      "Epoch 37/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8637 - mae: 2.0617 - val_loss: 85.3724 - val_mae: 5.2018\n",
      "Epoch 38/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6682 - mae: 2.0509 - val_loss: 89.2125 - val_mae: 5.5538\n",
      "Epoch 39/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6881 - mae: 2.0477 - val_loss: 92.6286 - val_mae: 5.4750\n",
      "Epoch 40/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6659 - mae: 2.0375 - val_loss: 90.6338 - val_mae: 5.5047\n",
      "Epoch 41/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.8532 - mae: 2.0707 - val_loss: 88.6611 - val_mae: 5.4428\n",
      "Epoch 42/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7852 - mae: 2.0582 - val_loss: 92.4675 - val_mae: 5.6035\n",
      "Epoch 43/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 18.6636 - mae: 2.0541 - val_loss: 88.3947 - val_mae: 5.4581\n",
      "Epoch 44/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.5987 - mae: 2.0381 - val_loss: 88.3841 - val_mae: 5.3598\n",
      "Epoch 45/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6337 - mae: 2.0421 - val_loss: 90.0059 - val_mae: 5.5887\n",
      "Epoch 46/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.7263 - mae: 2.0542 - val_loss: 91.2036 - val_mae: 5.5086\n",
      "Epoch 47/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.5805 - mae: 2.0370 - val_loss: 86.0238 - val_mae: 5.3903\n",
      "Epoch 48/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6700 - mae: 2.0589 - val_loss: 92.5865 - val_mae: 5.5146\n",
      "Epoch 49/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4942 - mae: 2.0269 - val_loss: 86.1894 - val_mae: 5.3282\n",
      "Epoch 50/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4652 - mae: 2.0236 - val_loss: 85.0295 - val_mae: 5.3203\n",
      "Epoch 51/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.5904 - mae: 2.0514 - val_loss: 86.2899 - val_mae: 5.2489\n",
      "Epoch 52/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4201 - mae: 2.0203 - val_loss: 89.3412 - val_mae: 5.4327\n",
      "Epoch 53/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6694 - mae: 2.0458 - val_loss: 88.7635 - val_mae: 5.3909\n",
      "Epoch 54/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.3565 - mae: 2.0361 - val_loss: 93.2986 - val_mae: 5.3474\n",
      "Epoch 55/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6372 - mae: 2.0624 - val_loss: 90.1309 - val_mae: 5.3834\n",
      "Epoch 56/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.3489 - mae: 2.0363 - val_loss: 91.0154 - val_mae: 5.4830\n",
      "Epoch 57/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.3644 - mae: 2.0375 - val_loss: 89.2414 - val_mae: 5.3926\n",
      "Epoch 58/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.2457 - mae: 2.0233 - val_loss: 84.6675 - val_mae: 5.2364\n",
      "Epoch 59/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.6042 - mae: 2.0437 - val_loss: 91.4052 - val_mae: 5.3658\n",
      "Epoch 60/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4023 - mae: 2.0405 - val_loss: 92.5453 - val_mae: 5.4846\n",
      "Epoch 61/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4624 - mae: 2.0574 - val_loss: 93.7872 - val_mae: 5.5176\n",
      "Epoch 62/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4086 - mae: 2.0274 - val_loss: 102.2807 - val_mae: 5.8168\n",
      "Epoch 63/75\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 18.3274 - mae: 2.0190 - val_loss: 93.0869 - val_mae: 5.7424\n",
      "Epoch 64/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.3010 - mae: 2.0327 - val_loss: 96.4945 - val_mae: 5.5939\n",
      "Epoch 65/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.2044 - mae: 2.0143 - val_loss: 93.4581 - val_mae: 5.5324\n",
      "Epoch 66/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.1610 - mae: 2.0158 - val_loss: 94.2255 - val_mae: 5.5153\n",
      "Epoch 67/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.4491 - mae: 2.0557 - val_loss: 91.3162 - val_mae: 5.5292\n",
      "Epoch 68/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.1942 - mae: 2.0159 - val_loss: 90.6744 - val_mae: 5.4339\n",
      "Epoch 69/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.2512 - mae: 2.0223 - val_loss: 98.3606 - val_mae: 5.6769\n",
      "Epoch 70/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.1299 - mae: 2.0015 - val_loss: 92.3614 - val_mae: 5.4625\n",
      "Epoch 71/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.2018 - mae: 2.0266 - val_loss: 94.1710 - val_mae: 5.4789\n",
      "Epoch 72/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.1987 - mae: 2.0141 - val_loss: 91.1756 - val_mae: 5.4674\n",
      "Epoch 73/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 17.9508 - mae: 1.9965 - val_loss: 93.4761 - val_mae: 5.5955\n",
      "Epoch 74/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.1304 - mae: 2.0275 - val_loss: 102.7386 - val_mae: 5.8378\n",
      "Epoch 75/75\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 18.0285 - mae: 2.0200 - val_loss: 95.1743 - val_mae: 5.5768\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "MSE: 3.2590745416666667 R²: -0.07294345232984711 MAE: 0.4707083333333333\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función. Actualización de datos reales del día 7 para reentrenar los modelos\n",
    "# Preparación de los datos del día posterior, en este caso del día 8, para la imputación y predicción\n",
    "\n",
    "dia_actual = 7\n",
    "dia_siguiente = 8\n",
    "\n",
    "df_predicciones_08_11, df_inicio_actualizado, mse_08_11, r2_08_11, mae_08_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_08_11, \"R²:\", r2_08_11, \"MAE:\", mae_08_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  11.296999999999999\n",
      "Suma predicha:  0.0\n",
      "Desviación porcentual:  -100.0 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_08_11 = df_predicciones_08_11['E_SIMEL'].sum()\n",
    "suma_predicha_08_11 = df_predicciones_08_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_08_11 = df_predicciones_08_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_08_11)\n",
    "print(\"Suma predicha: \", suma_predicha_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.0357 - mae: 2.0017 - val_loss: 92.5414 - val_mae: 5.4746\n",
      "Epoch 2/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9371 - mae: 2.0049 - val_loss: 96.4301 - val_mae: 5.5406\n",
      "Epoch 3/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.0012 - mae: 2.0077 - val_loss: 98.3483 - val_mae: 5.4249\n",
      "Epoch 4/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9727 - mae: 2.0027 - val_loss: 96.4917 - val_mae: 5.5492\n",
      "Epoch 5/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 18.0846 - mae: 2.0388 - val_loss: 103.2837 - val_mae: 5.6745\n",
      "Epoch 6/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9308 - mae: 2.0186 - val_loss: 96.9109 - val_mae: 5.8540\n",
      "Epoch 7/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.7132 - mae: 1.9943 - val_loss: 99.0949 - val_mae: 5.5641\n",
      "Epoch 8/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.0340 - mae: 2.0086 - val_loss: 96.5804 - val_mae: 5.6233\n",
      "Epoch 9/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.7836 - mae: 2.0135 - val_loss: 91.9203 - val_mae: 5.5582\n",
      "Epoch 10/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9836 - mae: 2.0079 - val_loss: 97.1990 - val_mae: 5.8179\n",
      "Epoch 11/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9127 - mae: 2.0055 - val_loss: 91.0832 - val_mae: 5.5369\n",
      "Epoch 12/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9255 - mae: 2.0065 - val_loss: 97.1170 - val_mae: 5.5616\n",
      "Epoch 13/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9100 - mae: 2.0111 - val_loss: 99.3509 - val_mae: 5.9833\n",
      "Epoch 14/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.7319 - mae: 1.9981 - val_loss: 98.6470 - val_mae: 5.9647\n",
      "Epoch 15/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.8947 - mae: 2.0317 - val_loss: 90.2980 - val_mae: 5.4739\n",
      "Epoch 16/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.9686 - mae: 2.0156 - val_loss: 95.8344 - val_mae: 5.4309\n",
      "Epoch 17/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.7935 - mae: 1.9920 - val_loss: 91.7368 - val_mae: 5.4469\n",
      "Epoch 18/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.2381 - mae: 2.0272 - val_loss: 100.7706 - val_mae: 5.6311\n",
      "Epoch 19/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.7264 - mae: 2.0003 - val_loss: 98.0593 - val_mae: 5.6233\n",
      "Epoch 20/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.6616 - mae: 1.9793 - val_loss: 95.2888 - val_mae: 5.9277\n",
      "Epoch 21/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.6314 - mae: 1.9752 - val_loss: 96.2063 - val_mae: 5.6251\n",
      "Epoch 22/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.6204 - mae: 1.9935 - val_loss: 93.9674 - val_mae: 5.5933\n",
      "Epoch 23/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.7624 - mae: 1.9845 - val_loss: 97.1771 - val_mae: 5.6517\n",
      "Epoch 24/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.5291 - mae: 1.9830 - val_loss: 95.2571 - val_mae: 5.5972\n",
      "Epoch 25/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.5096 - mae: 1.9783 - val_loss: 106.3743 - val_mae: 5.8951\n",
      "Epoch 26/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.6177 - mae: 1.9755 - val_loss: 97.7733 - val_mae: 5.7277\n",
      "Epoch 27/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.5575 - mae: 2.0035 - val_loss: 100.9448 - val_mae: 5.9603\n",
      "Epoch 28/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.6290 - mae: 1.9891 - val_loss: 96.3539 - val_mae: 5.6176\n",
      "Epoch 29/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.8713 - mae: 2.0230 - val_loss: 98.0346 - val_mae: 5.5766\n",
      "Epoch 30/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4776 - mae: 1.9973 - val_loss: 100.4779 - val_mae: 5.7675\n",
      "Epoch 31/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4939 - mae: 2.0012 - val_loss: 101.3178 - val_mae: 5.7956\n",
      "Epoch 32/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4770 - mae: 1.9902 - val_loss: 101.7161 - val_mae: 5.9719\n",
      "Epoch 33/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 18.1767 - mae: 2.0415 - val_loss: 97.9601 - val_mae: 5.6512\n",
      "Epoch 34/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4764 - mae: 1.9817 - val_loss: 99.2089 - val_mae: 5.6001\n",
      "Epoch 35/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4346 - mae: 1.9703 - val_loss: 103.9172 - val_mae: 6.2494\n",
      "Epoch 36/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4711 - mae: 1.9924 - val_loss: 96.3720 - val_mae: 5.5842\n",
      "Epoch 37/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.4380 - mae: 1.9846 - val_loss: 99.6467 - val_mae: 5.6109\n",
      "Epoch 38/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3353 - mae: 1.9607 - val_loss: 98.3761 - val_mae: 5.7046\n",
      "Epoch 39/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.5089 - mae: 1.9937 - val_loss: 97.5200 - val_mae: 5.6245\n",
      "Epoch 40/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.4711 - mae: 1.9865 - val_loss: 98.9966 - val_mae: 5.5185\n",
      "Epoch 41/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3765 - mae: 1.9870 - val_loss: 97.9633 - val_mae: 5.5545\n",
      "Epoch 42/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3241 - mae: 1.9748 - val_loss: 102.7226 - val_mae: 5.8454\n",
      "Epoch 43/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3682 - mae: 1.9663 - val_loss: 101.3039 - val_mae: 6.0753\n",
      "Epoch 44/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.2762 - mae: 1.9819 - val_loss: 100.6100 - val_mae: 5.7185\n",
      "Epoch 45/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3266 - mae: 1.9879 - val_loss: 97.3463 - val_mae: 5.6814\n",
      "Epoch 46/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.2203 - mae: 1.9855 - val_loss: 94.5731 - val_mae: 5.5076\n",
      "Epoch 47/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8822 - mae: 1.9332 - val_loss: 95.5464 - val_mae: 5.5029\n",
      "Epoch 48/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3356 - mae: 2.0038 - val_loss: 108.4510 - val_mae: 5.8965\n",
      "Epoch 49/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3607 - mae: 1.9797 - val_loss: 102.9410 - val_mae: 5.6952\n",
      "Epoch 50/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.0225 - mae: 1.9507 - val_loss: 99.4683 - val_mae: 5.6146\n",
      "Epoch 51/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.2751 - mae: 1.9753 - val_loss: 103.3739 - val_mae: 5.7462\n",
      "Epoch 52/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.0791 - mae: 1.9690 - val_loss: 100.6188 - val_mae: 5.7689\n",
      "Epoch 53/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9968 - mae: 1.9586 - val_loss: 101.0293 - val_mae: 5.5820\n",
      "Epoch 54/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.1806 - mae: 1.9745 - val_loss: 98.5592 - val_mae: 5.6016\n",
      "Epoch 55/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.3226 - mae: 1.9890 - val_loss: 101.1127 - val_mae: 5.7667\n",
      "Epoch 56/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.0642 - mae: 1.9569 - val_loss: 103.4546 - val_mae: 5.9360\n",
      "Epoch 57/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.0791 - mae: 1.9804 - val_loss: 105.6796 - val_mae: 5.7915\n",
      "Epoch 58/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.1375 - mae: 1.9744 - val_loss: 100.1544 - val_mae: 5.7099\n",
      "Epoch 59/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.1769 - mae: 1.9845 - val_loss: 107.2520 - val_mae: 5.8364\n",
      "Epoch 60/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.8651 - mae: 1.9329 - val_loss: 101.0184 - val_mae: 5.6748\n",
      "Epoch 61/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.0021 - mae: 1.9643 - val_loss: 106.2346 - val_mae: 5.9473\n",
      "Epoch 62/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.0425 - mae: 1.9692 - val_loss: 105.6576 - val_mae: 5.7908\n",
      "Epoch 63/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.1150 - mae: 1.9652 - val_loss: 112.2173 - val_mae: 5.7905\n",
      "Epoch 64/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.9577 - mae: 1.9415 - val_loss: 107.0766 - val_mae: 5.8139\n",
      "Epoch 65/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.2638 - mae: 1.9944 - val_loss: 109.5915 - val_mae: 5.9338\n",
      "Epoch 66/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.9494 - mae: 1.9601 - val_loss: 99.3245 - val_mae: 5.5842\n",
      "Epoch 67/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6873 - mae: 1.9279 - val_loss: 108.2953 - val_mae: 6.2399\n",
      "Epoch 68/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 17.3740 - mae: 2.0045 - val_loss: 101.6726 - val_mae: 5.6260\n",
      "Epoch 69/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6859 - mae: 1.9426 - val_loss: 103.7297 - val_mae: 5.8275\n",
      "Epoch 70/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8862 - mae: 1.9448 - val_loss: 102.4981 - val_mae: 5.7554\n",
      "Epoch 71/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8894 - mae: 1.9375 - val_loss: 102.4236 - val_mae: 5.7229\n",
      "Epoch 72/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8170 - mae: 1.9584 - val_loss: 106.0078 - val_mae: 5.8298\n",
      "Epoch 73/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8031 - mae: 1.9393 - val_loss: 104.5980 - val_mae: 5.7763\n",
      "Epoch 74/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.7951 - mae: 1.9418 - val_loss: 110.9041 - val_mae: 5.8130\n",
      "Epoch 75/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.9574 - mae: 1.9479 - val_loss: 101.0831 - val_mae: 5.5883\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "MSE: 45.566310871678034 R²: 0.5127678936556984 MAE: 3.0628508271972343\n"
     ]
    }
   ],
   "source": [
    "# Seguimos con el mismo proceso anterior. Llamamos a la función con los días específicos que queremos actualizar, imputar y predecir.\n",
    "\n",
    "dia_actual = 8\n",
    "dia_siguiente = 9\n",
    "\n",
    "df_predicciones_09_11, df_inicio_actualizado, mse_09_11, r2_09_11, mae_09_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_09_11, \"R²:\", r2_09_11, \"MAE:\", mae_09_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  102.43700000000001\n",
      "Suma predicha:  46.94646996259689\n",
      "Desviación porcentual:  -54.170397451509814 %\n",
      "Suma previsión:  127.30000000000001\n",
      "Desviación porcentual:  24.271503460663627 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y procentajes\n",
    "\n",
    "suma_real_09_11 = df_predicciones_09_11['E_SIMEL'].sum()\n",
    "suma_predicha_09_11 = df_predicciones_09_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_09_11 = df_predicciones_09_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_09_11)\n",
    "print(\"Suma predicha: \", suma_predicha_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 17.0617 - mae: 1.9673 - val_loss: 101.3276 - val_mae: 5.7920\n",
      "Epoch 2/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8681 - mae: 1.9501 - val_loss: 105.3429 - val_mae: 5.9292\n",
      "Epoch 3/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8524 - mae: 1.9554 - val_loss: 109.0673 - val_mae: 5.8718\n",
      "Epoch 4/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8630 - mae: 1.9415 - val_loss: 102.8899 - val_mae: 5.8287\n",
      "Epoch 5/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.9958 - mae: 1.9518 - val_loss: 99.7434 - val_mae: 5.6654\n",
      "Epoch 6/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8683 - mae: 1.9467 - val_loss: 108.4118 - val_mae: 5.9142\n",
      "Epoch 7/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.7882 - mae: 1.9446 - val_loss: 102.7002 - val_mae: 5.6172\n",
      "Epoch 8/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6743 - mae: 1.9371 - val_loss: 107.4632 - val_mae: 5.9116\n",
      "Epoch 9/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6711 - mae: 1.9426 - val_loss: 109.7997 - val_mae: 5.9518\n",
      "Epoch 10/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.8466 - mae: 1.9545 - val_loss: 109.0147 - val_mae: 5.9718\n",
      "Epoch 11/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.9370 - mae: 1.9636 - val_loss: 105.5133 - val_mae: 5.8800\n",
      "Epoch 12/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6279 - mae: 1.9193 - val_loss: 107.0095 - val_mae: 5.9063\n",
      "Epoch 13/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.5980 - mae: 1.9544 - val_loss: 107.7984 - val_mae: 5.9181\n",
      "Epoch 14/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6814 - mae: 1.9557 - val_loss: 104.8746 - val_mae: 5.8027\n",
      "Epoch 15/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6475 - mae: 1.9299 - val_loss: 107.0594 - val_mae: 5.9468\n",
      "Epoch 16/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6249 - mae: 1.9350 - val_loss: 107.1537 - val_mae: 6.1766\n",
      "Epoch 17/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.7209 - mae: 1.9544 - val_loss: 106.6778 - val_mae: 5.8702\n",
      "Epoch 18/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6584 - mae: 1.9429 - val_loss: 104.5837 - val_mae: 5.8549\n",
      "Epoch 19/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.5401 - mae: 1.9328 - val_loss: 115.6902 - val_mae: 6.0962\n",
      "Epoch 20/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.7939 - mae: 1.9357 - val_loss: 112.5082 - val_mae: 6.0445\n",
      "Epoch 21/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.6343 - mae: 1.9392 - val_loss: 102.0531 - val_mae: 5.8036\n",
      "Epoch 22/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.3746 - mae: 1.9217 - val_loss: 111.9154 - val_mae: 6.1911\n",
      "Epoch 23/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4117 - mae: 1.9232 - val_loss: 109.0140 - val_mae: 5.9821\n",
      "Epoch 24/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4433 - mae: 1.9169 - val_loss: 106.6650 - val_mae: 5.8672\n",
      "Epoch 25/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4665 - mae: 1.9293 - val_loss: 112.5648 - val_mae: 5.9501\n",
      "Epoch 26/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6816 - mae: 1.9634 - val_loss: 109.7788 - val_mae: 6.0154\n",
      "Epoch 27/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3791 - mae: 1.9338 - val_loss: 109.0186 - val_mae: 5.9937\n",
      "Epoch 28/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4665 - mae: 1.9378 - val_loss: 114.3670 - val_mae: 6.2023\n",
      "Epoch 29/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.4116 - mae: 1.9201 - val_loss: 110.1320 - val_mae: 5.8674\n",
      "Epoch 30/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.5115 - mae: 1.9276 - val_loss: 109.8598 - val_mae: 6.1131\n",
      "Epoch 31/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.3096 - mae: 1.9003 - val_loss: 110.6808 - val_mae: 5.9668\n",
      "Epoch 32/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4913 - mae: 1.9311 - val_loss: 116.2653 - val_mae: 6.3367\n",
      "Epoch 33/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2407 - mae: 1.9148 - val_loss: 104.6290 - val_mae: 5.8465\n",
      "Epoch 34/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.6509 - mae: 1.9416 - val_loss: 107.7478 - val_mae: 6.2058\n",
      "Epoch 35/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.3127 - mae: 1.9246 - val_loss: 107.8400 - val_mae: 5.8463\n",
      "Epoch 36/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.4533 - mae: 1.9328 - val_loss: 113.5183 - val_mae: 6.0441\n",
      "Epoch 37/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.2263 - mae: 1.9012 - val_loss: 110.8894 - val_mae: 6.0194\n",
      "Epoch 38/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.2835 - mae: 1.9101 - val_loss: 105.8178 - val_mae: 5.8474\n",
      "Epoch 39/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.3716 - mae: 1.9325 - val_loss: 106.4286 - val_mae: 5.8541\n",
      "Epoch 40/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.1988 - mae: 1.9158 - val_loss: 109.5691 - val_mae: 6.0462\n",
      "Epoch 41/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.3835 - mae: 1.9389 - val_loss: 109.4915 - val_mae: 6.0468\n",
      "Epoch 42/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.2378 - mae: 1.9051 - val_loss: 110.6700 - val_mae: 6.1250\n",
      "Epoch 43/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.2669 - mae: 1.9121 - val_loss: 105.1689 - val_mae: 5.9288\n",
      "Epoch 44/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1785 - mae: 1.9276 - val_loss: 114.8015 - val_mae: 6.1489\n",
      "Epoch 45/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1501 - mae: 1.9016 - val_loss: 104.4151 - val_mae: 5.8628\n",
      "Epoch 46/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.1388 - mae: 1.9013 - val_loss: 108.5647 - val_mae: 5.7927\n",
      "Epoch 47/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.2101 - mae: 1.9142 - val_loss: 113.8915 - val_mae: 6.1485\n",
      "Epoch 48/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.1894 - mae: 1.9217 - val_loss: 106.7718 - val_mae: 6.0144\n",
      "Epoch 49/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.0036 - mae: 1.9027 - val_loss: 114.6754 - val_mae: 6.0650\n",
      "Epoch 50/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.1069 - mae: 1.9180 - val_loss: 110.1495 - val_mae: 5.8751\n",
      "Epoch 51/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.0969 - mae: 1.9040 - val_loss: 112.8691 - val_mae: 6.1127\n",
      "Epoch 52/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.1742 - mae: 1.9107 - val_loss: 117.5997 - val_mae: 6.1297\n",
      "Epoch 53/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 16.0498 - mae: 1.9058 - val_loss: 106.8674 - val_mae: 5.8071\n",
      "Epoch 54/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.3101 - mae: 1.9150 - val_loss: 112.8295 - val_mae: 6.0761\n",
      "Epoch 55/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.1313 - mae: 1.9099 - val_loss: 112.2199 - val_mae: 5.8565\n",
      "Epoch 56/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.9866 - mae: 1.9021 - val_loss: 109.2722 - val_mae: 6.3076\n",
      "Epoch 57/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.0036 - mae: 1.8956 - val_loss: 117.2988 - val_mae: 5.9720\n",
      "Epoch 58/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.0962 - mae: 1.9017 - val_loss: 113.5601 - val_mae: 6.0702\n",
      "Epoch 59/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.0511 - mae: 1.9243 - val_loss: 116.7743 - val_mae: 6.1819\n",
      "Epoch 60/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.9590 - mae: 1.8810 - val_loss: 116.2052 - val_mae: 6.1568\n",
      "Epoch 61/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8334 - mae: 1.9104 - val_loss: 110.5505 - val_mae: 6.0747\n",
      "Epoch 62/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.9738 - mae: 1.9006 - val_loss: 110.8422 - val_mae: 5.9117\n",
      "Epoch 63/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8997 - mae: 1.8973 - val_loss: 116.8488 - val_mae: 6.0598\n",
      "Epoch 64/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.9717 - mae: 1.9247 - val_loss: 111.0108 - val_mae: 5.8713\n",
      "Epoch 65/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.8922 - mae: 1.9147 - val_loss: 114.3955 - val_mae: 6.1749\n",
      "Epoch 66/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 16.0810 - mae: 1.9224 - val_loss: 112.7861 - val_mae: 6.2264\n",
      "Epoch 67/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8262 - mae: 1.8695 - val_loss: 112.6335 - val_mae: 6.0240\n",
      "Epoch 68/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.7941 - mae: 1.8930 - val_loss: 114.4815 - val_mae: 6.2962\n",
      "Epoch 69/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8346 - mae: 1.9033 - val_loss: 115.8964 - val_mae: 6.0041\n",
      "Epoch 70/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.9696 - mae: 1.9062 - val_loss: 117.3209 - val_mae: 6.2601\n",
      "Epoch 71/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.7300 - mae: 1.8734 - val_loss: 116.3733 - val_mae: 6.1980\n",
      "Epoch 72/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8150 - mae: 1.8809 - val_loss: 118.2982 - val_mae: 6.6383\n",
      "Epoch 73/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.7946 - mae: 1.8857 - val_loss: 121.5287 - val_mae: 6.4716\n",
      "Epoch 74/75\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 15.7112 - mae: 1.8885 - val_loss: 116.0411 - val_mae: 6.0365\n",
      "Epoch 75/75\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 15.8211 - mae: 1.8886 - val_loss: 112.5452 - val_mae: 6.0640\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 13.867975335242265 R²: 0.42784016834782557 MAE: 1.7133134933312733\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 9\n",
    "dia_siguiente = 10\n",
    "\n",
    "df_predicciones_10_11, df_inicio_actualizado, mse_10_11, r2_10_11, mae_10_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_10_11, \"R²:\", r2_10_11, \"MAE:\", mae_10_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.955\n",
      "Suma predicha:  26.45813012123108\n",
      "Desviación porcentual:  -55.121482281009115 %\n",
      "Suma previsión:  112.6\n",
      "Desviación porcentual:  90.99313035365958 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_10_11 = df_predicciones_10_11['E_SIMEL'].sum()\n",
    "suma_predicha_10_11 = df_predicciones_10_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_10_11 = df_predicciones_10_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_10_11)\n",
    "print(\"Suma predicha: \", suma_predicha_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7733 - mae: 1.8905 - val_loss: 113.0414 - val_mae: 5.9949\n",
      "Epoch 2/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.9748 - mae: 1.9170 - val_loss: 120.1576 - val_mae: 6.3751\n",
      "Epoch 3/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.8270 - mae: 1.8803 - val_loss: 116.9907 - val_mae: 5.9929\n",
      "Epoch 4/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.9930 - mae: 1.9311 - val_loss: 118.5725 - val_mae: 6.1770\n",
      "Epoch 5/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7689 - mae: 1.8885 - val_loss: 119.1000 - val_mae: 6.3198\n",
      "Epoch 6/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.8730 - mae: 1.9061 - val_loss: 113.7123 - val_mae: 6.2310\n",
      "Epoch 7/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7139 - mae: 1.8857 - val_loss: 109.8845 - val_mae: 5.9030\n",
      "Epoch 8/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7313 - mae: 1.8657 - val_loss: 115.9128 - val_mae: 6.1334\n",
      "Epoch 9/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.6223 - mae: 1.8930 - val_loss: 119.4703 - val_mae: 6.2199\n",
      "Epoch 10/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7035 - mae: 1.8865 - val_loss: 115.1710 - val_mae: 6.2527\n",
      "Epoch 11/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.8081 - mae: 1.9161 - val_loss: 112.4030 - val_mae: 6.0092\n",
      "Epoch 12/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7815 - mae: 1.9062 - val_loss: 122.9015 - val_mae: 6.3697\n",
      "Epoch 13/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.6209 - mae: 1.8705 - val_loss: 113.9233 - val_mae: 6.1301\n",
      "Epoch 14/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.7477 - mae: 1.9071 - val_loss: 121.8957 - val_mae: 6.2207\n",
      "Epoch 15/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7421 - mae: 1.8996 - val_loss: 123.5863 - val_mae: 6.3307\n",
      "Epoch 16/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7552 - mae: 1.8880 - val_loss: 116.9325 - val_mae: 6.0771\n",
      "Epoch 17/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.6673 - mae: 1.8963 - val_loss: 112.6299 - val_mae: 5.9982\n",
      "Epoch 18/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.5303 - mae: 1.8769 - val_loss: 118.3015 - val_mae: 6.2328\n",
      "Epoch 19/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.5484 - mae: 1.8812 - val_loss: 115.4054 - val_mae: 6.3493\n",
      "Epoch 20/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7295 - mae: 1.9086 - val_loss: 117.8789 - val_mae: 5.9878\n",
      "Epoch 21/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3810 - mae: 1.8612 - val_loss: 117.3921 - val_mae: 6.2409\n",
      "Epoch 22/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7839 - mae: 1.9106 - val_loss: 119.2677 - val_mae: 6.2529\n",
      "Epoch 23/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.7760 - mae: 1.9068 - val_loss: 121.3066 - val_mae: 6.2218\n",
      "Epoch 24/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.4654 - mae: 1.8686 - val_loss: 124.8514 - val_mae: 6.2314\n",
      "Epoch 25/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.5969 - mae: 1.8807 - val_loss: 122.0796 - val_mae: 6.3552\n",
      "Epoch 26/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.4972 - mae: 1.8942 - val_loss: 129.3658 - val_mae: 6.5196\n",
      "Epoch 27/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.6328 - mae: 1.9071 - val_loss: 117.3704 - val_mae: 6.1368\n",
      "Epoch 28/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.5123 - mae: 1.8984 - val_loss: 125.9313 - val_mae: 6.5308\n",
      "Epoch 29/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.4138 - mae: 1.8674 - val_loss: 118.0693 - val_mae: 6.1031\n",
      "Epoch 30/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3614 - mae: 1.8750 - val_loss: 122.8448 - val_mae: 6.4259\n",
      "Epoch 31/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.4249 - mae: 1.8694 - val_loss: 120.2148 - val_mae: 6.3951\n",
      "Epoch 32/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.4973 - mae: 1.8923 - val_loss: 117.6319 - val_mae: 6.0235\n",
      "Epoch 33/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.2842 - mae: 1.8652 - val_loss: 113.4271 - val_mae: 5.9868\n",
      "Epoch 34/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3928 - mae: 1.8810 - val_loss: 117.9620 - val_mae: 6.0957\n",
      "Epoch 35/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3969 - mae: 1.8625 - val_loss: 124.1318 - val_mae: 6.3207\n",
      "Epoch 36/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3675 - mae: 1.8771 - val_loss: 122.4102 - val_mae: 6.3215\n",
      "Epoch 37/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3675 - mae: 1.8674 - val_loss: 122.8992 - val_mae: 6.2539\n",
      "Epoch 38/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.4168 - mae: 1.8809 - val_loss: 120.7145 - val_mae: 6.3357\n",
      "Epoch 39/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.4609 - mae: 1.8804 - val_loss: 124.0938 - val_mae: 6.4393\n",
      "Epoch 40/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1281 - mae: 1.8523 - val_loss: 123.5781 - val_mae: 6.5087\n",
      "Epoch 41/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1861 - mae: 1.8911 - val_loss: 116.4893 - val_mae: 6.0506\n",
      "Epoch 42/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.2581 - mae: 1.8678 - val_loss: 119.9681 - val_mae: 6.2935\n",
      "Epoch 43/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.2509 - mae: 1.8657 - val_loss: 126.3272 - val_mae: 6.5347\n",
      "Epoch 44/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.2204 - mae: 1.8696 - val_loss: 125.7434 - val_mae: 6.2324\n",
      "Epoch 45/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1163 - mae: 1.8476 - val_loss: 118.4180 - val_mae: 6.0889\n",
      "Epoch 46/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.3690 - mae: 1.8999 - val_loss: 114.5898 - val_mae: 6.0058\n",
      "Epoch 47/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.2173 - mae: 1.8800 - val_loss: 125.2024 - val_mae: 6.2889\n",
      "Epoch 48/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 15.3013 - mae: 1.8588 - val_loss: 116.8431 - val_mae: 6.1704\n",
      "Epoch 49/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1931 - mae: 1.8571 - val_loss: 130.3667 - val_mae: 6.4035\n",
      "Epoch 50/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0571 - mae: 1.8421 - val_loss: 127.0198 - val_mae: 6.5434\n",
      "Epoch 51/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1185 - mae: 1.8641 - val_loss: 123.7846 - val_mae: 6.1696\n",
      "Epoch 52/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0348 - mae: 1.8432 - val_loss: 125.5732 - val_mae: 6.5882\n",
      "Epoch 53/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1583 - mae: 1.8639 - val_loss: 126.7167 - val_mae: 6.4426\n",
      "Epoch 54/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1669 - mae: 1.8425 - val_loss: 131.6313 - val_mae: 6.5895\n",
      "Epoch 55/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9892 - mae: 1.8415 - val_loss: 129.1374 - val_mae: 6.4164\n",
      "Epoch 56/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1187 - mae: 1.8734 - val_loss: 126.6099 - val_mae: 6.5162\n",
      "Epoch 57/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9852 - mae: 1.8568 - val_loss: 123.7888 - val_mae: 6.4149\n",
      "Epoch 58/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0846 - mae: 1.8738 - val_loss: 129.1393 - val_mae: 6.4840\n",
      "Epoch 59/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1776 - mae: 1.8871 - val_loss: 138.7616 - val_mae: 6.9704\n",
      "Epoch 60/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0652 - mae: 1.8662 - val_loss: 126.6804 - val_mae: 6.2399\n",
      "Epoch 61/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7695 - mae: 1.8278 - val_loss: 126.9373 - val_mae: 6.4530\n",
      "Epoch 62/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0181 - mae: 1.8538 - val_loss: 123.8156 - val_mae: 6.3746\n",
      "Epoch 63/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8455 - mae: 1.8297 - val_loss: 122.4368 - val_mae: 6.3679\n",
      "Epoch 64/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1181 - mae: 1.8450 - val_loss: 128.2309 - val_mae: 6.4960\n",
      "Epoch 65/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9652 - mae: 1.8597 - val_loss: 128.1703 - val_mae: 6.3241\n",
      "Epoch 66/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0325 - mae: 1.8402 - val_loss: 120.9074 - val_mae: 6.2033\n",
      "Epoch 67/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8406 - mae: 1.8398 - val_loss: 127.8166 - val_mae: 6.6229\n",
      "Epoch 68/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7743 - mae: 1.8254 - val_loss: 129.0757 - val_mae: 6.5243\n",
      "Epoch 69/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.1358 - mae: 1.8641 - val_loss: 123.8686 - val_mae: 6.2229\n",
      "Epoch 70/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9322 - mae: 1.8568 - val_loss: 131.5324 - val_mae: 6.6626\n",
      "Epoch 71/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9654 - mae: 1.8626 - val_loss: 122.0762 - val_mae: 6.1537\n",
      "Epoch 72/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9641 - mae: 1.8463 - val_loss: 128.5884 - val_mae: 6.5050\n",
      "Epoch 73/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8294 - mae: 1.8417 - val_loss: 135.3902 - val_mae: 6.8581\n",
      "Epoch 74/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8026 - mae: 1.8500 - val_loss: 141.7796 - val_mae: 6.9520\n",
      "Epoch 75/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7444 - mae: 1.8397 - val_loss: 137.5392 - val_mae: 6.8888\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "MSE: 107.17540054166666 R²: -0.22842422797693418 MAE: 4.464208333333334\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 10\n",
    "dia_siguiente = 13\n",
    "\n",
    "df_predicciones_13_11, df_inicio_actualizado, mse_13_11, r2_13_11, mae_13_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_13_11, \"R²:\", r2_13_11, \"MAE:\", mae_13_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  107.141\n",
      "Suma predicha:  0.0\n",
      "Desviación porcentual:  -100.0 %\n",
      "Suma previsión:  20.900000000000002\n",
      "Desviación porcentual:  -80.492995211917 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_13_11 = df_predicciones_13_11['E_SIMEL'].sum()\n",
    "suma_predicha_13_11 = df_predicciones_13_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_13_11 = df_predicciones_13_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_13_11)\n",
    "print(\"Suma predicha: \", suma_predicha_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9094 - mae: 1.8360 - val_loss: 133.4017 - val_mae: 6.6930\n",
      "Epoch 2/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8615 - mae: 1.8360 - val_loss: 130.8438 - val_mae: 6.5471\n",
      "Epoch 3/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9893 - mae: 1.8604 - val_loss: 125.1303 - val_mae: 6.2818\n",
      "Epoch 4/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8188 - mae: 1.8412 - val_loss: 130.3037 - val_mae: 6.5094\n",
      "Epoch 5/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 15.0295 - mae: 1.8468 - val_loss: 136.5609 - val_mae: 6.8880\n",
      "Epoch 6/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.9339 - mae: 1.8568 - val_loss: 130.2867 - val_mae: 6.4838\n",
      "Epoch 7/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8812 - mae: 1.8594 - val_loss: 123.1039 - val_mae: 6.1942\n",
      "Epoch 8/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8019 - mae: 1.8441 - val_loss: 129.3000 - val_mae: 6.5021\n",
      "Epoch 9/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8884 - mae: 1.8484 - val_loss: 137.2124 - val_mae: 6.9563\n",
      "Epoch 10/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8125 - mae: 1.8389 - val_loss: 132.0859 - val_mae: 6.5700\n",
      "Epoch 11/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8705 - mae: 1.8564 - val_loss: 130.3583 - val_mae: 6.4655\n",
      "Epoch 12/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7733 - mae: 1.8654 - val_loss: 127.8621 - val_mae: 6.6465\n",
      "Epoch 13/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.9918 - mae: 1.8746 - val_loss: 129.8596 - val_mae: 6.6592\n",
      "Epoch 14/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.8318 - mae: 1.8402 - val_loss: 128.8492 - val_mae: 6.4130\n",
      "Epoch 15/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6465 - mae: 1.8294 - val_loss: 138.3505 - val_mae: 6.6944\n",
      "Epoch 16/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7599 - mae: 1.8453 - val_loss: 132.3530 - val_mae: 6.9325\n",
      "Epoch 17/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6285 - mae: 1.8465 - val_loss: 135.4677 - val_mae: 6.7701\n",
      "Epoch 18/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.4916 - mae: 1.8127 - val_loss: 134.5531 - val_mae: 6.6540\n",
      "Epoch 19/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6794 - mae: 1.8404 - val_loss: 132.6594 - val_mae: 6.5107\n",
      "Epoch 20/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7194 - mae: 1.8356 - val_loss: 121.8730 - val_mae: 6.1788\n",
      "Epoch 21/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.6719 - mae: 1.8477 - val_loss: 134.3589 - val_mae: 6.8498\n",
      "Epoch 22/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.7103 - mae: 1.8433 - val_loss: 130.6595 - val_mae: 6.7418\n",
      "Epoch 23/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6324 - mae: 1.8346 - val_loss: 129.4155 - val_mae: 6.4905\n",
      "Epoch 24/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6416 - mae: 1.8348 - val_loss: 132.1722 - val_mae: 6.4253\n",
      "Epoch 25/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.6962 - mae: 1.8308 - val_loss: 142.5338 - val_mae: 6.8684\n",
      "Epoch 26/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6149 - mae: 1.8209 - val_loss: 129.3265 - val_mae: 6.5382\n",
      "Epoch 27/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.5812 - mae: 1.8430 - val_loss: 131.7605 - val_mae: 6.4837\n",
      "Epoch 28/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.5711 - mae: 1.8464 - val_loss: 133.9311 - val_mae: 6.5753\n",
      "Epoch 29/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.4270 - mae: 1.8213 - val_loss: 148.4395 - val_mae: 7.0890\n",
      "Epoch 30/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.4479 - mae: 1.8026 - val_loss: 133.1159 - val_mae: 6.6236\n",
      "Epoch 31/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.4326 - mae: 1.8243 - val_loss: 137.3082 - val_mae: 6.8273\n",
      "Epoch 32/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.6164 - mae: 1.8448 - val_loss: 135.4584 - val_mae: 6.7288\n",
      "Epoch 33/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.4838 - mae: 1.8298 - val_loss: 134.2832 - val_mae: 6.7666\n",
      "Epoch 34/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.4735 - mae: 1.8155 - val_loss: 138.5260 - val_mae: 6.8320\n",
      "Epoch 35/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.5123 - mae: 1.8331 - val_loss: 133.2099 - val_mae: 6.8003\n",
      "Epoch 36/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.3692 - mae: 1.8316 - val_loss: 139.2200 - val_mae: 6.8530\n",
      "Epoch 37/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.5154 - mae: 1.8222 - val_loss: 139.0975 - val_mae: 6.8159\n",
      "Epoch 38/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.5280 - mae: 1.8286 - val_loss: 130.7243 - val_mae: 6.8262\n",
      "Epoch 39/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.3900 - mae: 1.8264 - val_loss: 137.9538 - val_mae: 6.7867\n",
      "Epoch 40/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.3709 - mae: 1.8124 - val_loss: 142.0867 - val_mae: 6.8458\n",
      "Epoch 41/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.4937 - mae: 1.8183 - val_loss: 138.2486 - val_mae: 6.9411\n",
      "Epoch 42/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.5621 - mae: 1.8270 - val_loss: 131.5547 - val_mae: 6.7102\n",
      "Epoch 43/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.3533 - mae: 1.8083 - val_loss: 132.8962 - val_mae: 6.5866\n",
      "Epoch 44/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.3237 - mae: 1.8197 - val_loss: 137.9288 - val_mae: 6.8578\n",
      "Epoch 45/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2839 - mae: 1.7896 - val_loss: 139.7505 - val_mae: 6.7872\n",
      "Epoch 46/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1611 - mae: 1.7925 - val_loss: 130.3141 - val_mae: 6.4220\n",
      "Epoch 47/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2537 - mae: 1.8119 - val_loss: 134.2704 - val_mae: 6.5369\n",
      "Epoch 48/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2021 - mae: 1.8141 - val_loss: 136.9220 - val_mae: 6.7456\n",
      "Epoch 49/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.3552 - mae: 1.8254 - val_loss: 138.9746 - val_mae: 6.8130\n",
      "Epoch 50/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2971 - mae: 1.7996 - val_loss: 142.1982 - val_mae: 6.8006\n",
      "Epoch 51/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2376 - mae: 1.7868 - val_loss: 133.1678 - val_mae: 6.9026\n",
      "Epoch 52/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2661 - mae: 1.8054 - val_loss: 133.1760 - val_mae: 6.6973\n",
      "Epoch 53/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2662 - mae: 1.8016 - val_loss: 131.7668 - val_mae: 6.7138\n",
      "Epoch 54/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.2169 - mae: 1.8038 - val_loss: 135.6646 - val_mae: 6.7499\n",
      "Epoch 55/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2636 - mae: 1.8161 - val_loss: 147.1030 - val_mae: 6.8751\n",
      "Epoch 56/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2476 - mae: 1.8211 - val_loss: 145.6076 - val_mae: 6.9310\n",
      "Epoch 57/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2920 - mae: 1.8062 - val_loss: 135.0363 - val_mae: 6.7442\n",
      "Epoch 58/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2108 - mae: 1.8140 - val_loss: 129.0106 - val_mae: 6.4253\n",
      "Epoch 59/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2357 - mae: 1.8000 - val_loss: 158.5305 - val_mae: 7.6454\n",
      "Epoch 60/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2766 - mae: 1.8311 - val_loss: 143.1629 - val_mae: 6.9533\n",
      "Epoch 61/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2268 - mae: 1.8270 - val_loss: 149.1782 - val_mae: 7.0150\n",
      "Epoch 62/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1485 - mae: 1.8064 - val_loss: 145.7970 - val_mae: 6.9457\n",
      "Epoch 63/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1393 - mae: 1.8039 - val_loss: 144.9260 - val_mae: 7.0140\n",
      "Epoch 64/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1549 - mae: 1.8162 - val_loss: 132.8750 - val_mae: 6.5792\n",
      "Epoch 65/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.2820 - mae: 1.8240 - val_loss: 142.0432 - val_mae: 7.0623\n",
      "Epoch 66/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1044 - mae: 1.8010 - val_loss: 135.8890 - val_mae: 6.8024\n",
      "Epoch 67/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1404 - mae: 1.8062 - val_loss: 139.6355 - val_mae: 6.6989\n",
      "Epoch 68/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 13.9751 - mae: 1.7846 - val_loss: 131.9076 - val_mae: 6.7616\n",
      "Epoch 69/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1301 - mae: 1.7930 - val_loss: 135.3959 - val_mae: 6.8718\n",
      "Epoch 70/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1268 - mae: 1.8086 - val_loss: 145.2066 - val_mae: 7.1541\n",
      "Epoch 71/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.0514 - mae: 1.8051 - val_loss: 140.7385 - val_mae: 6.6403\n",
      "Epoch 72/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.1881 - mae: 1.7935 - val_loss: 139.3121 - val_mae: 6.7216\n",
      "Epoch 73/75\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 14.1829 - mae: 1.8178 - val_loss: 143.3933 - val_mae: 6.7937\n",
      "Epoch 74/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.0155 - mae: 1.7971 - val_loss: 146.4875 - val_mae: 7.0673\n",
      "Epoch 75/75\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 14.0507 - mae: 1.7923 - val_loss: 142.2595 - val_mae: 7.0168\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "MSE: 83.69682493252928 R²: 0.11100502121015265 MAE: 4.090322758992513\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 13\n",
    "dia_siguiente = 14\n",
    "\n",
    "df_predicciones_14_11, df_inicio_actualizado, mse_14_11, r2_14_11, mae_14_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_14_11, \"R²:\", r2_14_11, \"MAE:\", mae_14_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  112.33099999999999\n",
      "Suma predicha:  14.163253784179688\n",
      "Desviación porcentual:  -87.39150031230943 %\n",
      "Suma previsión:  120.5\n",
      "Desviación porcentual:  7.272257880727503 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_14_11 = df_predicciones_14_11['E_SIMEL'].sum()\n",
    "suma_predicha_14_11 = df_predicciones_14_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_14_11 = df_predicciones_14_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_14_11)\n",
    "print(\"Suma predicha: \", suma_predicha_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9948 - mae: 1.7905 - val_loss: 141.8430 - val_mae: 6.7528\n",
      "Epoch 2/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 14.0725 - mae: 1.7873 - val_loss: 142.2749 - val_mae: 6.8270\n",
      "Epoch 3/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 14.1516 - mae: 1.8140 - val_loss: 139.4073 - val_mae: 6.7659\n",
      "Epoch 4/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9381 - mae: 1.8031 - val_loss: 141.0948 - val_mae: 7.0118\n",
      "Epoch 5/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.8802 - mae: 1.7892 - val_loss: 137.1393 - val_mae: 6.7593\n",
      "Epoch 6/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7777 - mae: 1.7783 - val_loss: 136.0222 - val_mae: 6.6676\n",
      "Epoch 7/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 14.0735 - mae: 1.8235 - val_loss: 144.7623 - val_mae: 6.8497\n",
      "Epoch 8/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 14.0475 - mae: 1.8012 - val_loss: 143.6189 - val_mae: 7.0969\n",
      "Epoch 9/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 14.0919 - mae: 1.7951 - val_loss: 140.8154 - val_mae: 6.8384\n",
      "Epoch 10/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9410 - mae: 1.8079 - val_loss: 136.2535 - val_mae: 6.6856\n",
      "Epoch 11/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9460 - mae: 1.8105 - val_loss: 140.8992 - val_mae: 6.9102\n",
      "Epoch 12/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9231 - mae: 1.8072 - val_loss: 146.8439 - val_mae: 6.9800\n",
      "Epoch 13/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 14.0056 - mae: 1.7992 - val_loss: 140.7304 - val_mae: 6.8548\n",
      "Epoch 14/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7641 - mae: 1.7720 - val_loss: 140.7852 - val_mae: 7.1061\n",
      "Epoch 15/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.9436 - mae: 1.8068 - val_loss: 144.0079 - val_mae: 6.7596\n",
      "Epoch 16/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7492 - mae: 1.7789 - val_loss: 141.3729 - val_mae: 6.7646\n",
      "Epoch 17/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.8427 - mae: 1.7908 - val_loss: 142.8235 - val_mae: 6.9614\n",
      "Epoch 18/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9644 - mae: 1.8043 - val_loss: 144.4793 - val_mae: 6.9902\n",
      "Epoch 19/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9339 - mae: 1.7891 - val_loss: 141.2476 - val_mae: 6.8526\n",
      "Epoch 20/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9202 - mae: 1.7980 - val_loss: 139.5446 - val_mae: 6.8838\n",
      "Epoch 21/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.8178 - mae: 1.7980 - val_loss: 137.7885 - val_mae: 6.5906\n",
      "Epoch 22/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7730 - mae: 1.7934 - val_loss: 140.2379 - val_mae: 6.7772\n",
      "Epoch 23/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7829 - mae: 1.7999 - val_loss: 134.3930 - val_mae: 6.6060\n",
      "Epoch 24/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7646 - mae: 1.7902 - val_loss: 151.3784 - val_mae: 7.1432\n",
      "Epoch 25/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6700 - mae: 1.7657 - val_loss: 148.8494 - val_mae: 7.0937\n",
      "Epoch 26/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9144 - mae: 1.8109 - val_loss: 140.9608 - val_mae: 6.7251\n",
      "Epoch 27/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7332 - mae: 1.7866 - val_loss: 144.7360 - val_mae: 6.9420\n",
      "Epoch 28/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7875 - mae: 1.8026 - val_loss: 146.7917 - val_mae: 7.0518\n",
      "Epoch 29/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.8042 - mae: 1.7789 - val_loss: 140.0728 - val_mae: 6.7056\n",
      "Epoch 30/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.8588 - mae: 1.8091 - val_loss: 158.0088 - val_mae: 7.3372\n",
      "Epoch 31/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7421 - mae: 1.7880 - val_loss: 145.9174 - val_mae: 6.8924\n",
      "Epoch 32/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7037 - mae: 1.7808 - val_loss: 140.5437 - val_mae: 6.7527\n",
      "Epoch 33/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6507 - mae: 1.7758 - val_loss: 141.2247 - val_mae: 6.8090\n",
      "Epoch 34/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6426 - mae: 1.7701 - val_loss: 140.4711 - val_mae: 6.7768\n",
      "Epoch 35/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.7365 - mae: 1.7806 - val_loss: 154.9606 - val_mae: 7.1110\n",
      "Epoch 36/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5605 - mae: 1.7674 - val_loss: 147.5487 - val_mae: 6.9845\n",
      "Epoch 37/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7811 - mae: 1.7839 - val_loss: 135.7338 - val_mae: 6.6642\n",
      "Epoch 38/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7012 - mae: 1.7844 - val_loss: 152.5993 - val_mae: 7.0651\n",
      "Epoch 39/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.9015 - mae: 1.8011 - val_loss: 148.8953 - val_mae: 6.9572\n",
      "Epoch 40/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7875 - mae: 1.7821 - val_loss: 151.6803 - val_mae: 7.2064\n",
      "Epoch 41/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6374 - mae: 1.7767 - val_loss: 145.5427 - val_mae: 7.0142\n",
      "Epoch 42/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.6731 - mae: 1.7743 - val_loss: 152.1819 - val_mae: 7.2161\n",
      "Epoch 43/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5908 - mae: 1.7670 - val_loss: 143.6856 - val_mae: 6.8735\n",
      "Epoch 44/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6318 - mae: 1.7782 - val_loss: 147.9273 - val_mae: 6.9954\n",
      "Epoch 45/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5909 - mae: 1.7669 - val_loss: 146.1954 - val_mae: 7.1450\n",
      "Epoch 46/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6257 - mae: 1.7768 - val_loss: 135.5882 - val_mae: 6.7166\n",
      "Epoch 47/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6214 - mae: 1.7667 - val_loss: 147.0680 - val_mae: 6.8873\n",
      "Epoch 48/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5944 - mae: 1.7776 - val_loss: 151.7847 - val_mae: 7.0736\n",
      "Epoch 49/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.7265 - mae: 1.7936 - val_loss: 151.3416 - val_mae: 6.9919\n",
      "Epoch 50/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5940 - mae: 1.7555 - val_loss: 144.2117 - val_mae: 6.8221\n",
      "Epoch 51/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5898 - mae: 1.7690 - val_loss: 148.7891 - val_mae: 7.0615\n",
      "Epoch 52/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5869 - mae: 1.8012 - val_loss: 146.4245 - val_mae: 7.0392\n",
      "Epoch 53/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6286 - mae: 1.7841 - val_loss: 149.5088 - val_mae: 6.9706\n",
      "Epoch 54/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.4532 - mae: 1.7588 - val_loss: 160.7552 - val_mae: 7.2703\n",
      "Epoch 55/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.5079 - mae: 1.7685 - val_loss: 147.8294 - val_mae: 7.2096\n",
      "Epoch 56/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5078 - mae: 1.7652 - val_loss: 139.9387 - val_mae: 6.8673\n",
      "Epoch 57/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6006 - mae: 1.7725 - val_loss: 144.4949 - val_mae: 6.9613\n",
      "Epoch 58/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5936 - mae: 1.7831 - val_loss: 137.0270 - val_mae: 6.5427\n",
      "Epoch 59/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3391 - mae: 1.7530 - val_loss: 148.0043 - val_mae: 6.9556\n",
      "Epoch 60/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.4723 - mae: 1.7732 - val_loss: 142.5862 - val_mae: 6.9089\n",
      "Epoch 61/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3605 - mae: 1.7613 - val_loss: 147.5312 - val_mae: 7.0050\n",
      "Epoch 62/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5584 - mae: 1.7795 - val_loss: 156.1417 - val_mae: 7.3467\n",
      "Epoch 63/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6782 - mae: 1.7904 - val_loss: 141.6228 - val_mae: 6.8545\n",
      "Epoch 64/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.4818 - mae: 1.7689 - val_loss: 150.1299 - val_mae: 7.0275\n",
      "Epoch 65/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5047 - mae: 1.7661 - val_loss: 148.9246 - val_mae: 6.9742\n",
      "Epoch 66/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.3947 - mae: 1.7508 - val_loss: 143.6230 - val_mae: 6.8023\n",
      "Epoch 67/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.2249 - mae: 1.7449 - val_loss: 149.2186 - val_mae: 6.9168\n",
      "Epoch 68/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5424 - mae: 1.7923 - val_loss: 139.8878 - val_mae: 6.7182\n",
      "Epoch 69/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.5377 - mae: 1.7789 - val_loss: 142.1124 - val_mae: 6.7341\n",
      "Epoch 70/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.4214 - mae: 1.7747 - val_loss: 150.2252 - val_mae: 6.9746\n",
      "Epoch 71/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.6041 - mae: 1.7779 - val_loss: 142.2061 - val_mae: 6.8428\n",
      "Epoch 72/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3982 - mae: 1.7706 - val_loss: 146.3784 - val_mae: 6.9037\n",
      "Epoch 73/75\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 13.2966 - mae: 1.7684 - val_loss: 144.4362 - val_mae: 6.7879\n",
      "Epoch 74/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3762 - mae: 1.7797 - val_loss: 149.0108 - val_mae: 6.9465\n",
      "Epoch 75/75\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 13.3832 - mae: 1.7533 - val_loss: 154.5871 - val_mae: 7.2405\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "MSE: 64.26332468180222 R²: -0.25634889138558914 MAE: 3.56435409228007\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 14\n",
    "dia_siguiente = 15\n",
    "\n",
    "df_predicciones_15_11, df_inicio_actualizado, mse_15_11, r2_15_11, mae_15_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_15_11, \"R²:\", r2_15_11, \"MAE:\", mae_15_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  88.333\n",
      "Suma predicha:  7.528628349304199\n",
      "Desviación porcentual:  -91.47699234792864 %\n",
      "Suma previsión:  131.89999999999998\n",
      "Desviación porcentual:  49.32131819365354 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_15_11 = df_predicciones_15_11['E_SIMEL'].sum()\n",
    "suma_predicha_15_11 = df_predicciones_15_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_15_11 = df_predicciones_15_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_15_11)\n",
    "print(\"Suma predicha: \", suma_predicha_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todos los Dataframes que contienen las prediccions, previsiones y datos reales para calcular las méstricas en conjunto\n",
    "\n",
    "df_predicciones_totales = pd.concat([df_final_05_11, df_final_06_11, df_predicciones_07_11, \n",
    "                                     df_predicciones_08_11, df_predicciones_09_11,df_predicciones_10_11, \n",
    "                                     df_predicciones_13_11, df_predicciones_14_11, df_predicciones_15_11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Predicciones:  2.6799377686568984\n",
      "MSE Predicciones:  44.57494542511017\n",
      "R² Predicciones:  0.11868249182433688\n",
      "MAE Previsiones:  2.3999490740740743\n",
      "MSE Previsiones:  35.93822944907407\n",
      "R² Previsiones:  0.2894440918718205\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de las métricas para ver si nos indican si mejoran los errores entren la predicción (Prediccion_E_SIMEL), previsión (PREVISION)y producción real(E_SIMEL)\n",
    "\n",
    "def calcular_metricas(df):\n",
    "    mae = mean_absolute_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    mse = mean_squared_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    return mae, mse, r2\n",
    "\n",
    "# Métricas para la predicciones\n",
    "\n",
    "mae_pred, mse_pred, r2_pred = calcular_metricas(df_predicciones_totales)\n",
    "\n",
    "# Cambiamos la columna de predicción por la de previsión\n",
    "\n",
    "df_previsiones = df_predicciones_totales.copy()\n",
    "df_previsiones['Prediccion_E_SIMEL'] = df_previsiones['PREVISION']\n",
    "\n",
    "# Métricas para la previsión\n",
    "\n",
    "mae_prev, mse_prev, r2_prev = calcular_metricas(df_previsiones)\n",
    "\n",
    "# Visualizamos los resultados\n",
    "\n",
    "print(\"MAE Predicciones: \", mae_pred)\n",
    "print(\"MSE Predicciones: \", mse_pred)\n",
    "print(\"R² Predicciones: \", r2_pred)\n",
    "print(\"MAE Previsiones: \", mae_prev)\n",
    "print(\"MSE Previsiones: \", mse_prev)\n",
    "print(\"R² Previsiones: \", r2_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLas métricas visualizadas nos indican que el modelo utilizado mejoran las métricas de las previsiones, vamos \\na ver a continuación si las métricas reflejan esta mejora en predicción respecto la previsión.\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Las métricas visualizadas nos indican que el modelo utilizado mejoran las métricas de las previsiones, vamos \n",
    "a ver a continuación si las métricas reflejan esta mejora en predicción respecto la previsión.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de los valores en la columna E_SIMEL: 622.953\n",
      "Suma de las predicciones: 228.6751546561718\n",
      "Suma de las previsiones : 696.6\n",
      "Diferencia entre predicciones totales y E_SIMEL total: 394.2778453438282\n",
      "Diferencia entre previsiones y E_SIMEL total: 73.64700000000005\n",
      "No mejoramos la predicción respecto la PREVISION real en: 320.6308453438281, por lo tanto, con este modelo, no estamos mejorando las previsiones.\n"
     ]
    }
   ],
   "source": [
    "# Para saber si llevamos una acumulado positivo de predicciones vs previsiones sobre la producción real,\n",
    "# calculamos las diferencias absolutas\n",
    "\n",
    "# Diferencia absoluta entre la predición y la producción real\n",
    "\n",
    "df_predicciones_totales['diff_pred_real'] = abs(df_predicciones_totales['Prediccion_E_SIMEL'] - df_predicciones_totales['E_SIMEL'])\n",
    "\n",
    "# Diferencia absoluta entre la previsión y la producción real\n",
    "\n",
    "df_predicciones_totales['diff_prev_real'] = abs(df_predicciones_totales['PREVISION'] - df_predicciones_totales['E_SIMEL'])\n",
    "\n",
    "\n",
    "# sumamos todos los valores de las columnas que queremo comparar\n",
    "\n",
    "suma_e_simel = df_predicciones_totales['E_SIMEL'].sum()\n",
    "sumas_totales_predicciones = df_predicciones_totales['Prediccion_E_SIMEL'].sum()\n",
    "sumas_previsiones = df_predicciones_totales['PREVISION'].sum()\n",
    "\n",
    "\n",
    "# Calculamos las diferencias entre la prediccion y la previsión respecto la producción real E_SIMEL\n",
    "\n",
    "diferencia_prediccion_vs_produccion_real = abs(sumas_totales_predicciones - suma_e_simel)\n",
    "diferencia_prevision_vs_produccion_real = abs(sumas_previsiones - suma_e_simel)\n",
    "\n",
    "\n",
    "# Imprimimos los resultados para poder visualizar si mejoramos las previsiones a lo largo de todas las predicciones.\n",
    "\n",
    "print(f\"Suma de los valores en la columna E_SIMEL: {suma_e_simel}\")\n",
    "print(f\"Suma de las predicciones: {sumas_totales_predicciones}\")\n",
    "print(f\"Suma de las previsiones : {sumas_previsiones}\")\n",
    "\n",
    "\n",
    "print(f\"Diferencia entre predicciones totales y E_SIMEL total: {diferencia_prediccion_vs_produccion_real}\")\n",
    "print(f\"Diferencia entre previsiones y E_SIMEL total: {diferencia_prevision_vs_produccion_real}\")\n",
    "\n",
    "\n",
    "# Calculamos la diferencia entre la predicción y la previsión para saber si el modelo de predicción mejora la previsión\n",
    "\n",
    "diferencia = diferencia_prediccion_vs_produccion_real - diferencia_prevision_vs_produccion_real\n",
    "\n",
    "if diferencia_prediccion_vs_produccion_real > diferencia_prevision_vs_produccion_real:\n",
    "    print(f\"No mejoramos la predicción respecto la PREVISION real en: {diferencia}, por lo tanto, con este modelo, no estamos mejorando las previsiones.\")\n",
    "else:\n",
    "    print(f\"La predicción es MEJOR que la previsión en: {-diferencia} unidades, por lo tanto, cumplimos nuestro objetivo de mejorar la PREVISIÓN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696.5999999999999"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_prevision = df_final['PREVISION'].sum()\n",
    "suma_prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622.953"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_E_SIMEL = df_final['E_SIMEL'].sum()\n",
    "suma_E_SIMEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
