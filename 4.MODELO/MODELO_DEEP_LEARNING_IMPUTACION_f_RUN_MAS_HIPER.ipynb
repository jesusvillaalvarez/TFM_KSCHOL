{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "\n",
    "import pandas as pd  # Manipulación y análisis de datos.\n",
    "import numpy as np  # Soporte para vectores y matrices.\n",
    "\n",
    "# Gráficos\n",
    "\n",
    "import matplotlib.pyplot as plt  # Creación de gráficos estáticos, animados e interactivos.\n",
    "from matplotlib import style  # Personalización del estilo de los gráficos.\n",
    "\n",
    "# Preprocesado y modelado\n",
    "\n",
    "from scipy.stats import pearsonr  # Coeficiente de correlación de Pearson.\n",
    "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba.\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # Métricas para evaluar modelos.\n",
    "import statsmodels.api as sm  # Modelos estadísticos y econometricos.\n",
    "import statsmodels.formula.api as smf  # Modelo estadísticos con fórmulas.\n",
    "from statsmodels.stats.anova import anova_lm  # Análisis de varianza.\n",
    "from scipy import stats  # Funciones estadísticas.\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler  # Preprocesamiento de datos.\n",
    "import category_encoders  # Codificación de variables categóricas.\n",
    "import missingno as msno  # Visualización de datos faltantes.\n",
    "from sklearn.pipeline import Pipeline  # Cadena de transformaciones con un estimador final.\n",
    "from sklearn.experimental import enable_iterative_imputer  # Permitir uso de IterativeImputer.\n",
    "from sklearn.impute import IterativeImputer  # Imputación de datos faltantes.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Modelos de ensamble.\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score  # Búsqueda de hiperparámetros y validación cruzada.\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf  # Biblioteca de Deep Learning.\n",
    "from tensorflow.keras import layers, models  # Construcción de modelos de deep learning.\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Callbacks para controlar el entrenamiento.\n",
    "from keras.models import Sequential  # Creación de modelos secuenciales.\n",
    "from keras.layers import Dense, Dropout, BatchNormalization  # Capas para construir modelos.\n",
    "from keras import regularizers  # Regularización de modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Period</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>DESVIO</th>\n",
       "      <th>f_PREV_HIGH</th>\n",
       "      <th>f_PREV_LOW</th>\n",
       "      <th>f_RUN</th>\n",
       "      <th>Dia_Semana</th>\n",
       "      <th>Es_fin_semana</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Día</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Fecha  Period  PREVISION  E_SIMEL  DESVIO  f_PREV_HIGH  \\\n",
       "0           0  2021-01-01       1        0.0      0.0     0.0            0   \n",
       "1           1  2021-01-01       2        0.0      0.0     0.0            0   \n",
       "2           2  2021-01-01       3        0.0      0.0     0.0            0   \n",
       "3           3  2021-01-01       4        0.0      0.0     0.0            0   \n",
       "4           4  2021-01-01       5        0.0      0.0     0.0            0   \n",
       "\n",
       "   f_PREV_LOW  f_RUN  Dia_Semana  Es_fin_semana   Año  Mes  Día  \n",
       "0           0      0           4          False  2021    1    1  \n",
       "1           0      0           4          False  2021    1    1  \n",
       "2           0      0           4          False  2021    1    1  \n",
       "3           0      0           4          False  2021    1    1  \n",
       "4           0      0           4          False  2021    1    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el archivo csv con los datos\n",
    "\n",
    "df_central = pd.read_csv(\"https://raw.githubusercontent.com/jesusvillaalvarez/TFM_KSCHOL/main/5.ARCHIVOS/df_central_2_1.csv\")\n",
    "\n",
    "df_central.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas innecesarias\n",
    "\n",
    "df_central = df_central.drop(columns=['Unnamed: 0', 'DESVIO', 'f_PREV_HIGH', 'f_PREV_LOW'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna 'Fecha' a Datetime para hacer las división en dos dfs\n",
    "\n",
    "df_central['Fecha'] = pd.to_datetime(df_central['Fecha'])\n",
    "\n",
    "# Dividimos el DataFrame en dos según las fechas especificas\n",
    "\n",
    "df_inicio = df_central[df_central['Fecha'] <= '2023-10-31']\n",
    "df_final = df_central[df_central['Fecha'] >= '2023-11-05']\n",
    "\n",
    "# Eliminamos la columna 'Fecha' de ambos DataFrames para poder preparar el modelo de Deep Learning\n",
    "\n",
    "df_inicio = df_inicio.drop(columns=['Fecha'])\n",
    "df_final = df_final.drop(columns=['Fecha'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19764, 8), (4942, 8), (19764,), (4942,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos df_inicio eliminando la variables objetivo del conjunto de entrenamiento (X) y especificamos la variable objetivo del conjunto de prueba (y)\n",
    "\n",
    "X = df_inicio.drop('E_SIMEL', axis=1)\n",
    "y = df_inicio['E_SIMEL']\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizamos las características, paso necesario para el modelo de deep learning\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verificamos las dimensiones de los conjuntos de datos para asegurarnos de que todo está correcto\n",
    "\n",
    "(X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "495/495 [==============================] - 2s 2ms/step - loss: 85.6433 - mae: 4.6969 - val_loss: 40.2485 - val_mae: 3.0502 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 55.2828 - mae: 3.7875 - val_loss: 38.9727 - val_mae: 2.8392 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 51.0915 - mae: 3.6202 - val_loss: 38.4024 - val_mae: 2.8108 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 50.6699 - mae: 3.5848 - val_loss: 39.6561 - val_mae: 3.0204 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 49.8041 - mae: 3.5650 - val_loss: 39.3850 - val_mae: 2.8001 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 52.4358 - mae: 3.6521 - val_loss: 38.0845 - val_mae: 2.8480 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 49.4967 - mae: 3.5309 - val_loss: 37.8001 - val_mae: 2.8037 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 48.8088 - mae: 3.5226 - val_loss: 37.9280 - val_mae: 2.8670 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 49.1661 - mae: 3.4819 - val_loss: 38.2392 - val_mae: 2.9206 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 47.5056 - mae: 3.4166 - val_loss: 37.7183 - val_mae: 2.7062 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 47.8455 - mae: 3.4268 - val_loss: 37.9482 - val_mae: 2.8303 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 48.3716 - mae: 3.4690 - val_loss: 38.0335 - val_mae: 2.8822 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 47.1832 - mae: 3.4010 - val_loss: 37.1356 - val_mae: 2.6836 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 46.9863 - mae: 3.4051 - val_loss: 37.4340 - val_mae: 2.6741 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 46.3414 - mae: 3.3770 - val_loss: 38.1508 - val_mae: 2.9067 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 45.5630 - mae: 3.3443 - val_loss: 37.2719 - val_mae: 2.6004 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 45.8155 - mae: 3.3397 - val_loss: 37.7990 - val_mae: 2.8581 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 46.3797 - mae: 3.3688 - val_loss: 36.7191 - val_mae: 2.7518 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 45.7309 - mae: 3.3323 - val_loss: 37.3781 - val_mae: 2.8324 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 46.7288 - mae: 3.3919 - val_loss: 38.5052 - val_mae: 2.9298 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 44.8779 - mae: 3.3347 - val_loss: 36.3039 - val_mae: 2.7548 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 44.9568 - mae: 3.3382 - val_loss: 36.5221 - val_mae: 2.7584 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 44.5654 - mae: 3.3139 - val_loss: 36.8511 - val_mae: 2.7676 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 46.2147 - mae: 3.3749 - val_loss: 36.2655 - val_mae: 2.5918 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 44.9588 - mae: 3.3388 - val_loss: 37.4439 - val_mae: 2.6053 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 44.9908 - mae: 3.3600 - val_loss: 36.6397 - val_mae: 2.7370 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.7263 - mae: 3.3167 - val_loss: 36.2679 - val_mae: 2.6925 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 44.0648 - mae: 3.3152 - val_loss: 36.0761 - val_mae: 2.6825 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.9365 - mae: 3.3414 - val_loss: 35.9530 - val_mae: 2.6229 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.7640 - mae: 3.3338 - val_loss: 36.0124 - val_mae: 2.6939 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 44.7785 - mae: 3.3577 - val_loss: 35.8307 - val_mae: 2.6582 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.6414 - mae: 3.3123 - val_loss: 36.0157 - val_mae: 2.7129 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 43.4650 - mae: 3.3092 - val_loss: 36.8015 - val_mae: 2.8805 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.9467 - mae: 3.3356 - val_loss: 36.8679 - val_mae: 2.6851 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.2963 - mae: 3.2691 - val_loss: 36.3811 - val_mae: 2.6466 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.1963 - mae: 3.2993 - val_loss: 37.2278 - val_mae: 2.8942 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 42.5501 - mae: 3.2790 - val_loss: 36.1862 - val_mae: 2.7347 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.8112 - mae: 3.3064 - val_loss: 36.0767 - val_mae: 2.6940 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 44.3036 - mae: 3.3489 - val_loss: 35.4942 - val_mae: 2.7293 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.1214 - mae: 3.3245 - val_loss: 35.6463 - val_mae: 2.6146 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 42.7408 - mae: 3.3003 - val_loss: 36.8328 - val_mae: 2.8636 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.8603 - mae: 3.2891 - val_loss: 35.6449 - val_mae: 2.6528 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.5600 - mae: 3.2970 - val_loss: 35.8218 - val_mae: 2.7396 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.0286 - mae: 3.3110 - val_loss: 35.3315 - val_mae: 2.6886 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.2972 - mae: 3.2969 - val_loss: 36.3579 - val_mae: 2.8441 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 42.1352 - mae: 3.3034 - val_loss: 35.2979 - val_mae: 2.5680 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 43.0568 - mae: 3.3379 - val_loss: 35.3414 - val_mae: 2.7304 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.4409 - mae: 3.2897 - val_loss: 34.8971 - val_mae: 2.6445 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.4092 - mae: 3.3037 - val_loss: 36.5165 - val_mae: 2.8551 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 43.2764 - mae: 3.3488 - val_loss: 35.1563 - val_mae: 2.6936 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.7096 - mae: 3.2848 - val_loss: 34.8142 - val_mae: 2.7182 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.9425 - mae: 3.2932 - val_loss: 34.9575 - val_mae: 2.6720 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 42.1462 - mae: 3.3221 - val_loss: 35.0619 - val_mae: 2.7205 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.4242 - mae: 3.3156 - val_loss: 34.4804 - val_mae: 2.6612 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 43.1366 - mae: 3.3670 - val_loss: 35.2847 - val_mae: 2.4872 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.6752 - mae: 3.3110 - val_loss: 34.6351 - val_mae: 2.6011 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.4895 - mae: 3.3108 - val_loss: 34.6489 - val_mae: 2.6651 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.9166 - mae: 3.3481 - val_loss: 35.0005 - val_mae: 2.6348 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.9862 - mae: 3.2831 - val_loss: 34.9725 - val_mae: 2.6824 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.9207 - mae: 3.3461 - val_loss: 35.3005 - val_mae: 2.6203 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.8021 - mae: 3.3113 - val_loss: 35.8870 - val_mae: 2.5251 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.1567 - mae: 3.3325 - val_loss: 34.7396 - val_mae: 2.6715 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.9522 - mae: 3.3342 - val_loss: 34.8419 - val_mae: 2.6620 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.4860 - mae: 3.2954 - val_loss: 35.0296 - val_mae: 2.7954 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.0158 - mae: 3.3613 - val_loss: 35.3676 - val_mae: 2.6861 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.7638 - mae: 3.3732 - val_loss: 35.1845 - val_mae: 2.7445 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.5816 - mae: 3.3353 - val_loss: 34.6982 - val_mae: 2.7278 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.0842 - mae: 3.3364 - val_loss: 35.2403 - val_mae: 2.7416 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.7675 - mae: 3.3162 - val_loss: 35.7610 - val_mae: 2.7043 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.5685 - mae: 3.3623 - val_loss: 34.5816 - val_mae: 2.6341 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.3171 - mae: 3.2905 - val_loss: 34.7265 - val_mae: 2.6738 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 42.0562 - mae: 3.3749 - val_loss: 34.8645 - val_mae: 2.6093 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.3814 - mae: 3.3570 - val_loss: 34.4130 - val_mae: 2.6621 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.6983 - mae: 3.3345 - val_loss: 34.9297 - val_mae: 2.6004 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.1889 - mae: 3.3566 - val_loss: 34.9252 - val_mae: 2.6318 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.1243 - mae: 3.3589 - val_loss: 35.1316 - val_mae: 2.5139 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.5985 - mae: 3.3369 - val_loss: 36.5451 - val_mae: 2.4445 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.1339 - mae: 3.3539 - val_loss: 35.6589 - val_mae: 2.8061 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.5376 - mae: 3.3801 - val_loss: 34.9785 - val_mae: 2.5843 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.5267 - mae: 3.3475 - val_loss: 34.8106 - val_mae: 2.6004 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.8616 - mae: 3.3516 - val_loss: 34.2726 - val_mae: 2.6835 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.6014 - mae: 3.3325 - val_loss: 34.5975 - val_mae: 2.6330 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.6104 - mae: 3.3989 - val_loss: 37.6190 - val_mae: 2.9543 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.3345 - mae: 3.3276 - val_loss: 34.7273 - val_mae: 2.6007 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.2351 - mae: 3.3542 - val_loss: 34.6711 - val_mae: 2.5680 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.8799 - mae: 3.3662 - val_loss: 34.6518 - val_mae: 2.6024 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.7501 - mae: 3.3574 - val_loss: 34.4376 - val_mae: 2.7257 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.6495 - mae: 3.3711 - val_loss: 34.6969 - val_mae: 2.6686 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.4081 - mae: 3.3751 - val_loss: 34.9651 - val_mae: 2.8003 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.7461 - mae: 3.3719 - val_loss: 35.5447 - val_mae: 2.8139 - lr: 0.0010\n",
      "Epoch 91/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.5415 - mae: 3.3643 - val_loss: 34.9102 - val_mae: 2.6781 - lr: 0.0010\n",
      "Epoch 92/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.6914 - mae: 3.3949 - val_loss: 34.5552 - val_mae: 2.5684 - lr: 0.0010\n",
      "Epoch 93/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.1997 - mae: 3.3713 - val_loss: 34.6946 - val_mae: 2.5310 - lr: 0.0010\n",
      "Epoch 94/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.4747 - mae: 3.3689 - val_loss: 35.1729 - val_mae: 2.6021 - lr: 0.0010\n",
      "Epoch 95/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 40.3819 - mae: 3.3908 - val_loss: 34.8098 - val_mae: 2.6382 - lr: 0.0010\n",
      "Epoch 96/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.0741 - mae: 3.4148 - val_loss: 35.4103 - val_mae: 2.8933 - lr: 0.0010\n",
      "Epoch 97/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 39.8108 - mae: 3.3782 - val_loss: 34.4498 - val_mae: 2.7321 - lr: 0.0010\n",
      "Epoch 98/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.3391 - mae: 3.4046 - val_loss: 34.6997 - val_mae: 2.4921 - lr: 0.0010\n",
      "Epoch 99/100\n",
      "495/495 [==============================] - 1s 1ms/step - loss: 41.0742 - mae: 3.4004 - val_loss: 34.8337 - val_mae: 2.7134 - lr: 0.0010\n",
      "Epoch 100/100\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 39.6166 - mae: 3.3649 - val_loss: 37.6451 - val_mae: 3.0536 - lr: 0.0010\n",
      "155/155 [==============================] - 0s 844us/step - loss: 35.6872 - mae: 3.0172\n",
      "Loss en el conjunto de prueba: 35.68724060058594, MAE: 3.0172338485717773\n"
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura del modelo, con dos capas ocultas de 64 unidades cada una con la función de activación 'relu', que proporciona\n",
    "# capacidad de modelado no lineal.\n",
    "# una capa de salida sin función de activación\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2, batch_size=32, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "\n",
    "# Evaluamos el conjunto de prueba\n",
    "\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}, MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imputador MICE entrenado con éxito.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuramos el imputador MICE con GradientBoostingRegressor y el estimador\n",
    "\n",
    "mice_imputer = IterativeImputer(estimator=GradientBoostingRegressor(\n",
    "                                    n_estimators=100,\n",
    "                                    max_depth=10,\n",
    "                                    min_samples_split=4,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    max_features='sqrt'),\n",
    "                                    max_iter=10, random_state=42)\n",
    "\n",
    "# Preparamos los datos para el entrenamiento del imputador MICE eliminando la variable objetivo E_SIMEL\n",
    "\n",
    "X_mice = df_inicio.drop(columns=['E_SIMEL'])\n",
    "\n",
    "# Entrenamos el imputador MICE\n",
    "\n",
    "mice_imputer.fit(X_mice)\n",
    "\n",
    "# Imprimimos confirmación\n",
    "\n",
    "\"Imputador MICE entrenado con éxito.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las filas del dia 5 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_05_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "\n",
    "df_final_05_11_para_imputar = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_05_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_05_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto y\n",
    "# establecemos una condición que si los valores son más grandes que 0.2 establecemos un 1, y si son inferiores establecemos un 0.\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_05_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5725751 ],\n",
       "       [0.51202583],\n",
       "       [0.4514761 ],\n",
       "       [0.39262104],\n",
       "       [0.34881783],\n",
       "       [0.357687  ],\n",
       "       [0.35219765],\n",
       "       [0.34502172],\n",
       "       [0.3678546 ],\n",
       "       [0.39615202],\n",
       "       [4.8212347 ],\n",
       "       [0.4466629 ],\n",
       "       [6.07112   ],\n",
       "       [0.44264984],\n",
       "       [7.4114823 ],\n",
       "       [8.12932   ],\n",
       "       [8.813966  ],\n",
       "       [8.501184  ],\n",
       "       [7.2393866 ],\n",
       "       [0.47146082],\n",
       "       [0.5101805 ],\n",
       "       [0.5299165 ],\n",
       "       [0.5983372 ],\n",
       "       [0.6704922 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos los datos de df_final_05_11 para la predicción quitando la variable objetivo\n",
    "\n",
    "X_final_05_11 = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalización de los datos\n",
    "\n",
    "X_final_05_11_scaled = scaler.transform(X_final_05_11)\n",
    "\n",
    "# Realizamos la predicciones de E_SIMEL con el modelo de deep learning\n",
    "\n",
    "e_simel_predicciones = model.predict(X_final_05_11_scaled)\n",
    "\n",
    "e_simel_predicciones = np.maximum(e_simel_predicciones, 0)\n",
    "\n",
    "# Mostramos los resultados de la predicción\n",
    "\n",
    "e_simel_predicciones[:25]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_84548\\496075226.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24802</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24803</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24804</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24805</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24806</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24807</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24808</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24809</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.345022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24810</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.367855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24811</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24812</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.821235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24813</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24814</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.071120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24815</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24816</th>\n",
       "      <td>4.122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.411482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24817</th>\n",
       "      <td>5.437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.129320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24818</th>\n",
       "      <td>6.378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.813966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24819</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.501184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24820</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.239387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24821</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24822</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24823</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.529917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24824</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24825</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.670492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24802    0.000        0.0            0.572575\n",
       "24803    0.000        0.0            0.512026\n",
       "24804    0.000        0.0            0.451476\n",
       "24805    0.000        0.0            0.392621\n",
       "24806    0.000        0.0            0.348818\n",
       "24807    0.000        0.0            0.357687\n",
       "24808    0.000        0.0            0.352198\n",
       "24809    0.000        0.0            0.345022\n",
       "24810    0.000        0.0            0.367855\n",
       "24811    0.000        0.0            0.396152\n",
       "24812    0.000        0.0            4.821235\n",
       "24813    0.000        0.0            0.446663\n",
       "24814    0.000        0.0            6.071120\n",
       "24815    0.000        0.0            0.442650\n",
       "24816    4.122        0.0            7.411482\n",
       "24817    5.437        0.0            8.129320\n",
       "24818    6.378        0.0            8.813966\n",
       "24819    0.000        0.0            8.501184\n",
       "24820    0.000        0.0            7.239387\n",
       "24821    0.000        0.0            0.471461\n",
       "24822    0.000        0.0            0.510180\n",
       "24823    0.000        0.0            0.529917\n",
       "24824    0.000        0.0            0.598337\n",
       "24825    0.000        0.0            0.670492"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos el array a una lista para facilitar la asignación a una nueva columna que llamamos 'Prediccion_E_SIMEL'\n",
    "\n",
    "predicciones_lista = e_simel_predicciones.flatten().tolist()\n",
    "\n",
    "# Asignar las predicciones a df_final_05_11\n",
    "\n",
    "df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Mostramos las primeras filas para verificar\n",
    "\n",
    "df_final_05_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  15.937000000000001\n",
      "Suma predicha:  58.753822565078735\n",
      "Desviación porcentual:  268.66300160054425 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# al igual que en todos los casos anteriores, hacemos un sumatorio y porcentaje de desviación para tener una primera idea de como van las predicciones\n",
    "\n",
    "suma_real_05 = df_final_05_11['E_SIMEL'].sum()\n",
    "suma_predicha_05 = df_final_05_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_05 = df_final_05_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_05)\n",
    "print(\"Suma predicha: \", suma_predicha_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.4961 - mae: 3.1963 - val_loss: 45.3619 - val_mae: 3.7445 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.4160 - mae: 3.1936 - val_loss: 46.2433 - val_mae: 3.7271 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.1311 - mae: 3.1728 - val_loss: 43.6104 - val_mae: 3.4730 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.5498 - mae: 3.1972 - val_loss: 46.4601 - val_mae: 3.5799 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.3609 - mae: 3.1711 - val_loss: 48.0006 - val_mae: 3.6201 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.5965 - mae: 3.1613 - val_loss: 50.5713 - val_mae: 3.8575 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.4977 - mae: 3.2157 - val_loss: 45.9406 - val_mae: 3.4038 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.1736 - mae: 3.1980 - val_loss: 47.0625 - val_mae: 3.5421 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.3846 - mae: 3.1738 - val_loss: 62.7556 - val_mae: 4.3465 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.2046 - mae: 3.2043 - val_loss: 57.5542 - val_mae: 3.9860 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.6231 - mae: 3.1959 - val_loss: 52.8253 - val_mae: 3.6469 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.0576 - mae: 3.1954 - val_loss: 55.5680 - val_mae: 3.8416 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.7560 - mae: 3.1902 - val_loss: 59.7897 - val_mae: 3.9974 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.0876 - mae: 3.1834 - val_loss: 64.1360 - val_mae: 4.2671 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 35.7657 - mae: 3.1448 - val_loss: 73.7681 - val_mae: 4.6437 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.3457 - mae: 3.1804 - val_loss: 76.6929 - val_mae: 4.7100 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 35.8577 - mae: 3.1573 - val_loss: 76.3031 - val_mae: 4.6409 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.6595 - mae: 3.1839 - val_loss: 87.5785 - val_mae: 5.1053 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.3349 - mae: 3.1741 - val_loss: 76.1694 - val_mae: 4.6419 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.1178 - mae: 3.2163 - val_loss: 86.5376 - val_mae: 5.0699 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.3390 - mae: 3.1707 - val_loss: 75.0620 - val_mae: 4.6273 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.4385 - mae: 3.1780 - val_loss: 75.4330 - val_mae: 4.6422 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.3590 - mae: 3.1942 - val_loss: 94.5065 - val_mae: 5.3198 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.1940 - mae: 3.1893 - val_loss: 79.1666 - val_mae: 4.8884 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.2206 - mae: 3.2300 - val_loss: 74.3181 - val_mae: 4.6478 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.9513 - mae: 3.2099 - val_loss: 88.4685 - val_mae: 5.1450 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.6948 - mae: 3.1877 - val_loss: 88.0424 - val_mae: 5.0975 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "601/619 [============================>.] - ETA: 0s - loss: 36.0946 - mae: 3.1866Restoring model weights from the end of the best epoch: 3.\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.3129 - mae: 3.1917 - val_loss: 87.9979 - val_mae: 5.0802 - lr: 0.0010\n",
      "Epoch 28: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IterativeImputer</label><div class=\"sk-toggleable__content\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Una vez tenemos la primera predicción, actualizamos df_inicio con los datos del día 5. Así estamos simulando como sería\n",
    "# el proceso de predicción en tiempo real\n",
    "\n",
    "datos_dia_5 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "df_inicio_actualizado = pd.concat([df_inicio, datos_dia_5])\n",
    "\n",
    "# Preparamos df_actualizado con los datos de df_inicio y los datos del día 5 para reentrenar el modelo de deep learning\n",
    "\n",
    "X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "\n",
    "# Normalizamos con fit_transform\n",
    "\n",
    "X_total_scaled = scaler.fit_transform(X_actualizado)  # Utilizamos fit_transform \n",
    "\n",
    "# Y reentrenamos el modelo de deep learning con todos los datos con los parámetros utilizados para el entrenamiento inicial\n",
    "\n",
    "model.fit(X_total_scaled, y_actualizado, epochs=100, validation_split=0.2, batch_size=32, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "# reentrenamos también el modelo imputador con los nuevos datos con todas las variables menos con E_SIMEL\n",
    "\n",
    "mice_imputer.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_84548\\2659706717.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24826</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.629007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24827</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24829</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.480338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24830</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24831</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24832</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24833</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24834</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24835</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.993653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24836</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7.603230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24837</th>\n",
       "      <td>0.000</td>\n",
       "      <td>11.6</td>\n",
       "      <td>10.055745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>0.000</td>\n",
       "      <td>15.8</td>\n",
       "      <td>11.068720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24839</th>\n",
       "      <td>0.000</td>\n",
       "      <td>19.4</td>\n",
       "      <td>11.673902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24840</th>\n",
       "      <td>3.946</td>\n",
       "      <td>24.3</td>\n",
       "      <td>12.339817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24841</th>\n",
       "      <td>22.905</td>\n",
       "      <td>29.8</td>\n",
       "      <td>13.948233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24842</th>\n",
       "      <td>21.310</td>\n",
       "      <td>32.9</td>\n",
       "      <td>14.339310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24843</th>\n",
       "      <td>9.663</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.498215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24844</th>\n",
       "      <td>0.718</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.001295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24845</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24846</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.543425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24848</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24849</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24826    0.000        0.0            0.629007\n",
       "24827    0.000        0.0            0.620591\n",
       "24828    0.000        0.0            0.544444\n",
       "24829    0.000        0.0            0.480338\n",
       "24830    0.000        0.0            0.432937\n",
       "24831    0.000        0.0            0.398815\n",
       "24832    0.000        0.0            0.382535\n",
       "24833    0.000        0.0            0.366253\n",
       "24834    0.000        0.0            0.374172\n",
       "24835    0.000        0.0            5.993653\n",
       "24836    0.000        2.6            7.603230\n",
       "24837    0.000       11.6           10.055745\n",
       "24838    0.000       15.8           11.068720\n",
       "24839    0.000       19.4           11.673902\n",
       "24840    3.946       24.3           12.339817\n",
       "24841   22.905       29.8           13.948233\n",
       "24842   21.310       32.9           14.339310\n",
       "24843    9.663       19.0           11.498215\n",
       "24844    0.718        4.0            7.001295\n",
       "24845    0.000        0.0            0.531765\n",
       "24846    0.000        0.0            3.543425\n",
       "24847    0.000        0.0            0.533733\n",
       "24848    0.000        0.0            0.510793\n",
       "24849    0.000        0.0            0.487854"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seleccionamos las filas del dia 6 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_06_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 6)]\n",
    "\n",
    "df_final_06_11_para_imputar = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_06_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_06_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_06_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n",
    "\n",
    "# Verificamos que los valores han sido imputados correctamente\n",
    "# df_final_05_11.head(25)\n",
    "\n",
    "# Prepararamos los datos de df_final_06_11 para la predicción\n",
    "\n",
    "X_final_06_11 = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalizamos los datos de X_final_05_11 \n",
    "\n",
    "X_final_06_11_scaled = scaler.transform(X_final_06_11)\n",
    "\n",
    "# Realizamos las predicciones de E_SIMEL\n",
    "\n",
    "e_simel_predicciones_06 = model.predict(X_final_06_11_scaled)\n",
    "\n",
    "# Para que no haya valores negativos los transformamos a cero\n",
    "\n",
    "e_simel_predicciones_06 = np.maximum(e_simel_predicciones_06, 0)\n",
    "\n",
    "# Mostrar las primeras 5 predicciones\n",
    "# e_simel_predicciones[:25]\n",
    "\n",
    "# Convertimos el array de predicciones a una lista para facilitar la asignación\n",
    "\n",
    "predicciones_lista = e_simel_predicciones_06.flatten().tolist()\n",
    "\n",
    "# Asignamos las predicciones a una nueva columna en el Datadrame df_final_06_11\n",
    "\n",
    "df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Visualizamos las columnas para ver el resultado\n",
    "\n",
    "df_final_06_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.542\n",
      "Suma predicha:  115.35878014564514\n",
      "Desviación porcentual:  97.0530220109411 %\n",
      "Suma previsión:  159.4\n",
      "Desviación porcentual:  172.2831471422227 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes de desviación\n",
    "\n",
    "suma_real_06 = df_final_06_11['E_SIMEL'].sum()\n",
    "suma_predicha_06 = df_final_06_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_06 = df_final_06_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_06)\n",
    "print(\"Suma predicha: \", suma_predicha_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un función para agilizar el proceso de actualización, reentreno de los modelos, imputación, predicción y cálculo de las métricas\n",
    "\n",
    "def predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente,  mes, año, df_inicio_actualizado, df_final, modelo_deep, imputador):\n",
    "    \"\"\"\n",
    "    Función para actualizar el conjunto de entrenamiento con los datos reales de un día específico,\n",
    "    realizar la imputación para el día siguiente y predecir los valores de E_SIMEL para ese día.\n",
    "\n",
    "    Args:\n",
    "    dia_actual (int): Día actual para el que se actualizarán los datos.\n",
    "    dia_siguiente (int): Datos del día que queremos hacer las imputaciones y la predicción\n",
    "    mes (int): Mes del día actual.\n",
    "    año (int): Año del día actual.\n",
    "    df_inicio_actualizado (DataFrame): DataFrame actualizado con los datos hasta el día anterior.\n",
    "    df_final (DataFrame): DataFrame con los datos a predecir.\n",
    "    modelo_deep (DeepLearning de TencerFlow): Modelo de Deep Learning entrenado.\n",
    "    imputador (IterativeImputer): Imputador MICE entrenado.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame con las predicciones para el día siguiente.\n",
    "    DataFrame: DataFrame actualizado con los datos reales del día actual.\n",
    "    \"\"\"\n",
    "    # Actualización de df_actualizado con los datos de dia_actual\n",
    "\n",
    "    datos_dia_actual = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_actual)]\n",
    "    df_inicio_actualizado = pd.concat([df_inicio_actualizado, datos_dia_actual])\n",
    "\n",
    "    \n",
    "    # Prepararamos los datos para el modelo de deep learning\n",
    "\n",
    "    X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "    y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "        \n",
    "    # Normalizaación las características\n",
    "\n",
    "    X_actualizado_scaled = scaler.transform(X_actualizado)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min', restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\n",
    "\n",
    "    # Reentrenamos el modelo de deep learning con todos los datos pasándole los parámetros de modelado\n",
    "\n",
    "    modelo_deep.fit(X_actualizado_scaled, y_actualizado, epochs=100, validation_split=0.2, batch_size=32, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "    # Reentrenamos el imputador para todas las variables menos con E_SIMEL\n",
    "\n",
    "    imputador.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])    \n",
    "\n",
    "    \n",
    "\n",
    "      # Imputación de valores a la columna f_RUN para la predicción\n",
    "\n",
    "    df_dia_siguiente = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_siguiente)]\n",
    "    df_dia_siguiente_para_imputar = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "    df_dia_siguiente_para_imputar[['f_RUN']] = np.nan  \n",
    "    \n",
    "    valores_imputados = imputador.transform(df_dia_siguiente_para_imputar)\n",
    "    \n",
    "    df_dia_siguiente.loc[:, 'f_RUN'] = np.where(valores_imputados[:, 2] > 0.2, 1, 0) \n",
    "\n",
    "    \n",
    "    \n",
    "    # Preparamos los datos para la predicción con el modelo de deep learning\n",
    "\n",
    "    X_prediccion = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "    # Normalizamos los datos \n",
    "\n",
    "    X_prediccion_scaled = scaler.transform(X_prediccion)\n",
    "\n",
    "    # Realizamos las predicciones de E_SIMEL\n",
    "    predicted_e_simel = model.predict(X_prediccion_scaled)\n",
    "\n",
    "    # Transformamos los valores negativos de la predicción a cero\n",
    "\n",
    "    predicted_e_simel = np.maximum(predicted_e_simel, 0)\n",
    "\n",
    "    # Mostramos las predicciones\n",
    "\n",
    "    # e_simel_predicciones[:25]\n",
    "\n",
    "    # Convertimos el array de las predicciones a una lista para facilitar la asignación\n",
    "    predicciones_lista = predicted_e_simel.flatten().tolist()\n",
    "\n",
    "    df_predicciones = df_dia_siguiente[['Año', 'Mes', 'Día', 'PREVISION', 'E_SIMEL']].copy()\n",
    "\n",
    "    # En el df_prediciones creamos una nueva columna con las predicciones\n",
    "\n",
    "    df_predicciones['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "    # Mostramos el resultado\n",
    "\n",
    "    df_predicciones[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculamos las métricas: mse (Error cuadrático medio), r2 (coeficiente de determinación) y el mae(error medio absoluto)\n",
    "\n",
    "    mse = mean_squared_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    mae = mean_absolute_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "\n",
    "\n",
    "    return df_predicciones, df_inicio_actualizado, mse, r2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.6773 - mae: 3.1649 - val_loss: 49.7858 - val_mae: 3.8873 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.4420 - mae: 3.1894 - val_loss: 48.8350 - val_mae: 3.6553 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 38.1914 - mae: 3.2095 - val_loss: 48.4827 - val_mae: 3.5961 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.5251 - mae: 3.1787 - val_loss: 50.2820 - val_mae: 3.6372 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.0553 - mae: 3.1954 - val_loss: 49.0924 - val_mae: 3.7404 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.3275 - mae: 3.1557 - val_loss: 58.4819 - val_mae: 4.1533 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.5265 - mae: 3.1739 - val_loss: 55.0660 - val_mae: 3.9500 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.7467 - mae: 3.1772 - val_loss: 56.6113 - val_mae: 4.0644 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.0657 - mae: 3.2023 - val_loss: 60.1344 - val_mae: 4.2479 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 35.8599 - mae: 3.1643 - val_loss: 67.1657 - val_mae: 4.5543 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.7203 - mae: 3.1786 - val_loss: 53.5060 - val_mae: 3.8434 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.4688 - mae: 3.2128 - val_loss: 54.1527 - val_mae: 3.9025 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 37.1067 - mae: 3.2001 - val_loss: 59.7841 - val_mae: 4.1654 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.1012 - mae: 3.1553 - val_loss: 54.7175 - val_mae: 3.8537 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.6767 - mae: 3.1977 - val_loss: 59.4764 - val_mae: 4.0977 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.0340 - mae: 3.2071 - val_loss: 54.8071 - val_mae: 3.8911 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 35.6460 - mae: 3.1531 - val_loss: 53.4967 - val_mae: 3.6766 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 37.1749 - mae: 3.2009 - val_loss: 58.1626 - val_mae: 4.0609 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.2856 - mae: 3.1966 - val_loss: 59.5878 - val_mae: 4.2362 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.7373 - mae: 3.1933 - val_loss: 65.7937 - val_mae: 4.3603 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.2480 - mae: 3.1829 - val_loss: 62.8852 - val_mae: 4.2180 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 36.7722 - mae: 3.1919 - val_loss: 54.8420 - val_mae: 3.8387 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 35.9720 - mae: 3.1717 - val_loss: 59.4293 - val_mae: 4.0736 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 35.3073 - mae: 3.1659 - val_loss: 54.7872 - val_mae: 3.8378 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 35.8675 - mae: 3.1899 - val_loss: 68.4652 - val_mae: 4.5375 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 35.7514 - mae: 3.1844 - val_loss: 56.5061 - val_mae: 4.1112 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 35.9536 - mae: 3.1872 - val_loss: 71.2831 - val_mae: 4.5048 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "590/619 [===========================>..] - ETA: 0s - loss: 36.5423 - mae: 3.2129Restoring model weights from the end of the best epoch: 3.\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 36.6245 - mae: 3.2165 - val_loss: 53.4181 - val_mae: 3.8163 - lr: 0.0010\n",
      "Epoch 28: early stopping\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "MSE: 35.24633472266964 R²: 0.11577635461216884 MAE: 3.5180397990544634\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 6    # Actualización de datos con los que reentrenamos los modelos\n",
    "dia_siguiente = 7    # Preparación de datos para la imputación y predicción\n",
    "df_predicciones_07_11, df_inicio_actualizado, mse_07_11, r2_07_11, mae_07_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_07_11, \"R²:\", r2_07_11, \"MAE:\", mae_07_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  67.97999999999999\n",
      "Suma predicha:  55.02162551879883\n",
      "Desviación porcentual:  -19.06203954280842 %\n",
      "Suma previsión:  24.0\n",
      "Desviación porcentual:  -64.6954986760812 %\n"
     ]
    }
   ],
   "source": [
    "# Y como en la predicción anterior calculamos los sumatorios y los porcentajes de desviación\n",
    "\n",
    "suma_real_07_11 = df_predicciones_07_11['E_SIMEL'].sum()\n",
    "suma_predicha_07_11 = df_predicciones_07_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_07_11 = df_predicciones_07_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_07_11)\n",
    "print(\"Suma predicha: \", suma_predicha_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.8364 - mae: 3.1849 - val_loss: 51.9102 - val_mae: 3.9058 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.8325 - mae: 3.1870 - val_loss: 52.9722 - val_mae: 3.9328 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 37.0266 - mae: 3.2008 - val_loss: 53.8467 - val_mae: 3.8825 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.8930 - mae: 3.1995 - val_loss: 59.1275 - val_mae: 4.1367 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 37.3634 - mae: 3.2071 - val_loss: 55.4141 - val_mae: 4.0494 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 37.5713 - mae: 3.2033 - val_loss: 58.7749 - val_mae: 4.1443 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 35.9905 - mae: 3.1522 - val_loss: 52.4432 - val_mae: 3.8474 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 37.5094 - mae: 3.2248 - val_loss: 70.1736 - val_mae: 4.5967 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.5759 - mae: 3.1866 - val_loss: 59.9724 - val_mae: 4.1074 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.1698 - mae: 3.1705 - val_loss: 54.9553 - val_mae: 4.0315 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.2824 - mae: 3.1960 - val_loss: 55.8044 - val_mae: 3.9034 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.2608 - mae: 3.1671 - val_loss: 59.8455 - val_mae: 4.1450 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.2718 - mae: 3.1887 - val_loss: 62.6974 - val_mae: 4.2438 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 37.1683 - mae: 3.2110 - val_loss: 63.5343 - val_mae: 4.3545 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.3464 - mae: 3.1892 - val_loss: 76.5373 - val_mae: 4.8815 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 37.2129 - mae: 3.2241 - val_loss: 61.0617 - val_mae: 4.2781 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.4120 - mae: 3.2038 - val_loss: 76.7346 - val_mae: 4.6824 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.7033 - mae: 3.2006 - val_loss: 64.0854 - val_mae: 4.3580 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 37.2167 - mae: 3.2245 - val_loss: 56.7659 - val_mae: 3.8582 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.2067 - mae: 3.1740 - val_loss: 76.3851 - val_mae: 4.8771 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.0245 - mae: 3.1886 - val_loss: 74.2698 - val_mae: 4.7240 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 35.4155 - mae: 3.1644 - val_loss: 64.0604 - val_mae: 4.2705 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.3127 - mae: 3.1911 - val_loss: 69.5875 - val_mae: 4.4725 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.0115 - mae: 3.1702 - val_loss: 72.4630 - val_mae: 4.5370 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 36.2363 - mae: 3.1883 - val_loss: 69.9520 - val_mae: 4.4934 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "615/620 [============================>.] - ETA: 0s - loss: 36.0141 - mae: 3.1875Restoring model weights from the end of the best epoch: 1.\n",
      "620/620 [==============================] - 1s 1ms/step - loss: 36.0910 - mae: 3.1893 - val_loss: 63.7926 - val_mae: 4.2903 - lr: 0.0010\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 13.948702312363055 R²: -3.592152963428247 MAE: 1.9996263157526653\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función. Actualización de datos reales del día 7 para reentrenar los modelos\n",
    "# Preparación de los datos del día posterior, en este caso del día 8, para la imputación y predicción\n",
    "\n",
    "dia_actual = 7\n",
    "dia_siguiente = 8\n",
    "\n",
    "df_predicciones_08_11, df_inicio_actualizado, mse_08_11, r2_08_11, mae_08_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_08_11, \"R²:\", r2_08_11, \"MAE:\", mae_08_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  11.296999999999999\n",
      "Suma predicha:  59.288031578063965\n",
      "Desviación porcentual:  424.81217648989974 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_08_11 = df_predicciones_08_11['E_SIMEL'].sum()\n",
    "suma_predicha_08_11 = df_predicciones_08_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_08_11 = df_predicciones_08_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_08_11)\n",
    "print(\"Suma predicha: \", suma_predicha_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.7192 - mae: 3.1823 - val_loss: 48.1978 - val_mae: 3.6014 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 37.3296 - mae: 3.2106 - val_loss: 49.1349 - val_mae: 3.6723 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 37.2409 - mae: 3.2288 - val_loss: 54.6130 - val_mae: 3.9891 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.9155 - mae: 3.1918 - val_loss: 50.4646 - val_mae: 3.7081 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.8751 - mae: 3.1813 - val_loss: 55.1976 - val_mae: 3.8792 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 37.1705 - mae: 3.2121 - val_loss: 52.6358 - val_mae: 3.7094 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5730 - mae: 3.1765 - val_loss: 50.4467 - val_mae: 3.6861 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.6043 - mae: 3.1863 - val_loss: 56.1389 - val_mae: 4.0350 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.4482 - mae: 3.1677 - val_loss: 57.5606 - val_mae: 3.9962 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.5222 - mae: 3.1898 - val_loss: 62.3549 - val_mae: 4.0852 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.6427 - mae: 3.1823 - val_loss: 58.9779 - val_mae: 4.0262 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5479 - mae: 3.1732 - val_loss: 69.5883 - val_mae: 4.5172 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 35.8792 - mae: 3.1640 - val_loss: 57.7787 - val_mae: 4.0527 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.3014 - mae: 3.1752 - val_loss: 52.5644 - val_mae: 3.5908 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 35.7649 - mae: 3.1538 - val_loss: 54.5297 - val_mae: 3.7748 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.3456 - mae: 3.1900 - val_loss: 54.4534 - val_mae: 3.7268 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 35.7201 - mae: 3.1635 - val_loss: 56.2469 - val_mae: 3.9143 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.9319 - mae: 3.2133 - val_loss: 56.6752 - val_mae: 3.8063 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5763 - mae: 3.1939 - val_loss: 53.4970 - val_mae: 3.7977 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.8622 - mae: 3.1983 - val_loss: 62.2985 - val_mae: 4.2946 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.2379 - mae: 3.1693 - val_loss: 64.4175 - val_mae: 4.3003 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5829 - mae: 3.2049 - val_loss: 60.1827 - val_mae: 4.1153 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.8373 - mae: 3.2057 - val_loss: 69.8813 - val_mae: 4.4220 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.3867 - mae: 3.1947 - val_loss: 67.6350 - val_mae: 4.4935 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.1198 - mae: 3.1946 - val_loss: 64.5526 - val_mae: 4.2654 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "588/621 [===========================>..] - ETA: 0s - loss: 36.3231 - mae: 3.1896Restoring model weights from the end of the best epoch: 1.\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.5620 - mae: 3.2000 - val_loss: 66.8391 - val_mae: 4.3630 - lr: 0.0010\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 31.390237118559828 R²: 0.6643500196231762 MAE: 2.9096666529973354\n"
     ]
    }
   ],
   "source": [
    "# Seguimos con el mismo proceso anterior. Llamamos a la función con los días específicos que queremos actualizar, imputar y predecir.\n",
    "\n",
    "dia_actual = 8\n",
    "dia_siguiente = 9\n",
    "\n",
    "df_predicciones_09_11, df_inicio_actualizado, mse_09_11, r2_09_11, mae_09_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_09_11, \"R²:\", r2_09_11, \"MAE:\", mae_09_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  102.43700000000001\n",
      "Suma predicha:  89.9802942276001\n",
      "Desviación porcentual:  -12.160357851557457 %\n",
      "Suma previsión:  127.30000000000001\n",
      "Desviación porcentual:  24.271503460663627 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y procentajes\n",
    "\n",
    "suma_real_09_11 = df_predicciones_09_11['E_SIMEL'].sum()\n",
    "suma_predicha_09_11 = df_predicciones_09_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_09_11 = df_predicciones_09_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_09_11)\n",
    "print(\"Suma predicha: \", suma_predicha_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.7002 - mae: 3.1757 - val_loss: 57.0638 - val_mae: 4.0306 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.2477 - mae: 3.1828 - val_loss: 53.2661 - val_mae: 3.9153 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 37.0027 - mae: 3.1848 - val_loss: 50.0708 - val_mae: 3.7598 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.8989 - mae: 3.1699 - val_loss: 57.1742 - val_mae: 4.0685 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.9328 - mae: 3.1889 - val_loss: 56.0694 - val_mae: 3.9582 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.7306 - mae: 3.1927 - val_loss: 60.1799 - val_mae: 4.2505 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5984 - mae: 3.2038 - val_loss: 56.2975 - val_mae: 3.9699 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5937 - mae: 3.1949 - val_loss: 59.9448 - val_mae: 4.2240 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.9013 - mae: 3.1949 - val_loss: 56.9977 - val_mae: 4.0537 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.2926 - mae: 3.1746 - val_loss: 57.6584 - val_mae: 4.0621 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5069 - mae: 3.1727 - val_loss: 58.3607 - val_mae: 4.0905 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.2667 - mae: 3.1831 - val_loss: 60.2345 - val_mae: 4.1778 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.4802 - mae: 3.1900 - val_loss: 59.1863 - val_mae: 4.1622 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 35.5627 - mae: 3.1430 - val_loss: 67.5996 - val_mae: 4.4512 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 35.8993 - mae: 3.1612 - val_loss: 67.0093 - val_mae: 4.4036 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.5189 - mae: 3.1954 - val_loss: 66.6637 - val_mae: 4.4723 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.3972 - mae: 3.1853 - val_loss: 66.6471 - val_mae: 4.3848 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.1254 - mae: 3.1835 - val_loss: 55.3911 - val_mae: 3.9690 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.9778 - mae: 3.2107 - val_loss: 61.7228 - val_mae: 4.1207 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.3442 - mae: 3.1791 - val_loss: 71.7604 - val_mae: 4.5905 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.1853 - mae: 3.1806 - val_loss: 57.8490 - val_mae: 3.9881 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.9809 - mae: 3.2136 - val_loss: 68.9052 - val_mae: 4.5016 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.1834 - mae: 3.1727 - val_loss: 68.7155 - val_mae: 4.6161 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 36.0485 - mae: 3.1686 - val_loss: 60.2617 - val_mae: 4.1827 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 35.8965 - mae: 3.1835 - val_loss: 55.3602 - val_mae: 3.9410 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 36.1467 - mae: 3.1792 - val_loss: 85.9036 - val_mae: 5.1397 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "621/621 [==============================] - 1s 1ms/step - loss: 35.3755 - mae: 3.1676 - val_loss: 60.2655 - val_mae: 4.2107 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "609/621 [============================>.] - ETA: 0s - loss: 35.5126 - mae: 3.1455Restoring model weights from the end of the best epoch: 3.\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 35.5614 - mae: 3.1476 - val_loss: 74.2910 - val_mae: 4.8489 - lr: 0.0010\n",
      "Epoch 28: early stopping\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "MSE: 16.781109197868815 R²: 0.30765116165231154 MAE: 2.2938155656655628\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 9\n",
    "dia_siguiente = 10\n",
    "\n",
    "df_predicciones_10_11, df_inicio_actualizado, mse_10_11, r2_10_11, mae_10_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_10_11, \"R²:\", r2_10_11, \"MAE:\", mae_10_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.955\n",
      "Suma predicha:  105.15507960319519\n",
      "Desviación porcentual:  78.36498957373453 %\n",
      "Suma previsión:  112.6\n",
      "Desviación porcentual:  90.99313035365958 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_10_11 = df_predicciones_10_11['E_SIMEL'].sum()\n",
    "suma_predicha_10_11 = df_predicciones_10_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_10_11 = df_predicciones_10_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_10_11)\n",
    "print(\"Suma predicha: \", suma_predicha_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 37.4087 - mae: 3.2170 - val_loss: 48.4112 - val_mae: 3.6188 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.8386 - mae: 3.1734 - val_loss: 50.2885 - val_mae: 3.6524 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.4058 - mae: 3.1977 - val_loss: 57.9244 - val_mae: 4.2517 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.9869 - mae: 3.1797 - val_loss: 48.8612 - val_mae: 3.6392 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 37.5300 - mae: 3.2250 - val_loss: 49.1117 - val_mae: 3.4881 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5813 - mae: 3.1898 - val_loss: 50.7514 - val_mae: 3.6639 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 36.9242 - mae: 3.1945 - val_loss: 53.3651 - val_mae: 3.7270 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 36.2379 - mae: 3.1830 - val_loss: 51.0346 - val_mae: 3.6732 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 35.8781 - mae: 3.1602 - val_loss: 54.8667 - val_mae: 3.9020 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.4456 - mae: 3.1883 - val_loss: 54.7914 - val_mae: 3.9383 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 36.7379 - mae: 3.1864 - val_loss: 51.6103 - val_mae: 3.6231 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 36.2108 - mae: 3.1766 - val_loss: 63.9137 - val_mae: 4.4256 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 36.3140 - mae: 3.1914 - val_loss: 52.9340 - val_mae: 3.6475 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5781 - mae: 3.1865 - val_loss: 58.1458 - val_mae: 4.0923 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 37.0306 - mae: 3.2314 - val_loss: 53.2778 - val_mae: 3.7547 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 35.9841 - mae: 3.1816 - val_loss: 56.9291 - val_mae: 3.7956 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "622/622 [==============================] - 1s 1ms/step - loss: 36.5107 - mae: 3.1921 - val_loss: 68.5473 - val_mae: 4.4651 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5049 - mae: 3.2030 - val_loss: 61.1790 - val_mae: 4.1462 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1546 - mae: 3.1979 - val_loss: 70.8190 - val_mae: 4.5727 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 35.7196 - mae: 3.1754 - val_loss: 61.4074 - val_mae: 4.2118 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 35.6844 - mae: 3.1603 - val_loss: 61.9886 - val_mae: 4.2138 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1535 - mae: 3.1740 - val_loss: 57.0938 - val_mae: 3.9344 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "622/622 [==============================] - 2s 3ms/step - loss: 35.6430 - mae: 3.1613 - val_loss: 60.4623 - val_mae: 4.1288 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.2883 - mae: 3.1886 - val_loss: 72.0031 - val_mae: 4.4885 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.2327 - mae: 3.2029 - val_loss: 65.8159 - val_mae: 4.3000 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "589/622 [===========================>..] - ETA: 0s - loss: 35.9311 - mae: 3.1682Restoring model weights from the end of the best epoch: 1.\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1358 - mae: 3.1738 - val_loss: 72.5487 - val_mae: 4.6518 - lr: 0.0010\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "MSE: 55.91828833338679 R²: 0.35907512518204143 MAE: 3.672456077893575\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 10\n",
    "dia_siguiente = 13\n",
    "\n",
    "df_predicciones_13_11, df_inicio_actualizado, mse_13_11, r2_13_11, mae_13_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_13_11, \"R²:\", r2_13_11, \"MAE:\", mae_13_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  107.141\n",
      "Suma predicha:  48.816144943237305\n",
      "Desviación porcentual:  -54.437474969211316 %\n",
      "Suma previsión:  20.900000000000002\n",
      "Desviación porcentual:  -80.492995211917 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_13_11 = df_predicciones_13_11['E_SIMEL'].sum()\n",
    "suma_predicha_13_11 = df_predicciones_13_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_13_11 = df_predicciones_13_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_13_11)\n",
    "print(\"Suma predicha: \", suma_predicha_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.6765 - mae: 3.2048 - val_loss: 53.8203 - val_mae: 3.7454 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 37.0925 - mae: 3.2019 - val_loss: 60.3228 - val_mae: 4.0534 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 37.0011 - mae: 3.1959 - val_loss: 56.6936 - val_mae: 3.8823 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.6380 - mae: 3.1792 - val_loss: 54.1987 - val_mae: 3.9451 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.7971 - mae: 3.2119 - val_loss: 48.0186 - val_mae: 3.5048 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 37.5440 - mae: 3.2309 - val_loss: 53.0101 - val_mae: 3.8720 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.8881 - mae: 3.2103 - val_loss: 56.9745 - val_mae: 4.1201 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.3743 - mae: 3.2040 - val_loss: 52.7334 - val_mae: 3.7001 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.6929 - mae: 3.2140 - val_loss: 58.8851 - val_mae: 4.0680 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.8757 - mae: 3.2037 - val_loss: 57.2105 - val_mae: 3.9683 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1771 - mae: 3.1749 - val_loss: 60.6558 - val_mae: 4.1052 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5202 - mae: 3.2124 - val_loss: 58.7368 - val_mae: 4.1684 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.7112 - mae: 3.2046 - val_loss: 56.6368 - val_mae: 3.9906 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 37.4691 - mae: 3.2349 - val_loss: 60.8566 - val_mae: 4.1573 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5434 - mae: 3.1989 - val_loss: 61.1230 - val_mae: 4.2212 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.9353 - mae: 3.2269 - val_loss: 57.5625 - val_mae: 4.1047 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.6997 - mae: 3.1867 - val_loss: 52.8292 - val_mae: 3.7930 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.4782 - mae: 3.2052 - val_loss: 60.6740 - val_mae: 4.3344 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.6201 - mae: 3.2231 - val_loss: 61.9076 - val_mae: 4.2031 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1690 - mae: 3.1938 - val_loss: 68.4332 - val_mae: 4.4787 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.2171 - mae: 3.1853 - val_loss: 63.0781 - val_mae: 4.3521 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.3136 - mae: 3.1984 - val_loss: 56.6971 - val_mae: 4.0476 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5098 - mae: 3.2285 - val_loss: 63.6564 - val_mae: 4.2773 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5350 - mae: 3.2299 - val_loss: 69.3714 - val_mae: 4.4944 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.5084 - mae: 3.2209 - val_loss: 54.0968 - val_mae: 3.8659 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1973 - mae: 3.2148 - val_loss: 61.3794 - val_mae: 4.1610 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.1345 - mae: 3.1986 - val_loss: 55.2982 - val_mae: 3.9799 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.4092 - mae: 3.2191 - val_loss: 59.8797 - val_mae: 4.1134 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.3036 - mae: 3.1997 - val_loss: 65.3686 - val_mae: 4.3650 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "622/622 [==============================] - ETA: 0s - loss: 36.3444 - mae: 3.2214Restoring model weights from the end of the best epoch: 5.\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 36.3444 - mae: 3.2214 - val_loss: 54.2636 - val_mae: 3.7757 - lr: 0.0010\n",
      "Epoch 30: early stopping\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 37.5489596839477 R²: 0.6011696185043902 MAE: 3.1523399705886845\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 13\n",
    "dia_siguiente = 14\n",
    "\n",
    "df_predicciones_14_11, df_inicio_actualizado, mse_14_11, r2_14_11, mae_14_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_14_11, \"R²:\", r2_14_11, \"MAE:\", mae_14_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  112.33099999999999\n",
      "Suma predicha:  75.55607509613037\n",
      "Desviación porcentual:  -32.7380018907244 %\n",
      "Suma previsión:  120.5\n",
      "Desviación porcentual:  7.272257880727503 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_14_11 = df_predicciones_14_11['E_SIMEL'].sum()\n",
    "suma_predicha_14_11 = df_predicciones_14_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_14_11 = df_predicciones_14_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_14_11)\n",
    "print(\"Suma predicha: \", suma_predicha_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.6156 - mae: 3.2208 - val_loss: 59.5967 - val_mae: 4.1438 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 36.9407 - mae: 3.2153 - val_loss: 52.1364 - val_mae: 3.7550 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.9115 - mae: 3.1795 - val_loss: 58.8104 - val_mae: 3.9716 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.9207 - mae: 3.1598 - val_loss: 60.9334 - val_mae: 4.0777 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.6240 - mae: 3.2000 - val_loss: 63.7278 - val_mae: 4.2053 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.8174 - mae: 3.2046 - val_loss: 60.3666 - val_mae: 4.0533 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.8382 - mae: 3.1929 - val_loss: 58.1352 - val_mae: 4.0874 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.1303 - mae: 3.1644 - val_loss: 70.5744 - val_mae: 4.5192 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.6013 - mae: 3.1666 - val_loss: 61.0019 - val_mae: 4.1084 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.7159 - mae: 3.2016 - val_loss: 72.2775 - val_mae: 4.6598 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.4706 - mae: 3.1938 - val_loss: 71.8916 - val_mae: 4.6603 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.7389 - mae: 3.2020 - val_loss: 65.6650 - val_mae: 4.2709 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.2840 - mae: 3.2239 - val_loss: 67.2946 - val_mae: 4.4124 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.0691 - mae: 3.1725 - val_loss: 60.1773 - val_mae: 4.1480 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.9434 - mae: 3.1826 - val_loss: 54.9653 - val_mae: 3.8792 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.4682 - mae: 3.2154 - val_loss: 56.9846 - val_mae: 3.9724 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.5215 - mae: 3.1718 - val_loss: 61.3183 - val_mae: 4.0684 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.1424 - mae: 3.2042 - val_loss: 59.7264 - val_mae: 4.1061 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.1992 - mae: 3.2092 - val_loss: 64.6920 - val_mae: 4.4533 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.0576 - mae: 3.1843 - val_loss: 58.9155 - val_mae: 4.0529 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.4440 - mae: 3.2010 - val_loss: 60.4052 - val_mae: 4.2672 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.0882 - mae: 3.2015 - val_loss: 58.1806 - val_mae: 4.0013 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.9227 - mae: 3.1763 - val_loss: 59.1948 - val_mae: 4.0791 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.0572 - mae: 3.1822 - val_loss: 58.8990 - val_mae: 4.1542 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.2077 - mae: 3.2109 - val_loss: 58.9196 - val_mae: 4.0822 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 36.4658 - mae: 3.2200 - val_loss: 69.1709 - val_mae: 4.4392 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "607/623 [============================>.] - ETA: 0s - loss: 35.5693 - mae: 3.1855Restoring model weights from the end of the best epoch: 2.\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 35.5683 - mae: 3.1811 - val_loss: 58.0269 - val_mae: 3.9586 - lr: 0.0010\n",
      "Epoch 27: early stopping\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 37.04781480033082 R²: 0.27571470534523035 MAE: 3.6550547846158348\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 14\n",
    "dia_siguiente = 15\n",
    "\n",
    "df_predicciones_15_11, df_inicio_actualizado, mse_15_11, r2_15_11, mae_15_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_15_11, \"R²:\", r2_15_11, \"MAE:\", mae_15_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  88.333\n",
      "Suma predicha:  93.20066690444946\n",
      "Desviación porcentual:  5.51058710159223 %\n",
      "Suma previsión:  131.89999999999998\n",
      "Desviación porcentual:  49.32131819365354 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_15_11 = df_predicciones_15_11['E_SIMEL'].sum()\n",
    "suma_predicha_15_11 = df_predicciones_15_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_15_11 = df_predicciones_15_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_15_11)\n",
    "print(\"Suma predicha: \", suma_predicha_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todos los Dataframes que contienen las prediccions, previsiones y datos reales para calcular las méstricas en conjunto\n",
    "\n",
    "df_predicciones_totales = pd.concat([df_final_05_11, df_final_06_11, df_predicciones_07_11, \n",
    "                                     df_predicciones_08_11, df_predicciones_09_11,df_predicciones_10_11, \n",
    "                                     df_predicciones_13_11, df_predicciones_14_11, df_predicciones_15_11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Predicciones:  2.964409713065183\n",
      "MSE Predicciones:  29.597694122493554\n",
      "R² Predicciones:  0.41480655146047174\n",
      "MAE Previsiones:  2.3999490740740743\n",
      "MSE Previsiones:  35.93822944907407\n",
      "R² Previsiones:  0.2894440918718205\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de las métricas para ver si nos indican si mejoran los errores entren la predicción (Prediccion_E_SIMEL), previsión (PREVISION)y producción real(E_SIMEL)\n",
    "\n",
    "def calcular_metricas(df):\n",
    "    mae = mean_absolute_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    mse = mean_squared_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    return mae, mse, r2\n",
    "\n",
    "# Métricas para la predicciones\n",
    "\n",
    "mae_pred, mse_pred, r2_pred = calcular_metricas(df_predicciones_totales)\n",
    "\n",
    "# Cambiamos la columna de predicción por la de previsión\n",
    "\n",
    "df_previsiones = df_predicciones_totales.copy()\n",
    "df_previsiones['Prediccion_E_SIMEL'] = df_previsiones['PREVISION']\n",
    "\n",
    "# Métricas para la previsión\n",
    "\n",
    "mae_prev, mse_prev, r2_prev = calcular_metricas(df_previsiones)\n",
    "\n",
    "# Visualizamos los resultados\n",
    "\n",
    "print(\"MAE Predicciones: \", mae_pred)\n",
    "print(\"MSE Predicciones: \", mse_pred)\n",
    "print(\"R² Predicciones: \", r2_pred)\n",
    "print(\"MAE Previsiones: \", mae_prev)\n",
    "print(\"MSE Previsiones: \", mse_prev)\n",
    "print(\"R² Previsiones: \", r2_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de los valores en la columna E_SIMEL: 622.953\n",
      "Suma de las predicciones: 701.1305205821991\n",
      "Suma de las previsiones : 696.6\n",
      "Diferencia entre predicciones totales y E_SIMEL total: 78.17752058219912\n",
      "Diferencia entre previsiones y E_SIMEL total: 73.64700000000005\n",
      "No mejoramos la predicción respecto la PREVISION real en: 4.530520582199074, por lo tanto, con este modelo, no estamos mejorando las previsiones.\n"
     ]
    }
   ],
   "source": [
    "# sumamos todos los valores de las columnas que queremo comparar\n",
    "\n",
    "suma_e_simel = df_predicciones_totales['E_SIMEL'].sum()\n",
    "sumas_totales_predicciones = df_predicciones_totales['Prediccion_E_SIMEL'].sum()\n",
    "sumas_previsiones = df_predicciones_totales['PREVISION'].sum()\n",
    "\n",
    "\n",
    "# Calculamos las diferencias entre la prediccion y la previsión respecto la producción real E_SIMEL\n",
    "\n",
    "diferencia_prediccion_vs_produccion_real = abs(sumas_totales_predicciones - suma_e_simel)\n",
    "diferencia_prevision_vs_produccion_real = abs(sumas_previsiones - suma_e_simel)\n",
    "\n",
    "\n",
    "# Imprimimos los resultados para poder visualizar si mejoramos las previsiones a lo largo de todas las predicciones.\n",
    "\n",
    "print(f\"Suma de los valores en la columna E_SIMEL: {suma_e_simel}\")\n",
    "print(f\"Suma de las predicciones: {sumas_totales_predicciones}\")\n",
    "print(f\"Suma de las previsiones : {sumas_previsiones}\")\n",
    "\n",
    "\n",
    "print(f\"Diferencia entre predicciones totales y E_SIMEL total: {diferencia_prediccion_vs_produccion_real}\")\n",
    "print(f\"Diferencia entre previsiones y E_SIMEL total: {diferencia_prevision_vs_produccion_real}\")\n",
    "\n",
    "\n",
    "# Calculamos la diferencia entre la predicción y la previsión para saber si el modelo de predicción mejora la previsión\n",
    "\n",
    "diferencia = diferencia_prediccion_vs_produccion_real - diferencia_prevision_vs_produccion_real\n",
    "\n",
    "if diferencia_prediccion_vs_produccion_real > diferencia_prevision_vs_produccion_real:\n",
    "    print(f\"No mejoramos la predicción respecto la PREVISION real en: {diferencia}, por lo tanto, con este modelo, no estamos mejorando las previsiones.\")\n",
    "else:\n",
    "    print(f\"La predicción es MEJOR que la previsión en: {-diferencia} unidades, por lo tanto, cumplimos nuestro objetivo de mejorar la PREVISIÓN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valores que cogemos para el gráfico\n",
    "\n",
    "categorias = ['E_SIMEL Total', 'Predicciones Total', 'Previsiones Total']\n",
    "valores = [suma_e_simel, sumas_totales_predicciones, sumas_previsiones]\n",
    "\n",
    "# Creamos un gráfico de barras\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barra = plt.bar(categorias, valores, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Añadimos las etiqutas\n",
    "\n",
    "for rect in barra:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Título del gráfico y ponemos las etiquteas a cada barra\n",
    "    \n",
    "plt.title('Comparación de la Producción Real, Predicciones y Previsiones Totales')\n",
    "plt.xlabel('Categorías')\n",
    "plt.ylabel('Valor Total')\n",
    "\n",
    "\n",
    "# Ubicamos el texto de la diferencia\n",
    "\n",
    "pos_y = valores[1] / 2\n",
    "pos_x = categorias[1]\n",
    "plt.text(pos_x, pos_y, f'No mejoramos la predicción respecto a la previsión en\\n{diferencia:.2f} unidades', ha='center', va='center', fontsize=12, color='black', bbox=dict(facecolor='green', alpha=0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696.5999999999999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_prevision = df_final['PREVISION'].sum()\n",
    "suma_prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622.953"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_E_SIMEL = df_final['E_SIMEL'].sum()\n",
    "suma_E_SIMEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
