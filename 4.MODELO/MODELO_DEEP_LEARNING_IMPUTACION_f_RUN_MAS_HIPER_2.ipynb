{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "\n",
    "import pandas as pd  # Manipulación y análisis de datos.\n",
    "import numpy as np  # Soporte para vectores y matrices.\n",
    "\n",
    "# Gráficos\n",
    "\n",
    "import matplotlib.pyplot as plt  # Creación de gráficos estáticos, animados e interactivos.\n",
    "from matplotlib import style  # Personalización del estilo de los gráficos.\n",
    "\n",
    "# Preprocesado y modelado\n",
    "\n",
    "from scipy.stats import pearsonr  # Coeficiente de correlación de Pearson.\n",
    "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba.\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # Métricas para evaluar modelos.\n",
    "import statsmodels.api as sm  # Modelos estadísticos y econometricos.\n",
    "import statsmodels.formula.api as smf  # Modelo estadísticos con fórmulas.\n",
    "from statsmodels.stats.anova import anova_lm  # Análisis de varianza.\n",
    "from scipy import stats  # Funciones estadísticas.\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler  # Preprocesamiento de datos.\n",
    "import category_encoders  # Codificación de variables categóricas.\n",
    "import missingno as msno  # Visualización de datos faltantes.\n",
    "from sklearn.pipeline import Pipeline  # Cadena de transformaciones con un estimador final.\n",
    "from sklearn.experimental import enable_iterative_imputer  # Permitir uso de IterativeImputer.\n",
    "from sklearn.impute import IterativeImputer  # Imputación de datos faltantes.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Modelos de ensamble.\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score  # Búsqueda de hiperparámetros y validación cruzada.\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf  # Biblioteca de Deep Learning.\n",
    "from tensorflow.keras import layers, models  # Construcción de modelos de deep learning.\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Callbacks para controlar el entrenamiento.\n",
    "from keras.models import Sequential  # Creación de modelos secuenciales.\n",
    "from keras.layers import Dense, Dropout, BatchNormalization  # Capas para construir modelos.\n",
    "from keras import regularizers  # Regularización de modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Period</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>DESVIO</th>\n",
       "      <th>f_PREV_HIGH</th>\n",
       "      <th>f_PREV_LOW</th>\n",
       "      <th>f_RUN</th>\n",
       "      <th>Dia_Semana</th>\n",
       "      <th>Es_fin_semana</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Día</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Fecha  Period  PREVISION  E_SIMEL  DESVIO  f_PREV_HIGH  \\\n",
       "0           0  2021-01-01       1        0.0      0.0     0.0            0   \n",
       "1           1  2021-01-01       2        0.0      0.0     0.0            0   \n",
       "2           2  2021-01-01       3        0.0      0.0     0.0            0   \n",
       "3           3  2021-01-01       4        0.0      0.0     0.0            0   \n",
       "4           4  2021-01-01       5        0.0      0.0     0.0            0   \n",
       "\n",
       "   f_PREV_LOW  f_RUN  Dia_Semana  Es_fin_semana   Año  Mes  Día  \n",
       "0           0      0           4          False  2021    1    1  \n",
       "1           0      0           4          False  2021    1    1  \n",
       "2           0      0           4          False  2021    1    1  \n",
       "3           0      0           4          False  2021    1    1  \n",
       "4           0      0           4          False  2021    1    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el archivo csv con los datos\n",
    "\n",
    "df_central = pd.read_csv(\"https://raw.githubusercontent.com/jesusvillaalvarez/TFM_KSCHOL/main/5.ARCHIVOS/df_central_2_1.csv\")\n",
    "\n",
    "df_central.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas innecesarias\n",
    "\n",
    "df_central = df_central.drop(columns=['Unnamed: 0', 'DESVIO', 'f_PREV_HIGH', 'f_PREV_LOW'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna 'Fecha' a Datetime para hacer las división en dos dfs\n",
    "\n",
    "df_central['Fecha'] = pd.to_datetime(df_central['Fecha'])\n",
    "\n",
    "# Dividimos el DataFrame en dos según las fechas especificas\n",
    "\n",
    "df_inicio = df_central[df_central['Fecha'] <= '2023-10-31']\n",
    "df_final = df_central[df_central['Fecha'] >= '2023-11-05']\n",
    "\n",
    "# Eliminamos la columna 'Fecha' de ambos DataFrames para poder preparar el modelo de Deep Learning\n",
    "\n",
    "df_inicio = df_inicio.drop(columns=['Fecha'])\n",
    "df_final = df_final.drop(columns=['Fecha'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19764, 8), (4942, 8), (19764,), (4942,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos df_inicio eliminando la variables objetivo del conjunto de entrenamiento (X) y especificamos la variable objetivo del conjunto de prueba (y)\n",
    "\n",
    "X = df_inicio.drop('E_SIMEL', axis=1)\n",
    "y = df_inicio['E_SIMEL']\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizamos las características, paso necesario para el modelo de deep learning\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verificamos las dimensiones de los conjuntos de datos para asegurarnos de que todo está correcto\n",
    "\n",
    "(X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVistos los resultados negativos  de los dos modelo anteriores de deep learning para nuestro objetivo, vamos a experimentar\\ncon otra arquitectura modificando hiperparámetros añadiendo más unidades en las capas ocultas, regularizaciones, normalizaciones,\\nañadiendo Dropout, además de afinar los Callbacks.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vistos los resultados negativos  de los dos modelo anteriores de deep learning para nuestro objetivo, vamos a experimentar\n",
    "con otra arquitectura modificando hiperparámetros añadiendo más unidades en las capas ocultas, regularizaciones, normalizaciones,\n",
    "añadiendo Dropout, además de afinar los Callbacks.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "495/495 [==============================] - 2s 2ms/step - loss: 63.8524 - mae: 4.0574 - val_loss: 41.5337 - val_mae: 3.1793 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 47.2180 - mae: 3.5277 - val_loss: 40.5516 - val_mae: 2.7283 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 44.0411 - mae: 3.3532 - val_loss: 37.9938 - val_mae: 2.8400 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 43.0908 - mae: 3.3165 - val_loss: 38.0331 - val_mae: 2.9519 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.6407 - mae: 3.2496 - val_loss: 37.0592 - val_mae: 2.7877 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 42.4551 - mae: 3.3207 - val_loss: 37.5820 - val_mae: 2.7879 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.7705 - mae: 3.2368 - val_loss: 37.0573 - val_mae: 2.8428 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 41.4015 - mae: 3.2036 - val_loss: 36.8877 - val_mae: 2.7389 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 40.0308 - mae: 3.1442 - val_loss: 36.5345 - val_mae: 2.6997 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 39.5500 - mae: 3.1099 - val_loss: 36.4585 - val_mae: 2.7344 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 39.0419 - mae: 3.1109 - val_loss: 36.7313 - val_mae: 2.8826 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 38.8098 - mae: 3.0927 - val_loss: 35.6278 - val_mae: 2.7942 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 39.0934 - mae: 3.0926 - val_loss: 35.7368 - val_mae: 2.7215 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 37.4276 - mae: 2.9930 - val_loss: 36.9647 - val_mae: 2.8990 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 38.1447 - mae: 3.0274 - val_loss: 36.6150 - val_mae: 2.7087 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 37.2407 - mae: 2.9904 - val_loss: 35.4902 - val_mae: 2.6946 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 37.5549 - mae: 2.9831 - val_loss: 36.1955 - val_mae: 2.8496 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 37.5346 - mae: 2.9845 - val_loss: 35.9304 - val_mae: 2.7569 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 38.0912 - mae: 2.9799 - val_loss: 35.6702 - val_mae: 2.7805 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.6824 - mae: 2.9300 - val_loss: 35.1396 - val_mae: 2.7681 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 37.4249 - mae: 2.9663 - val_loss: 35.6464 - val_mae: 2.6845 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.3794 - mae: 2.9181 - val_loss: 34.6266 - val_mae: 2.6246 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.7480 - mae: 2.9094 - val_loss: 37.0196 - val_mae: 2.5753 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.8008 - mae: 2.9095 - val_loss: 35.2148 - val_mae: 2.5809 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.1638 - mae: 2.8854 - val_loss: 36.5382 - val_mae: 2.6818 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.3686 - mae: 2.9042 - val_loss: 36.8072 - val_mae: 2.6790 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 36.1714 - mae: 2.8836 - val_loss: 34.8173 - val_mae: 2.7276 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 35.2120 - mae: 2.8334 - val_loss: 34.2117 - val_mae: 2.6025 - lr: 8.0000e-04\n",
      "Epoch 29/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 34.5663 - mae: 2.7877 - val_loss: 37.6840 - val_mae: 2.6264 - lr: 8.0000e-04\n",
      "Epoch 30/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 35.3289 - mae: 2.8017 - val_loss: 34.8325 - val_mae: 2.5527 - lr: 8.0000e-04\n",
      "Epoch 31/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 34.6833 - mae: 2.7784 - val_loss: 34.8054 - val_mae: 2.6714 - lr: 8.0000e-04\n",
      "Epoch 32/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 35.1292 - mae: 2.8059 - val_loss: 34.5121 - val_mae: 2.6438 - lr: 8.0000e-04\n",
      "Epoch 33/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 35.1022 - mae: 2.7756 - val_loss: 34.3067 - val_mae: 2.6090 - lr: 8.0000e-04\n",
      "Epoch 34/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 34.5339 - mae: 2.7564 - val_loss: 33.8411 - val_mae: 2.6433 - lr: 6.4000e-04\n",
      "Epoch 35/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 34.2280 - mae: 2.7516 - val_loss: 34.8799 - val_mae: 2.5987 - lr: 6.4000e-04\n",
      "Epoch 36/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.7201 - mae: 2.7259 - val_loss: 34.4046 - val_mae: 2.5147 - lr: 6.4000e-04\n",
      "Epoch 37/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 34.0665 - mae: 2.7460 - val_loss: 33.9475 - val_mae: 2.5964 - lr: 6.4000e-04\n",
      "Epoch 38/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.8133 - mae: 2.7503 - val_loss: 34.9049 - val_mae: 2.7908 - lr: 6.4000e-04\n",
      "Epoch 39/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.8712 - mae: 2.7531 - val_loss: 34.3104 - val_mae: 2.6409 - lr: 6.4000e-04\n",
      "Epoch 40/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.9667 - mae: 2.7370 - val_loss: 33.6967 - val_mae: 2.5302 - lr: 5.1200e-04\n",
      "Epoch 41/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.5123 - mae: 2.7286 - val_loss: 34.0944 - val_mae: 2.5729 - lr: 5.1200e-04\n",
      "Epoch 42/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.4194 - mae: 2.7227 - val_loss: 34.3889 - val_mae: 2.6315 - lr: 5.1200e-04\n",
      "Epoch 43/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.5763 - mae: 2.7154 - val_loss: 34.5303 - val_mae: 2.5284 - lr: 5.1200e-04\n",
      "Epoch 44/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 33.7085 - mae: 2.7281 - val_loss: 34.0959 - val_mae: 2.5025 - lr: 5.1200e-04\n",
      "Epoch 45/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.9817 - mae: 2.6890 - val_loss: 33.8920 - val_mae: 2.5905 - lr: 5.1200e-04\n",
      "Epoch 46/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.2763 - mae: 2.6514 - val_loss: 33.0432 - val_mae: 2.5695 - lr: 4.0960e-04\n",
      "Epoch 47/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.9963 - mae: 2.6687 - val_loss: 33.5067 - val_mae: 2.4859 - lr: 4.0960e-04\n",
      "Epoch 48/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.5040 - mae: 2.6685 - val_loss: 33.3359 - val_mae: 2.5667 - lr: 4.0960e-04\n",
      "Epoch 49/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.4450 - mae: 2.6642 - val_loss: 34.5142 - val_mae: 2.6296 - lr: 4.0960e-04\n",
      "Epoch 50/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.5638 - mae: 2.6666 - val_loss: 33.4915 - val_mae: 2.5917 - lr: 4.0960e-04\n",
      "Epoch 51/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.5157 - mae: 2.6575 - val_loss: 34.2451 - val_mae: 2.4603 - lr: 4.0960e-04\n",
      "Epoch 52/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.0605 - mae: 2.6465 - val_loss: 33.4488 - val_mae: 2.4842 - lr: 3.2768e-04\n",
      "Epoch 53/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.4146 - mae: 2.6694 - val_loss: 33.0317 - val_mae: 2.5997 - lr: 3.2768e-04\n",
      "Epoch 54/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.3619 - mae: 2.6591 - val_loss: 33.6154 - val_mae: 2.4813 - lr: 3.2768e-04\n",
      "Epoch 55/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.8187 - mae: 2.6508 - val_loss: 33.2721 - val_mae: 2.4621 - lr: 3.2768e-04\n",
      "Epoch 56/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.8238 - mae: 2.6426 - val_loss: 33.0580 - val_mae: 2.4977 - lr: 3.2768e-04\n",
      "Epoch 57/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.2388 - mae: 2.6615 - val_loss: 33.1617 - val_mae: 2.4776 - lr: 3.2768e-04\n",
      "Epoch 58/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.8194 - mae: 2.6501 - val_loss: 33.1901 - val_mae: 2.5885 - lr: 3.2768e-04\n",
      "Epoch 59/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.7430 - mae: 2.6407 - val_loss: 34.3193 - val_mae: 2.4808 - lr: 2.6214e-04\n",
      "Epoch 60/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.4971 - mae: 2.6471 - val_loss: 32.7609 - val_mae: 2.5560 - lr: 2.6214e-04\n",
      "Epoch 61/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.8936 - mae: 2.6437 - val_loss: 33.0554 - val_mae: 2.5213 - lr: 2.6214e-04\n",
      "Epoch 62/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.1945 - mae: 2.6219 - val_loss: 33.0746 - val_mae: 2.4745 - lr: 2.6214e-04\n",
      "Epoch 63/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.2586 - mae: 2.6069 - val_loss: 33.3105 - val_mae: 2.4875 - lr: 2.6214e-04\n",
      "Epoch 64/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.5193 - mae: 2.6202 - val_loss: 32.5606 - val_mae: 2.5085 - lr: 2.6214e-04\n",
      "Epoch 65/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.2339 - mae: 2.6096 - val_loss: 33.0005 - val_mae: 2.5550 - lr: 2.6214e-04\n",
      "Epoch 66/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.5952 - mae: 2.6294 - val_loss: 33.4374 - val_mae: 2.4609 - lr: 2.6214e-04\n",
      "Epoch 67/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.2523 - mae: 2.6152 - val_loss: 32.3937 - val_mae: 2.4731 - lr: 2.6214e-04\n",
      "Epoch 68/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.0061 - mae: 2.5949 - val_loss: 33.3109 - val_mae: 2.4332 - lr: 2.6214e-04\n",
      "Epoch 69/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 32.0263 - mae: 2.6358 - val_loss: 32.7001 - val_mae: 2.5332 - lr: 2.6214e-04\n",
      "Epoch 70/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.9885 - mae: 2.6170 - val_loss: 32.6727 - val_mae: 2.4756 - lr: 2.6214e-04\n",
      "Epoch 71/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.0596 - mae: 2.6127 - val_loss: 33.1299 - val_mae: 2.4304 - lr: 2.6214e-04\n",
      "Epoch 72/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.1903 - mae: 2.6129 - val_loss: 32.9513 - val_mae: 2.5482 - lr: 2.6214e-04\n",
      "Epoch 73/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.6064 - mae: 2.5928 - val_loss: 33.5952 - val_mae: 2.4345 - lr: 2.0972e-04\n",
      "Epoch 74/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.2970 - mae: 2.6048 - val_loss: 32.5407 - val_mae: 2.4867 - lr: 2.0972e-04\n",
      "Epoch 75/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.1342 - mae: 2.6120 - val_loss: 32.7389 - val_mae: 2.4361 - lr: 2.0972e-04\n",
      "Epoch 76/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.6554 - mae: 2.6158 - val_loss: 32.5046 - val_mae: 2.4788 - lr: 2.0972e-04\n",
      "Epoch 77/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.0249 - mae: 2.6020 - val_loss: 33.0268 - val_mae: 2.4396 - lr: 2.0972e-04\n",
      "Epoch 78/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.2795 - mae: 2.6041 - val_loss: 32.6992 - val_mae: 2.4593 - lr: 1.6777e-04\n",
      "Epoch 79/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.5737 - mae: 2.5948 - val_loss: 32.4071 - val_mae: 2.5009 - lr: 1.6777e-04\n",
      "Epoch 80/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.4082 - mae: 2.5669 - val_loss: 32.6548 - val_mae: 2.4672 - lr: 1.6777e-04\n",
      "Epoch 81/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.9565 - mae: 2.6111 - val_loss: 32.4720 - val_mae: 2.4439 - lr: 1.6777e-04\n",
      "Epoch 82/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.9115 - mae: 2.6128 - val_loss: 32.3246 - val_mae: 2.4694 - lr: 1.6777e-04\n",
      "Epoch 83/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.9217 - mae: 2.6046 - val_loss: 33.0182 - val_mae: 2.4452 - lr: 1.6777e-04\n",
      "Epoch 84/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.4506 - mae: 2.5725 - val_loss: 32.9113 - val_mae: 2.5522 - lr: 1.6777e-04\n",
      "Epoch 85/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.8314 - mae: 2.6063 - val_loss: 32.2953 - val_mae: 2.4349 - lr: 1.6777e-04\n",
      "Epoch 86/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.6454 - mae: 2.5829 - val_loss: 32.8419 - val_mae: 2.4110 - lr: 1.6777e-04\n",
      "Epoch 87/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.3824 - mae: 2.5707 - val_loss: 32.5625 - val_mae: 2.5202 - lr: 1.6777e-04\n",
      "Epoch 88/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.5967 - mae: 2.5801 - val_loss: 32.9709 - val_mae: 2.4669 - lr: 1.6777e-04\n",
      "Epoch 89/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.4083 - mae: 2.5767 - val_loss: 32.1752 - val_mae: 2.4676 - lr: 1.6777e-04\n",
      "Epoch 90/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.5975 - mae: 2.6028 - val_loss: 32.5102 - val_mae: 2.4879 - lr: 1.6777e-04\n",
      "Epoch 91/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 31.3400 - mae: 2.6227 - val_loss: 32.7114 - val_mae: 2.5355 - lr: 1.6777e-04\n",
      "Epoch 92/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.3891 - mae: 2.5781 - val_loss: 32.5256 - val_mae: 2.4115 - lr: 1.6777e-04\n",
      "Epoch 93/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.7574 - mae: 2.5601 - val_loss: 32.6416 - val_mae: 2.5484 - lr: 1.6777e-04\n",
      "Epoch 94/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.2887 - mae: 2.5943 - val_loss: 32.5738 - val_mae: 2.4316 - lr: 1.6777e-04\n",
      "Epoch 95/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.5450 - mae: 2.5552 - val_loss: 32.0778 - val_mae: 2.4440 - lr: 1.3422e-04\n",
      "Epoch 96/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.8414 - mae: 2.5630 - val_loss: 32.4048 - val_mae: 2.5416 - lr: 1.3422e-04\n",
      "Epoch 97/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.7752 - mae: 2.6199 - val_loss: 32.3813 - val_mae: 2.5128 - lr: 1.3422e-04\n",
      "Epoch 98/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.2314 - mae: 2.5720 - val_loss: 32.5362 - val_mae: 2.4473 - lr: 1.3422e-04\n",
      "Epoch 99/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.3396 - mae: 2.5800 - val_loss: 32.2757 - val_mae: 2.4868 - lr: 1.3422e-04\n",
      "Epoch 100/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.2274 - mae: 2.5787 - val_loss: 32.2185 - val_mae: 2.4793 - lr: 1.3422e-04\n",
      "Epoch 101/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.1103 - mae: 2.5846 - val_loss: 32.4492 - val_mae: 2.4194 - lr: 1.0737e-04\n",
      "Epoch 102/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.7290 - mae: 2.5538 - val_loss: 32.5354 - val_mae: 2.4495 - lr: 1.0737e-04\n",
      "Epoch 103/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.2181 - mae: 2.5875 - val_loss: 32.2067 - val_mae: 2.4671 - lr: 1.0737e-04\n",
      "Epoch 104/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.1218 - mae: 2.5732 - val_loss: 32.4715 - val_mae: 2.4319 - lr: 1.0737e-04\n",
      "Epoch 105/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.6853 - mae: 2.5627 - val_loss: 32.2029 - val_mae: 2.4326 - lr: 1.0737e-04\n",
      "Epoch 106/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.1218 - mae: 2.5901 - val_loss: 32.3484 - val_mae: 2.4178 - lr: 1.0000e-04\n",
      "Epoch 107/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.8428 - mae: 2.5491 - val_loss: 32.4978 - val_mae: 2.4356 - lr: 1.0000e-04\n",
      "Epoch 108/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.3532 - mae: 2.5677 - val_loss: 32.1785 - val_mae: 2.4688 - lr: 1.0000e-04\n",
      "Epoch 109/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.8956 - mae: 2.5655 - val_loss: 32.2368 - val_mae: 2.5155 - lr: 1.0000e-04\n",
      "Epoch 110/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.1495 - mae: 2.5277 - val_loss: 32.2468 - val_mae: 2.4397 - lr: 1.0000e-04\n",
      "Epoch 111/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.1253 - mae: 2.5758 - val_loss: 32.1907 - val_mae: 2.4392 - lr: 1.0000e-04\n",
      "Epoch 112/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.9287 - mae: 2.5622 - val_loss: 32.4144 - val_mae: 2.4992 - lr: 1.0000e-04\n",
      "Epoch 113/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.5923 - mae: 2.5606 - val_loss: 32.3459 - val_mae: 2.4087 - lr: 1.0000e-04\n",
      "Epoch 114/150\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 30.4296 - mae: 2.5991 - val_loss: 32.1371 - val_mae: 2.4356 - lr: 1.0000e-04\n",
      "Epoch 115/150\n",
      "458/495 [==========================>...] - ETA: 0s - loss: 29.9684 - mae: 2.5657Restoring model weights from the end of the best epoch: 95.\n",
      "495/495 [==============================] - 1s 2ms/step - loss: 29.7609 - mae: 2.5531 - val_loss: 32.6035 - val_mae: 2.4475 - lr: 1.0000e-04\n",
      "Epoch 115: early stopping\n",
      "155/155 [==============================] - 0s 842us/step - loss: 29.8222 - mae: 2.3775\n",
      "Loss en el conjunto de prueba: 29.822193145751953, MAE: 2.377504348754883\n"
     ]
    }
   ],
   "source": [
    "# Arquitectura del modelo\n",
    "\n",
    "\"\"\"  \n",
    ". configuramos un modelo secuencial donde la salida de una capa es la entrada de otra capa\n",
    ". Vamos a incrementar el número de unidades (ahora 128) en la primera y segunda capa oculta para intentar capturar más complejidad.\n",
    ". Regularización L2 para prevenir el sobreajuste\n",
    ". Normalización por lotes para estabilizar y acelerar el entrenamiento\n",
    ". en la segunda capa oculta añadimos Dropout: apagamos aleatoriamente un porcentaje de conexiones entre las neuronas. Reducimos respecto anteriores notebooks\n",
    ". última capa de salida sin función de activación\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)), Dropout(0.3),\n",
    "    \n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Mantenemos el optimizador 'adam' que ajusta la tasa de aprendizaje durante el entrenamiento, ajustando la función de pérdida y métricas si es necesario\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Ajustamos los callbacks disminuyendo el parámetro 'patience' y disminuyendo el 'min_lr' para tener una tasa de aprendizaje más amplio\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Incrementamos el número de epochs con los ajustes finos de los callbacks\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=150, validation_split=0.2, batch_size=32, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "# Evaluación del conjunto de prueba\n",
    "\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}, MAE: {test_mae}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imputador MICE entrenado con éxito.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuramos el imputador MICE con GradientBoostingRegressor y el estimador\n",
    "\n",
    "mice_imputer = IterativeImputer(estimator=GradientBoostingRegressor(\n",
    "                                    n_estimators=100,               # construcción de 100 árboles\n",
    "                                    max_depth=10,                   # profundidad máxima de los árboles\n",
    "                                    min_samples_split=4,            # número mínimo de muestras para dividir un nodo interno\n",
    "                                    min_samples_leaf=2,             # número mínimo de muestras para ser un nodo hoja\n",
    "                                    max_features='sqrt'),           # características a considerar al buscar la mejor división; en cada división se considera la raíz cuadrada del numero total de características.\n",
    "                                    max_iter=10,                    # número máximo de iteraciones de imputación.\n",
    "                                    random_state=42)\n",
    "\n",
    "# Preparamos los datos para el entrenamiento del imputador MICE eliminando la variable objetivo E_SIMEL\n",
    "\n",
    "X_mice = df_inicio.drop(columns=['E_SIMEL'])\n",
    "\n",
    "# Entrenamos el imputador MICE\n",
    "\n",
    "mice_imputer.fit(X_mice)\n",
    "\n",
    "# Imprimimos confirmación\n",
    "\n",
    "\"Imputador MICE entrenado con éxito.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las filas del dia 5 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_05_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "\n",
    "df_final_05_11_para_imputar = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_05_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_05_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto y\n",
    "# establecemos una condición que si los valores son más grandes que 0.2 establecemos un 1, y si son inferiores establecemos un 0.\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2] > 0.2, 1, 0)\n",
    "\n",
    "df_final_05_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 107ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [1.23025179e-01],\n",
       "       [1.46993637e-01],\n",
       "       [8.63879919e-02],\n",
       "       [3.87659073e-02],\n",
       "       [1.81292295e-02],\n",
       "       [1.84155703e-02],\n",
       "       [2.29549408e-02],\n",
       "       [1.83361769e-02],\n",
       "       [4.10163403e-03],\n",
       "       [3.90620184e+00],\n",
       "       [7.08654642e+00],\n",
       "       [1.05592585e+01],\n",
       "       [1.17294950e+01],\n",
       "       [1.06448069e+01],\n",
       "       [9.09349346e+00],\n",
       "       [6.92917681e+00],\n",
       "       [4.32571173e-02],\n",
       "       [2.02260351e+00],\n",
       "       [7.70964622e-02],\n",
       "       [9.36499834e-02],\n",
       "       [1.23699546e-01]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos los datos de df_final_05_11 para la predicción quitando la variable objetivo\n",
    "\n",
    "X_final_05_11 = df_final_05_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalización de los datos\n",
    "\n",
    "X_final_05_11_scaled = scaler.transform(X_final_05_11)\n",
    "\n",
    "# Realizamos la predicciones de E_SIMEL con el modelo de deep learning\n",
    "\n",
    "e_simel_predicciones = model.predict(X_final_05_11_scaled)\n",
    "\n",
    "e_simel_predicciones = np.maximum(e_simel_predicciones, 0)\n",
    "\n",
    "# Mostramos los resultados de la predicción\n",
    "\n",
    "e_simel_predicciones[:25]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_97720\\496075226.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24802</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24803</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24804</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24805</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24806</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24807</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24808</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24809</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24810</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24811</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24812</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24813</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24814</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.906202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24815</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.086546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24816</th>\n",
       "      <td>4.122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.559258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24817</th>\n",
       "      <td>5.437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.729495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24818</th>\n",
       "      <td>6.378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.644807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24819</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.093493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24820</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.929177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24821</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24822</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.022604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24823</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24824</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24825</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24802    0.000        0.0            0.000000\n",
       "24803    0.000        0.0            0.000000\n",
       "24804    0.000        0.0            0.000000\n",
       "24805    0.000        0.0            0.123025\n",
       "24806    0.000        0.0            0.146994\n",
       "24807    0.000        0.0            0.086388\n",
       "24808    0.000        0.0            0.038766\n",
       "24809    0.000        0.0            0.018129\n",
       "24810    0.000        0.0            0.018416\n",
       "24811    0.000        0.0            0.022955\n",
       "24812    0.000        0.0            0.018336\n",
       "24813    0.000        0.0            0.004102\n",
       "24814    0.000        0.0            3.906202\n",
       "24815    0.000        0.0            7.086546\n",
       "24816    4.122        0.0           10.559258\n",
       "24817    5.437        0.0           11.729495\n",
       "24818    6.378        0.0           10.644807\n",
       "24819    0.000        0.0            9.093493\n",
       "24820    0.000        0.0            6.929177\n",
       "24821    0.000        0.0            0.043257\n",
       "24822    0.000        0.0            2.022604\n",
       "24823    0.000        0.0            0.077096\n",
       "24824    0.000        0.0            0.093650\n",
       "24825    0.000        0.0            0.123700"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos el array a una lista para facilitar la asignación a una nueva columna que llamamos 'Prediccion_E_SIMEL'\n",
    "\n",
    "predicciones_lista = e_simel_predicciones.flatten().tolist()\n",
    "\n",
    "# Asignar las predicciones a df_final_05_11\n",
    "\n",
    "df_final_05_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Mostramos las primeras filas para verificar\n",
    "\n",
    "df_final_05_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  15.937000000000001\n",
      "Suma predicha:  62.78639578819275\n",
      "Desviación porcentual:  293.96621565032785 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# al igual que en todos los casos anteriores, hacemos un sumatorio y porcentaje de desviación para tener una primera idea de como van las predicciones\n",
    "\n",
    "suma_real_05 = df_final_05_11['E_SIMEL'].sum()\n",
    "suma_predicha_05 = df_final_05_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_05 = df_final_05_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_05 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_05 - suma_real_05) / suma_real_05\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_05)\n",
    "print(\"Suma predicha: \", suma_predicha_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_05)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 29.8723 - mae: 2.4927 - val_loss: 36.7958 - val_mae: 3.0325 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.8479 - mae: 2.4389 - val_loss: 36.6826 - val_mae: 2.9156 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.6558 - mae: 2.4327 - val_loss: 37.1788 - val_mae: 3.0429 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.6914 - mae: 2.4335 - val_loss: 37.2756 - val_mae: 2.9850 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.7714 - mae: 2.4208 - val_loss: 37.5580 - val_mae: 3.0274 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.8641 - mae: 2.4474 - val_loss: 37.8951 - val_mae: 3.0544 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.8574 - mae: 2.4267 - val_loss: 37.8784 - val_mae: 3.0477 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.7319 - mae: 2.4217 - val_loss: 38.0607 - val_mae: 3.0712 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 29.0383 - mae: 2.4481 - val_loss: 38.4314 - val_mae: 3.1030 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.6008 - mae: 2.4200 - val_loss: 38.5747 - val_mae: 3.1136 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.1129 - mae: 2.3936 - val_loss: 38.8136 - val_mae: 3.1388 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.3425 - mae: 2.4149 - val_loss: 39.4989 - val_mae: 3.1670 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.6418 - mae: 2.4210 - val_loss: 39.0405 - val_mae: 3.1321 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.4928 - mae: 2.4132 - val_loss: 39.5995 - val_mae: 3.2038 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2232 - mae: 2.3986 - val_loss: 40.0300 - val_mae: 3.2257 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2115 - mae: 2.4050 - val_loss: 39.6509 - val_mae: 3.1903 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.3949 - mae: 2.4050 - val_loss: 39.5542 - val_mae: 3.1703 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.9243 - mae: 2.3842 - val_loss: 39.6747 - val_mae: 3.1798 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.1352 - mae: 2.4082 - val_loss: 39.6347 - val_mae: 3.1289 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2642 - mae: 2.4027 - val_loss: 39.5928 - val_mae: 3.1168 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.1764 - mae: 2.4035 - val_loss: 39.8737 - val_mae: 3.2061 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "611/619 [============================>.] - ETA: 0s - loss: 28.3982 - mae: 2.4085Restoring model weights from the end of the best epoch: 2.\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.4119 - mae: 2.4072 - val_loss: 39.8555 - val_mae: 3.1869 - lr: 1.0000e-04\n",
      "Epoch 22: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IterativeImputer</label><div class=\"sk-toggleable__content\"><pre>IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2,\n",
       "                          min_samples_split=4)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IterativeImputer(estimator=GradientBoostingRegressor(max_depth=10,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     min_samples_split=4),\n",
       "                 random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Una vez tenemos la primera predicción, actualizamos df_inicio con los datos del día 5. Así estamos simulando como sería\n",
    "# el proceso de predicción en tiempo real\n",
    "\n",
    "datos_dia_5 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 5)]\n",
    "df_inicio_actualizado = pd.concat([df_inicio, datos_dia_5])\n",
    "\n",
    "# Preparamos df_actualizado con los datos de df_inicio y los datos del día 5 para reentrenar el modelo de deep learning\n",
    "\n",
    "X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "\n",
    "# Normalizamos con fit_transform\n",
    "\n",
    "X_total_scaled = scaler.fit_transform(X_actualizado)   \n",
    "\n",
    "# Y reentrenamos el modelo de deep learning con todos los datos con los parámetros utilizados para el entrenamiento inicial\n",
    "\n",
    "model.fit(X_total_scaled, y_actualizado, epochs=150, validation_split=0.2, batch_size=32, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "\n",
    "# reentrenamos también el modelo imputador con los nuevos datos con todas las variables menos con E_SIMEL\n",
    "\n",
    "mice_imputer.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Temp\\ipykernel_97720\\2659706717.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_SIMEL</th>\n",
       "      <th>PREVISION</th>\n",
       "      <th>Prediccion_E_SIMEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24826</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24827</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24829</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24830</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24831</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24832</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24833</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24834</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24835</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24836</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.195389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24837</th>\n",
       "      <td>0.000</td>\n",
       "      <td>11.6</td>\n",
       "      <td>11.239955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>0.000</td>\n",
       "      <td>15.8</td>\n",
       "      <td>14.367939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24839</th>\n",
       "      <td>0.000</td>\n",
       "      <td>19.4</td>\n",
       "      <td>16.467451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24840</th>\n",
       "      <td>3.946</td>\n",
       "      <td>24.3</td>\n",
       "      <td>18.555826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24841</th>\n",
       "      <td>22.905</td>\n",
       "      <td>29.8</td>\n",
       "      <td>17.202091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24842</th>\n",
       "      <td>21.310</td>\n",
       "      <td>32.9</td>\n",
       "      <td>16.108065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24843</th>\n",
       "      <td>9.663</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.568557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24844</th>\n",
       "      <td>0.718</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.994485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24845</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.383643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24846</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24848</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24849</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E_SIMEL  PREVISION  Prediccion_E_SIMEL\n",
       "24826    0.000        0.0            0.000000\n",
       "24827    0.000        0.0            0.000000\n",
       "24828    0.000        0.0            0.000000\n",
       "24829    0.000        0.0            0.000000\n",
       "24830    0.000        0.0            0.105763\n",
       "24831    0.000        0.0            0.174608\n",
       "24832    0.000        0.0            0.225763\n",
       "24833    0.000        0.0            0.257112\n",
       "24834    0.000        0.0            0.269135\n",
       "24835    0.000        0.0            0.248041\n",
       "24836    0.000        2.6            3.195389\n",
       "24837    0.000       11.6           11.239955\n",
       "24838    0.000       15.8           14.367939\n",
       "24839    0.000       19.4           16.467451\n",
       "24840    3.946       24.3           18.555826\n",
       "24841   22.905       29.8           17.202091\n",
       "24842   21.310       32.9           16.108065\n",
       "24843    9.663       19.0           12.568557\n",
       "24844    0.718        4.0            5.994485\n",
       "24845    0.000        0.0            2.383643\n",
       "24846    0.000        0.0            0.000000\n",
       "24847    0.000        0.0            0.000000\n",
       "24848    0.000        0.0            0.000000\n",
       "24849    0.000        0.0            0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seleccionamos las filas del dia 6 del df_final para hacer el tratamiento de la variable f_RUN\n",
    "\n",
    "df_final_06_11 = df_final[(df_final['Año'] == 2023) & (df_final['Mes'] == 11) & (df_final['Día'] == 6)]\n",
    "\n",
    "df_final_06_11_para_imputar = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "df_final_06_11_para_imputar[['f_RUN']] = np.nan  # Primero convertimos la columna f_RUN a NaN\n",
    "\n",
    "# En la variable creada para la imputación, predecimos los valores para las columnas siguientes\n",
    "\n",
    "valores_imputados = mice_imputer.transform(df_final_06_11_para_imputar[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])\n",
    "\n",
    "# Realmente solo queremos imputar la columna f_RUN, por lo tanto cogemos solo los valores imputados a la columna en concreto\n",
    "\n",
    "valores_imputados_f_RUN = np.where(valores_imputados[:, 2]> 0.2, 1, 0)\n",
    "\n",
    "df_final_06_11.loc[:, 'f_RUN'] = valores_imputados_f_RUN\n",
    "\n",
    "\n",
    "# Verificamos que los valores han sido imputados correctamente\n",
    "# df_final_05_11.head(25)\n",
    "\n",
    "# Prepararamos los datos de df_final_06_11 para la predicción\n",
    "\n",
    "X_final_06_11 = df_final_06_11.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "# Normalizamos los datos de X_final_05_11 \n",
    "\n",
    "X_final_06_11_scaled = scaler.transform(X_final_06_11)\n",
    "\n",
    "# Realizamos las predicciones de E_SIMEL\n",
    "\n",
    "e_simel_predicciones_06 = model.predict(X_final_06_11_scaled)\n",
    "\n",
    "# Para que no haya valores negativos los transformamos a cero\n",
    "\n",
    "e_simel_predicciones_06 = np.maximum(e_simel_predicciones_06, 0)\n",
    "\n",
    "# Mostrar las primeras 5 predicciones\n",
    "# e_simel_predicciones[:25]\n",
    "\n",
    "# Convertimos el array de predicciones a una lista para facilitar la asignación\n",
    "\n",
    "predicciones_lista = e_simel_predicciones_06.flatten().tolist()\n",
    "\n",
    "# Asignamos las predicciones a una nueva columna en el Datadrame df_final_06_11\n",
    "\n",
    "df_final_06_11['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "# Visualizamos las columnas para ver el resultado\n",
    "\n",
    "df_final_06_11[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.542\n",
      "Suma predicha:  119.36382293701172\n",
      "Desviación porcentual:  103.89433729119558 %\n",
      "Suma previsión:  159.4\n",
      "Desviación porcentual:  172.2831471422227 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes de desviación\n",
    "\n",
    "suma_real_06 = df_final_06_11['E_SIMEL'].sum()\n",
    "suma_predicha_06 = df_final_06_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_06 = df_final_06_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "\n",
    "\n",
    "if suma_real_06 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_06 - suma_real_06) / suma_real_06\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  # en caso de división por cero, retorna un valor especial para que no nos dé error\n",
    "\n",
    "print(\"Suma real: \", suma_real_06)\n",
    "print(\"Suma predicha: \", suma_predicha_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_06)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un función para agilizar el proceso de actualización, reentreno de los modelos, imputación, predicción y cálculo de las métricas\n",
    "\n",
    "def predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente,  mes, año, df_inicio_actualizado, df_final, modelo_deep, imputador):\n",
    "    \"\"\"\n",
    "    Función para actualizar el conjunto de entrenamiento con los datos reales de un día específico,\n",
    "    realizar la imputación para el día siguiente y predecir los valores de E_SIMEL para ese día.\n",
    "\n",
    "    Args:\n",
    "    dia_actual (int): Día actual para el que se actualizarán los datos.\n",
    "    dia_siguiente (int): Datos del día que queremos hacer las imputaciones y la predicción\n",
    "    mes (int): Mes del día actual.\n",
    "    año (int): Año del día actual.\n",
    "    df_inicio_actualizado (DataFrame): DataFrame actualizado con los datos hasta el día anterior.\n",
    "    df_final (DataFrame): DataFrame con los datos a predecir.\n",
    "    modelo_deep (DeepLearning de TencerFlow): Modelo de Deep Learning entrenado.\n",
    "    imputador (IterativeImputer): Imputador MICE entrenado.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame con las predicciones para el día siguiente.\n",
    "    DataFrame: DataFrame actualizado con los datos reales del día actual.\n",
    "    \"\"\"\n",
    "    # Actualización de df_actualizado con los datos de dia_actual\n",
    "\n",
    "    datos_dia_actual = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_actual)]\n",
    "    df_inicio_actualizado = pd.concat([df_inicio_actualizado, datos_dia_actual])\n",
    "\n",
    "    \n",
    "    # Prepararamos los datos para el modelo de deep learning\n",
    "\n",
    "    X_actualizado = df_inicio_actualizado.drop('E_SIMEL', axis=1)\n",
    "    y_actualizado = df_inicio_actualizado['E_SIMEL']\n",
    "\n",
    "        \n",
    "    # Normalizaación las características\n",
    "\n",
    "    X_actualizado_scaled = scaler.transform(X_actualizado)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min', restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\n",
    "\n",
    "    # Reentrenamos el modelo de deep learning con todos los datos pasándole los parámetros de modelado\n",
    "\n",
    "    modelo_deep.fit(X_actualizado_scaled, y_actualizado, epochs=150, validation_split=0.2, batch_size=32, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "    # Reentrenamos el imputador para todas las variables menos con E_SIMEL\n",
    "\n",
    "    imputador.fit(df_inicio_actualizado[['Period', 'PREVISION', 'f_RUN', 'Dia_Semana', 'Es_fin_semana', 'Año', 'Mes', 'Día']])    \n",
    "\n",
    "    \n",
    "\n",
    "      # Imputación de valores a la columna f_RUN para la predicción\n",
    "\n",
    "    df_dia_siguiente = df_final[(df_final['Año'] == año) & (df_final['Mes'] == mes) & (df_final['Día'] == dia_siguiente)]\n",
    "    df_dia_siguiente_para_imputar = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "    df_dia_siguiente_para_imputar[['f_RUN']] = np.nan  \n",
    "    \n",
    "    valores_imputados = imputador.transform(df_dia_siguiente_para_imputar)\n",
    "    \n",
    "    df_dia_siguiente.loc[:, 'f_RUN'] = np.where(valores_imputados[:, 2] > 0.2, 1, 0) \n",
    "\n",
    "    \n",
    "    \n",
    "    # Preparamos los datos para la predicción con el modelo de deep learning\n",
    "\n",
    "    X_prediccion = df_dia_siguiente.drop(['E_SIMEL'], axis=1)\n",
    "\n",
    "    # Normalizamos los datos \n",
    "\n",
    "    X_prediccion_scaled = scaler.transform(X_prediccion)\n",
    "\n",
    "    # Realizamos las predicciones de E_SIMEL\n",
    "    predicted_e_simel = model.predict(X_prediccion_scaled)\n",
    "\n",
    "    # Transformamos los valores negativos de la predicción a cero\n",
    "\n",
    "    predicted_e_simel = np.maximum(predicted_e_simel, 0)\n",
    "\n",
    "    # Mostramos las predicciones\n",
    "\n",
    "    # e_simel_predicciones[:25]\n",
    "\n",
    "    # Convertimos el array de las predicciones a una lista para facilitar la asignación\n",
    "    predicciones_lista = predicted_e_simel.flatten().tolist()\n",
    "\n",
    "    df_predicciones = df_dia_siguiente[['Año', 'Mes', 'Día', 'PREVISION', 'E_SIMEL']].copy()\n",
    "\n",
    "    # En el df_prediciones creamos una nueva columna con las predicciones\n",
    "\n",
    "    df_predicciones['Prediccion_E_SIMEL'] = predicciones_lista\n",
    "\n",
    "    # Mostramos el resultado\n",
    "\n",
    "    df_predicciones[['E_SIMEL', 'PREVISION', 'Prediccion_E_SIMEL']].head(25)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculamos las métricas: mse (Error cuadrático medio), r2 (coeficiente de determinación) y el mae(error medio absoluto)\n",
    "\n",
    "    mse = mean_squared_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "    mae = mean_absolute_error(df_predicciones['E_SIMEL'], df_predicciones['Prediccion_E_SIMEL'])\n",
    "\n",
    "\n",
    "    return df_predicciones, df_inicio_actualizado, mse, r2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.8962 - mae: 2.4414 - val_loss: 37.2010 - val_mae: 3.0410 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.9118 - mae: 2.4335 - val_loss: 38.2960 - val_mae: 3.1542 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.9191 - mae: 2.4265 - val_loss: 37.7209 - val_mae: 3.0859 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.7663 - mae: 2.4264 - val_loss: 37.6686 - val_mae: 3.0102 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.7725 - mae: 2.4390 - val_loss: 38.9180 - val_mae: 3.1847 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.3830 - mae: 2.4170 - val_loss: 38.2593 - val_mae: 3.0986 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.5791 - mae: 2.4191 - val_loss: 38.3640 - val_mae: 3.0780 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.7565 - mae: 2.4252 - val_loss: 39.0650 - val_mae: 3.1494 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2146 - mae: 2.4007 - val_loss: 39.4424 - val_mae: 3.2003 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2468 - mae: 2.4208 - val_loss: 38.9427 - val_mae: 3.1305 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.4778 - mae: 2.4263 - val_loss: 39.3661 - val_mae: 3.1861 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.6960 - mae: 2.4065 - val_loss: 38.8255 - val_mae: 3.0919 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2151 - mae: 2.4034 - val_loss: 39.5971 - val_mae: 3.1734 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.4879 - mae: 2.3809 - val_loss: 39.5078 - val_mae: 3.1619 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.8553 - mae: 2.3839 - val_loss: 39.7060 - val_mae: 3.1725 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.3476 - mae: 2.4193 - val_loss: 39.6294 - val_mae: 3.1271 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.9900 - mae: 2.3854 - val_loss: 40.1480 - val_mae: 3.2029 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.9714 - mae: 2.3868 - val_loss: 40.1770 - val_mae: 3.2186 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.0249 - mae: 2.4025 - val_loss: 39.5736 - val_mae: 3.1490 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.2401 - mae: 2.4022 - val_loss: 40.5401 - val_mae: 3.2408 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.0102 - mae: 2.3997 - val_loss: 40.2493 - val_mae: 3.2018 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.5096 - mae: 2.3677 - val_loss: 41.0318 - val_mae: 3.3037 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.8888 - mae: 2.3912 - val_loss: 41.9170 - val_mae: 3.3344 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.1471 - mae: 2.3965 - val_loss: 40.9177 - val_mae: 3.2429 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 28.0548 - mae: 2.3928 - val_loss: 41.4960 - val_mae: 3.3204 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "600/619 [============================>.] - ETA: 0s - loss: 28.1120 - mae: 2.4002Restoring model weights from the end of the best epoch: 1.\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 27.9900 - mae: 2.3956 - val_loss: 41.5198 - val_mae: 3.3228 - lr: 1.0000e-04\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "MSE: 42.871646347112154 R²: -0.07551958849326179 MAE: 3.75091641775767\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 6    # Actualización de datos con los que reentrenamos los modelos\n",
    "dia_siguiente = 7    # Preparación de datos para la imputación y predicción\n",
    "df_predicciones_07_11, df_inicio_actualizado, mse_07_11, r2_07_11, mae_07_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_07_11, \"R²:\", r2_07_11, \"MAE:\", mae_07_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  67.97999999999999\n",
      "Suma predicha:  66.23576879501343\n",
      "Desviación porcentual:  -2.565800536902857 %\n",
      "Suma previsión:  24.0\n",
      "Desviación porcentual:  -64.6954986760812 %\n"
     ]
    }
   ],
   "source": [
    "# Y como en la predicción anterior calculamos los sumatorios y los porcentajes de desviación\n",
    "\n",
    "suma_real_07_11 = df_predicciones_07_11['E_SIMEL'].sum()\n",
    "suma_predicha_07_11 = df_predicciones_07_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_07_11 = df_predicciones_07_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_07_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_07_11 - suma_real_07_11) / suma_real_07_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_07_11)\n",
    "print(\"Suma predicha: \", suma_predicha_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_07_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.9149 - mae: 2.4238 - val_loss: 37.5265 - val_mae: 3.0442 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.4715 - mae: 2.4160 - val_loss: 38.0755 - val_mae: 3.1390 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.8092 - mae: 2.4319 - val_loss: 37.9627 - val_mae: 3.0744 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.6617 - mae: 2.4194 - val_loss: 38.5368 - val_mae: 3.1066 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.5671 - mae: 2.4167 - val_loss: 38.4310 - val_mae: 3.1068 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.9437 - mae: 2.4271 - val_loss: 38.5834 - val_mae: 3.1123 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 29.0370 - mae: 2.4212 - val_loss: 39.2286 - val_mae: 3.1654 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.7826 - mae: 2.4337 - val_loss: 39.4248 - val_mae: 3.2132 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.3706 - mae: 2.4099 - val_loss: 39.2006 - val_mae: 3.1710 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.2916 - mae: 2.4065 - val_loss: 39.6501 - val_mae: 3.1949 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.5954 - mae: 2.4018 - val_loss: 39.4316 - val_mae: 3.1687 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.7547 - mae: 2.4234 - val_loss: 40.4879 - val_mae: 3.2779 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 27.7277 - mae: 2.3896 - val_loss: 40.0187 - val_mae: 3.2258 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.5895 - mae: 2.4096 - val_loss: 40.2012 - val_mae: 3.2175 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.0346 - mae: 2.4049 - val_loss: 39.6963 - val_mae: 3.1617 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 27.9700 - mae: 2.3898 - val_loss: 40.4481 - val_mae: 3.2544 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.4255 - mae: 2.4217 - val_loss: 40.3713 - val_mae: 3.2344 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.3723 - mae: 2.4007 - val_loss: 40.2318 - val_mae: 3.2249 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 27.7909 - mae: 2.3802 - val_loss: 41.2838 - val_mae: 3.2785 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.3576 - mae: 2.4054 - val_loss: 40.6928 - val_mae: 3.2116 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 27.8435 - mae: 2.4018 - val_loss: 41.9469 - val_mae: 3.3335 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 27.6465 - mae: 2.3712 - val_loss: 41.4797 - val_mae: 3.2947 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.0373 - mae: 2.4018 - val_loss: 41.5010 - val_mae: 3.2996 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.1641 - mae: 2.4071 - val_loss: 43.1239 - val_mae: 3.4165 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 28.1653 - mae: 2.4014 - val_loss: 40.6457 - val_mae: 3.2048 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "606/620 [============================>.] - ETA: 0s - loss: 27.8409 - mae: 2.3978Restoring model weights from the end of the best epoch: 1.\n",
      "620/620 [==============================] - 1s 2ms/step - loss: 27.9219 - mae: 2.4021 - val_loss: 40.9946 - val_mae: 3.2490 - lr: 1.0000e-04\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 17.712879938026486 R²: -4.831385047637251 MAE: 2.1503420369029045\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función. Actualización de datos reales del día 7 para reentrenar los modelos\n",
    "# Preparación de los datos del día posterior, en este caso del día 8, para la imputación y predicción\n",
    "\n",
    "dia_actual = 7\n",
    "dia_siguiente = 8\n",
    "\n",
    "df_predicciones_08_11, df_inicio_actualizado, mse_08_11, r2_08_11, mae_08_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_08_11, \"R²:\", r2_08_11, \"MAE:\", mae_08_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  11.296999999999999\n",
      "Suma predicha:  62.90520888566971\n",
      "Desviación porcentual:  456.8310957393088 %\n",
      "Suma previsión:  0.0\n",
      "Desviación porcentual:  -100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_08_11 = df_predicciones_08_11['E_SIMEL'].sum()\n",
    "suma_predicha_08_11 = df_predicciones_08_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_08_11 = df_predicciones_08_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_08_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_08_11 - suma_real_08_11) / suma_real_08_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_08_11)\n",
    "print(\"Suma predicha: \", suma_predicha_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_08_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.7036 - mae: 2.4154 - val_loss: 37.6584 - val_mae: 2.9552 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4911 - mae: 2.4139 - val_loss: 37.8453 - val_mae: 3.0668 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.5892 - mae: 2.4259 - val_loss: 37.8838 - val_mae: 3.0218 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.6122 - mae: 2.4078 - val_loss: 38.4140 - val_mae: 3.1016 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.9316 - mae: 2.4299 - val_loss: 38.3814 - val_mae: 3.0798 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.5159 - mae: 2.4132 - val_loss: 38.7997 - val_mae: 3.1359 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4652 - mae: 2.4291 - val_loss: 38.7651 - val_mae: 3.1245 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4490 - mae: 2.4189 - val_loss: 38.7730 - val_mae: 3.1143 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.2305 - mae: 2.4116 - val_loss: 39.2199 - val_mae: 3.1685 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.7023 - mae: 2.4246 - val_loss: 39.0418 - val_mae: 3.1086 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.5838 - mae: 2.4301 - val_loss: 39.3192 - val_mae: 3.1471 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0526 - mae: 2.4086 - val_loss: 39.6802 - val_mae: 3.1748 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.3194 - mae: 2.4035 - val_loss: 39.2226 - val_mae: 3.0910 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4147 - mae: 2.4152 - val_loss: 39.8226 - val_mae: 3.1834 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0818 - mae: 2.3980 - val_loss: 39.4439 - val_mae: 3.0789 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4270 - mae: 2.4076 - val_loss: 40.1926 - val_mae: 3.2190 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.1443 - mae: 2.4049 - val_loss: 39.4853 - val_mae: 3.1112 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.1174 - mae: 2.4052 - val_loss: 39.8463 - val_mae: 3.1724 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.2404 - mae: 2.4009 - val_loss: 40.0344 - val_mae: 3.1224 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0603 - mae: 2.3953 - val_loss: 39.9283 - val_mae: 3.1651 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.7245 - mae: 2.3877 - val_loss: 40.4738 - val_mae: 3.1979 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0309 - mae: 2.4027 - val_loss: 40.5436 - val_mae: 3.2006 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.2972 - mae: 2.4020 - val_loss: 40.8836 - val_mae: 3.2374 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4653 - mae: 2.4187 - val_loss: 41.0028 - val_mae: 3.2623 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.9636 - mae: 2.3982 - val_loss: 40.1054 - val_mae: 3.1221 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "613/621 [============================>.] - ETA: 0s - loss: 28.1925 - mae: 2.4102Restoring model weights from the end of the best epoch: 1.\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0606 - mae: 2.4039 - val_loss: 41.1245 - val_mae: 3.2600 - lr: 1.0000e-04\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "MSE: 29.629446369860748 R²: 0.6831778283465164 MAE: 2.573092273592949\n"
     ]
    }
   ],
   "source": [
    "# Seguimos con el mismo proceso anterior. Llamamos a la función con los días específicos que queremos actualizar, imputar y predecir.\n",
    "\n",
    "dia_actual = 8\n",
    "dia_siguiente = 9\n",
    "\n",
    "df_predicciones_09_11, df_inicio_actualizado, mse_09_11, r2_09_11, mae_09_11 = predecir_y_actualizar_para_un_dia(dia_actual, dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_09_11, \"R²:\", r2_09_11, \"MAE:\", mae_09_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  102.43700000000001\n",
      "Suma predicha:  98.3506852388382\n",
      "Desviación porcentual:  -3.9891003847846145 %\n",
      "Suma previsión:  127.30000000000001\n",
      "Desviación porcentual:  24.271503460663627 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y procentajes\n",
    "\n",
    "suma_real_09_11 = df_predicciones_09_11['E_SIMEL'].sum()\n",
    "suma_predicha_09_11 = df_predicciones_09_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_09_11 = df_predicciones_09_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_09_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_09_11 - suma_real_09_11) / suma_real_09_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "    \n",
    "\n",
    "print(\"Suma real: \", suma_real_09_11)\n",
    "print(\"Suma predicha: \", suma_predicha_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_09_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.7783 - mae: 2.4328 - val_loss: 38.0597 - val_mae: 3.1138 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4557 - mae: 2.4124 - val_loss: 37.7654 - val_mae: 3.0734 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.5303 - mae: 2.4264 - val_loss: 38.9161 - val_mae: 3.1871 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4532 - mae: 2.4264 - val_loss: 37.7268 - val_mae: 3.0389 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.5306 - mae: 2.4123 - val_loss: 38.4891 - val_mae: 3.1265 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0078 - mae: 2.3955 - val_loss: 38.5786 - val_mae: 3.1316 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4296 - mae: 2.4069 - val_loss: 39.4930 - val_mae: 3.1993 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.3771 - mae: 2.4073 - val_loss: 38.5452 - val_mae: 3.0752 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.2997 - mae: 2.4093 - val_loss: 39.1382 - val_mae: 3.1505 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4289 - mae: 2.4041 - val_loss: 39.2146 - val_mae: 3.1611 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.2306 - mae: 2.3968 - val_loss: 39.5350 - val_mae: 3.1876 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.1581 - mae: 2.3892 - val_loss: 39.3432 - val_mae: 3.1525 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.1625 - mae: 2.4047 - val_loss: 40.6335 - val_mae: 3.2775 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.8919 - mae: 2.3990 - val_loss: 40.4608 - val_mae: 3.2690 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0599 - mae: 2.3915 - val_loss: 40.8791 - val_mae: 3.2847 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.2430 - mae: 2.4040 - val_loss: 40.5011 - val_mae: 3.2397 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.8969 - mae: 2.3861 - val_loss: 41.4877 - val_mae: 3.3252 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4642 - mae: 2.4145 - val_loss: 41.1365 - val_mae: 3.3088 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.0328 - mae: 2.3946 - val_loss: 40.5221 - val_mae: 3.2578 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.5071 - mae: 2.4112 - val_loss: 40.8455 - val_mae: 3.2651 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.7395 - mae: 2.3781 - val_loss: 40.6479 - val_mae: 3.2293 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.9290 - mae: 2.3949 - val_loss: 41.2431 - val_mae: 3.2896 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.5362 - mae: 2.3759 - val_loss: 42.2147 - val_mae: 3.3559 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.4934 - mae: 2.4176 - val_loss: 42.5425 - val_mae: 3.3776 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 28.1069 - mae: 2.3977 - val_loss: 42.9672 - val_mae: 3.4236 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.8586 - mae: 2.4016 - val_loss: 45.0354 - val_mae: 3.5305 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.9997 - mae: 2.4023 - val_loss: 42.4139 - val_mae: 3.3787 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.9692 - mae: 2.4101 - val_loss: 40.8865 - val_mae: 3.2241 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "608/621 [============================>.] - ETA: 0s - loss: 28.0031 - mae: 2.3985Restoring model weights from the end of the best epoch: 4.\n",
      "621/621 [==============================] - 1s 2ms/step - loss: 27.9466 - mae: 2.3963 - val_loss: 41.2804 - val_mae: 3.2433 - lr: 1.0000e-04\n",
      "Epoch 29: early stopping\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "MSE: 18.043212607880942 R²: 0.25557976282561623 MAE: 2.1493928457895914\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "dia_actual = 9\n",
    "dia_siguiente = 10\n",
    "\n",
    "df_predicciones_10_11, df_inicio_actualizado, mse_10_11, r2_10_11, mae_10_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_10_11, \"R²:\", r2_10_11, \"MAE:\", mae_10_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  58.955\n",
      "Suma predicha:  104.57176780700684\n",
      "Desviación porcentual:  77.3755708710149 %\n",
      "Suma previsión:  112.6\n",
      "Desviación porcentual:  90.99313035365958 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_10_11 = df_predicciones_10_11['E_SIMEL'].sum()\n",
    "suma_predicha_10_11 = df_predicciones_10_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_10_11 = df_predicciones_10_11['PREVISION'].sum()\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_10_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_10_11 - suma_real_10_11) / suma_real_10_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_10_11)\n",
    "print(\"Suma predicha: \", suma_predicha_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_10_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 29.1631 - mae: 2.4495 - val_loss: 37.6400 - val_mae: 3.0685 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 29.1080 - mae: 2.4526 - val_loss: 37.9030 - val_mae: 3.0795 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4634 - mae: 2.4043 - val_loss: 37.9820 - val_mae: 3.0831 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4995 - mae: 2.4344 - val_loss: 38.3075 - val_mae: 3.0873 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.6746 - mae: 2.4307 - val_loss: 40.1163 - val_mae: 3.2855 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.8181 - mae: 2.4181 - val_loss: 38.9872 - val_mae: 3.1743 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.3359 - mae: 2.4232 - val_loss: 38.5061 - val_mae: 3.0885 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4451 - mae: 2.4221 - val_loss: 38.5482 - val_mae: 3.0590 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2980 - mae: 2.4176 - val_loss: 39.1906 - val_mae: 3.1412 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2077 - mae: 2.4027 - val_loss: 39.7515 - val_mae: 3.2001 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.3198 - mae: 2.4128 - val_loss: 39.8080 - val_mae: 3.2214 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2749 - mae: 2.4115 - val_loss: 39.5540 - val_mae: 3.1699 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4999 - mae: 2.4184 - val_loss: 40.1840 - val_mae: 3.2258 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.1889 - mae: 2.3890 - val_loss: 41.2114 - val_mae: 3.3539 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.9086 - mae: 2.3918 - val_loss: 39.0620 - val_mae: 3.0959 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.6251 - mae: 2.4299 - val_loss: 39.4630 - val_mae: 3.1388 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.0922 - mae: 2.4089 - val_loss: 41.0390 - val_mae: 3.3029 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.1592 - mae: 2.4125 - val_loss: 40.2697 - val_mae: 3.2186 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.8813 - mae: 2.3830 - val_loss: 40.2187 - val_mae: 3.1966 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.6781 - mae: 2.3890 - val_loss: 40.1701 - val_mae: 3.2137 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.6540 - mae: 2.4254 - val_loss: 39.6980 - val_mae: 3.1854 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.0802 - mae: 2.4100 - val_loss: 40.7708 - val_mae: 3.2438 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.6705 - mae: 2.3843 - val_loss: 40.2044 - val_mae: 3.2269 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.7485 - mae: 2.3810 - val_loss: 41.2967 - val_mae: 3.3177 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.5860 - mae: 2.3805 - val_loss: 41.0719 - val_mae: 3.2612 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "620/622 [============================>.] - ETA: 0s - loss: 27.8050 - mae: 2.3949Restoring model weights from the end of the best epoch: 1.\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.8221 - mae: 2.3961 - val_loss: 40.9280 - val_mae: 3.2028 - lr: 1.0000e-04\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MSE: 58.73390762757729 R²: 0.3268030278512901 MAE: 3.390036135574182\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 10\n",
    "dia_siguiente = 13\n",
    "\n",
    "df_predicciones_13_11, df_inicio_actualizado, mse_13_11, r2_13_11, mae_13_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_13_11, \"R²:\", r2_13_11, \"MAE:\", mae_13_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  107.141\n",
      "Suma predicha:  42.53458350896835\n",
      "Desviación porcentual:  -60.30036726466213 %\n",
      "Suma previsión:  20.900000000000002\n",
      "Desviación porcentual:  -80.492995211917 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_13_11 = df_predicciones_13_11['E_SIMEL'].sum()\n",
    "suma_predicha_13_11 = df_predicciones_13_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_13_11 = df_predicciones_13_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_13_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_13_11 - suma_real_13_11) / suma_real_13_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_13_11)\n",
    "print(\"Suma predicha: \", suma_predicha_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_13_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.5982 - mae: 2.4245 - val_loss: 38.0239 - val_mae: 3.0838 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.8162 - mae: 2.4242 - val_loss: 39.1239 - val_mae: 3.2152 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.5067 - mae: 2.4227 - val_loss: 39.2930 - val_mae: 3.2078 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.3035 - mae: 2.3971 - val_loss: 38.9356 - val_mae: 3.1639 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.7228 - mae: 2.4327 - val_loss: 38.3179 - val_mae: 3.0732 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.3966 - mae: 2.4263 - val_loss: 38.8136 - val_mae: 3.1132 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.3084 - mae: 2.4106 - val_loss: 38.7368 - val_mae: 3.1126 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4312 - mae: 2.4113 - val_loss: 39.4378 - val_mae: 3.2131 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4919 - mae: 2.4140 - val_loss: 38.7477 - val_mae: 3.1196 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.5913 - mae: 2.4098 - val_loss: 39.1008 - val_mae: 3.1482 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2561 - mae: 2.4166 - val_loss: 38.8412 - val_mae: 3.0311 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.0966 - mae: 2.4050 - val_loss: 39.4488 - val_mae: 3.1566 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4546 - mae: 2.4186 - val_loss: 39.5088 - val_mae: 3.1576 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4016 - mae: 2.4089 - val_loss: 39.2112 - val_mae: 3.1330 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2240 - mae: 2.4071 - val_loss: 39.5751 - val_mae: 3.1443 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.5380 - mae: 2.4322 - val_loss: 39.4498 - val_mae: 3.1463 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.9849 - mae: 2.4124 - val_loss: 40.4011 - val_mae: 3.2306 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.0078 - mae: 2.3936 - val_loss: 40.2733 - val_mae: 3.2240 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.4663 - mae: 2.4155 - val_loss: 40.7437 - val_mae: 3.2884 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.7529 - mae: 2.3869 - val_loss: 41.2614 - val_mae: 3.3201 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.8502 - mae: 2.3893 - val_loss: 41.6199 - val_mae: 3.3331 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.3096 - mae: 2.4148 - val_loss: 41.6070 - val_mae: 3.3108 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.9628 - mae: 2.4037 - val_loss: 42.1903 - val_mae: 3.3947 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2439 - mae: 2.4028 - val_loss: 40.5132 - val_mae: 3.2445 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 27.7999 - mae: 2.4001 - val_loss: 41.7011 - val_mae: 3.3653 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "598/622 [===========================>..] - ETA: 0s - loss: 28.5793 - mae: 2.4351Restoring model weights from the end of the best epoch: 1.\n",
      "622/622 [==============================] - 1s 2ms/step - loss: 28.2992 - mae: 2.4245 - val_loss: 40.7573 - val_mae: 3.2472 - lr: 1.0000e-04\n",
      "Epoch 26: early stopping\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "MSE: 29.233430272143323 R²: 0.6894939235067925 MAE: 2.560818786938985\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 13\n",
    "dia_siguiente = 14\n",
    "\n",
    "df_predicciones_14_11, df_inicio_actualizado, mse_14_11, r2_14_11, mae_14_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_14_11, \"R²:\", r2_14_11, \"MAE:\", mae_14_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  112.33099999999999\n",
      "Suma predicha:  75.38449382781982\n",
      "Desviación porcentual:  -32.890748032315365 %\n",
      "Suma previsión:  120.5\n",
      "Desviación porcentual:  7.272257880727503 %\n"
     ]
    }
   ],
   "source": [
    "# Sumatorios y porcentajes\n",
    "\n",
    "suma_real_14_11 = df_predicciones_14_11['E_SIMEL'].sum()\n",
    "suma_predicha_14_11 = df_predicciones_14_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_14_11 = df_predicciones_14_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_14_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_14_11 - suma_real_14_11) / suma_real_14_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_14_11)\n",
    "print(\"Suma predicha: \", suma_predicha_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_14_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.8595 - mae: 2.4229 - val_loss: 38.3143 - val_mae: 3.1213 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4010 - mae: 2.4195 - val_loss: 37.8518 - val_mae: 3.0515 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.3045 - mae: 2.4092 - val_loss: 39.0222 - val_mae: 3.1713 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4517 - mae: 2.4179 - val_loss: 38.2962 - val_mae: 3.0795 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.5805 - mae: 2.4154 - val_loss: 39.2775 - val_mae: 3.1877 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.5550 - mae: 2.4297 - val_loss: 39.5044 - val_mae: 3.2093 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.5323 - mae: 2.4240 - val_loss: 38.7637 - val_mae: 3.0704 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.5019 - mae: 2.4127 - val_loss: 39.7685 - val_mae: 3.2010 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4517 - mae: 2.4174 - val_loss: 39.0395 - val_mae: 3.1285 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.2590 - mae: 2.4156 - val_loss: 39.5938 - val_mae: 3.2028 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4703 - mae: 2.4104 - val_loss: 39.9066 - val_mae: 3.2223 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.0510 - mae: 2.4007 - val_loss: 41.7105 - val_mae: 3.3620 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4567 - mae: 2.4184 - val_loss: 39.8504 - val_mae: 3.1948 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.5311 - mae: 2.4176 - val_loss: 41.9280 - val_mae: 3.3706 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.5814 - mae: 2.3794 - val_loss: 41.4955 - val_mae: 3.3343 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.8204 - mae: 2.4235 - val_loss: 40.0702 - val_mae: 3.1806 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4729 - mae: 2.4061 - val_loss: 41.0568 - val_mae: 3.2749 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.1894 - mae: 2.3980 - val_loss: 40.2755 - val_mae: 3.1897 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.4862 - mae: 2.4305 - val_loss: 41.0712 - val_mae: 3.2586 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.9346 - mae: 2.3971 - val_loss: 41.6889 - val_mae: 3.3112 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.0218 - mae: 2.3950 - val_loss: 41.3909 - val_mae: 3.2930 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.7281 - mae: 2.3947 - val_loss: 41.6078 - val_mae: 3.2830 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 28.2485 - mae: 2.4007 - val_loss: 41.7030 - val_mae: 3.3400 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.7705 - mae: 2.4102 - val_loss: 41.6660 - val_mae: 3.3084 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.9775 - mae: 2.3892 - val_loss: 42.0891 - val_mae: 3.3645 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.7297 - mae: 2.3944 - val_loss: 43.5953 - val_mae: 3.4541 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "598/623 [===========================>..] - ETA: 0s - loss: 27.9647 - mae: 2.4014Restoring model weights from the end of the best epoch: 2.\n",
      "623/623 [==============================] - 1s 2ms/step - loss: 27.8019 - mae: 2.3967 - val_loss: 43.9889 - val_mae: 3.4637 - lr: 1.0000e-04\n",
      "Epoch 27: early stopping\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "MSE: 49.64275092717219 R²: 0.029483528879031717 MAE: 4.036477751334508\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función\n",
    "\n",
    "dia_actual = 14\n",
    "dia_siguiente = 15\n",
    "\n",
    "df_predicciones_15_11, df_inicio_actualizado, mse_15_11, r2_15_11, mae_15_11 = predecir_y_actualizar_para_un_dia(dia_actual,dia_siguiente, 11, 2023, df_inicio_actualizado, df_final, model, mice_imputer)\n",
    "print(\"MSE:\", mse_15_11, \"R²:\", r2_15_11, \"MAE:\", mae_15_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma real:  88.333\n",
      "Suma predicha:  103.69700598716736\n",
      "Desviación porcentual:  17.393279960113844 %\n",
      "Suma previsión:  131.89999999999998\n",
      "Desviación porcentual:  49.32131819365354 %\n"
     ]
    }
   ],
   "source": [
    "# Sumas y porcentajes\n",
    "\n",
    "suma_real_15_11 = df_predicciones_15_11['E_SIMEL'].sum()\n",
    "suma_predicha_15_11 = df_predicciones_15_11['Prediccion_E_SIMEL'].sum()\n",
    "suma_prevision_15_11 = df_predicciones_15_11['PREVISION'].sum()\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual = 100 * (suma_predicha_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual = float('inf')  \n",
    "\n",
    "\n",
    "\n",
    "if suma_real_15_11 != 0:\n",
    "    desviacion_porcentual_prevision = 100 * (suma_prevision_15_11 - suma_real_15_11) / suma_real_15_11\n",
    "else:\n",
    "    desviacion_porcentual_prevision = float('inf')  \n",
    "\n",
    "print(\"Suma real: \", suma_real_15_11)\n",
    "print(\"Suma predicha: \", suma_predicha_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual, \"%\")\n",
    "print(\"Suma previsión: \", suma_prevision_15_11)\n",
    "print(\"Desviación porcentual: \", desviacion_porcentual_prevision, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todos los Dataframes que contienen las prediccions, previsiones y datos reales para calcular las méstricas en conjunto\n",
    "\n",
    "df_predicciones_totales = pd.concat([df_final_05_11, df_final_06_11, df_predicciones_07_11, \n",
    "                                     df_predicciones_08_11, df_predicciones_09_11,df_predicciones_10_11, \n",
    "                                     df_predicciones_13_11, df_predicciones_14_11, df_predicciones_15_11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Predicciones:  2.8895682265626057\n",
      "MSE Predicciones:  33.00859628309593\n",
      "R² Predicciones:  0.3473675952454033\n",
      "MAE Previsiones:  2.3999490740740743\n",
      "MSE Previsiones:  35.93822944907407\n",
      "R² Previsiones:  0.2894440918718205\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de las métricas para ver si nos indican si mejoran los errores entren la predicción (Prediccion_E_SIMEL), previsión (PREVISION)y producción real(E_SIMEL)\n",
    "\n",
    "def calcular_metricas(df):\n",
    "    mae = mean_absolute_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    mse = mean_squared_error(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    r2 = r2_score(df['E_SIMEL'], df['Prediccion_E_SIMEL'])\n",
    "    return mae, mse, r2\n",
    "\n",
    "# Métricas para la predicciones\n",
    "\n",
    "mae_pred, mse_pred, r2_pred = calcular_metricas(df_predicciones_totales)\n",
    "\n",
    "# Cambiamos la columna de predicción por la de previsión\n",
    "\n",
    "df_previsiones = df_predicciones_totales.copy()\n",
    "df_previsiones['Prediccion_E_SIMEL'] = df_previsiones['PREVISION']\n",
    "\n",
    "# Métricas para la previsión\n",
    "\n",
    "mae_prev, mse_prev, r2_prev = calcular_metricas(df_previsiones)\n",
    "\n",
    "# Visualizamos los resultados\n",
    "\n",
    "print(\"MAE Predicciones: \", mae_pred)\n",
    "print(\"MSE Predicciones: \", mse_pred)\n",
    "print(\"R² Predicciones: \", r2_pred)\n",
    "print(\"MAE Previsiones: \", mae_prev)\n",
    "print(\"MSE Previsiones: \", mse_prev)\n",
    "print(\"R² Previsiones: \", r2_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de los valores en la columna E_SIMEL: 622.953\n",
      "Suma de las predicciones: 735.8297327756882\n",
      "Suma de las previsiones : 696.6\n",
      "Diferencia entre predicciones totales y E_SIMEL total: 112.8767327756882\n",
      "Diferencia entre previsiones y E_SIMEL total: 73.64700000000005\n",
      "No mejoramos la predicción respecto la PREVISION real en: 39.22973277568815, por lo tanto, con este modelo, no estamos mejorando las previsiones.\n"
     ]
    }
   ],
   "source": [
    "# sumamos todos los valores de las columnas que queremo comparar\n",
    "\n",
    "suma_e_simel = df_predicciones_totales['E_SIMEL'].sum()\n",
    "sumas_totales_predicciones = df_predicciones_totales['Prediccion_E_SIMEL'].sum()\n",
    "sumas_previsiones = df_predicciones_totales['PREVISION'].sum()\n",
    "\n",
    "\n",
    "# Calculamos las diferencias entre la prediccion y la previsión respecto la producción real E_SIMEL\n",
    "\n",
    "diferencia_prediccion_vs_produccion_real = abs(sumas_totales_predicciones - suma_e_simel)\n",
    "diferencia_prevision_vs_produccion_real = abs(sumas_previsiones - suma_e_simel)\n",
    "\n",
    "\n",
    "# Imprimimos los resultados para poder visualizar si mejoramos las previsiones a lo largo de todas las predicciones.\n",
    "\n",
    "print(f\"Suma de los valores en la columna E_SIMEL: {suma_e_simel}\")\n",
    "print(f\"Suma de las predicciones: {sumas_totales_predicciones}\")\n",
    "print(f\"Suma de las previsiones : {sumas_previsiones}\")\n",
    "\n",
    "\n",
    "print(f\"Diferencia entre predicciones totales y E_SIMEL total: {diferencia_prediccion_vs_produccion_real}\")\n",
    "print(f\"Diferencia entre previsiones y E_SIMEL total: {diferencia_prevision_vs_produccion_real}\")\n",
    "\n",
    "\n",
    "# Calculamos la diferencia entre la predicción y la previsión para saber si el modelo de predicción mejora la previsión\n",
    "\n",
    "diferencia = diferencia_prediccion_vs_produccion_real - diferencia_prevision_vs_produccion_real\n",
    "\n",
    "if diferencia_prediccion_vs_produccion_real > diferencia_prevision_vs_produccion_real:\n",
    "    print(f\"No mejoramos la predicción respecto la PREVISION real en: {diferencia}, por lo tanto, con este modelo, no estamos mejorando las previsiones.\")\n",
    "else:\n",
    "    print(f\"La predicción es MEJOR que la previsión en: {-diferencia} unidades, por lo tanto, cumplimos nuestro objetivo de mejorar la PREVISIÓN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valores que cogemos para el gráfico\n",
    "\n",
    "categorias = ['E_SIMEL Total', 'Predicciones Total', 'Previsiones Total']\n",
    "valores = [suma_e_simel, sumas_totales_predicciones, sumas_previsiones]\n",
    "\n",
    "# Creamos un gráfico de barras\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barra = plt.bar(categorias, valores, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Añadimos las etiqutas\n",
    "\n",
    "for rect in barra:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Título del gráfico y ponemos las etiquteas a cada barra\n",
    "    \n",
    "plt.title('Comparación de la Producción Real, Predicciones y Previsiones Totales')\n",
    "plt.xlabel('Categorías')\n",
    "plt.ylabel('Valor Total')\n",
    "\n",
    "\n",
    "# Ubicamos el texto de la diferencia\n",
    "\n",
    "pos_y = valores[1] / 2\n",
    "pos_x = categorias[1]\n",
    "plt.text(pos_x, pos_y, f'Mejoramos la predicción respecto a la previsión en\\n{diferencia:.2f} unidades', ha='center', va='center', fontsize=12, color='black', bbox=dict(facecolor='green', alpha=0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696.5999999999999"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_prevision = df_final['PREVISION'].sum()\n",
    "suma_prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622.953"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma_E_SIMEL = df_final['E_SIMEL'].sum()\n",
    "suma_E_SIMEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
